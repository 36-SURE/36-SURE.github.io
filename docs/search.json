[
  {
    "objectID": "sports.html",
    "href": "sports.html",
    "title": "Sports",
    "section": "",
    "text": "Note: To download a .qmd file, right click and choose “Save Link As…”.\n\n\n\nDate\nTitle\nMaterials\n\n\n\n\nJune 3\nGetting started with R\n.html   .qmd\n\n\nJune 4\nData wrangling\n.html   .qmd\n\n\nJune 5\nData visualization\n.html   .qmd\n\n\nJune 6\nUsing GitHub for project collaboration\n.html"
  },
  {
    "objectID": "sports.html#labs",
    "href": "sports.html#labs",
    "title": "Sports",
    "section": "",
    "text": "Note: To download a .qmd file, right click and choose “Save Link As…”.\n\n\n\nDate\nTitle\nMaterials\n\n\n\n\nJune 3\nGetting started with R\n.html   .qmd\n\n\nJune 4\nData wrangling\n.html   .qmd\n\n\nJune 5\nData visualization\n.html   .qmd\n\n\nJune 6\nUsing GitHub for project collaboration\n.html"
  },
  {
    "objectID": "sports.html#eda-projects",
    "href": "sports.html#eda-projects",
    "title": "Sports",
    "section": "EDA projects",
    "text": "EDA projects\n\n\n\nProject 1\nWTA Grand Slam statistics\ndescription\n\n\nProject 2\nWNBA shooting\ndescription\n\n\nProject 3\nNWSL team statistics\ndescription\n\n\nProject 4\nNHL shooting\ndescription\n\n\nProject 5\nNFL passing\ndescription"
  },
  {
    "objectID": "sports.html#resources",
    "href": "sports.html#resources",
    "title": "Sports",
    "section": "Resources",
    "text": "Resources\n\nCMSAC\nSCORE Network\nCRAN Task View: Sports Analytics\nSportsDataverse\nNFL Big Data Bowl: 2019, 2020, 2021, 2022, 2023, 2024\nAnalyzing Baseball Data with R\nBig ideas in sports analytics and statistical tools for their investigation\nPlayer Tracking Data in Sports\nHandbook of Statistical Methods and Analyses in Sports\nJournal of Quantitative Analysis in Sports\nOther conferences: NESSIS, CASSIS, CSAS\nRecent papers by CMSAC\n\nFractional tackles\nSTRAIN\nAugmented adjusted plus-minus (and Intraocular)\nGoing deep\nnflWAR\n\n\nLooking for the 2023, 2022, 2021, or 2020 websites?"
  },
  {
    "objectID": "health/labs/02-wrangling.html",
    "href": "health/labs/02-wrangling.html",
    "title": "Lab: data wrangling",
    "section": "",
    "text": "Our data are usually stored as a .csv file and after loading a .csv file into RStudio, we will have a “data frame”. A data frame can be considered a special case of matrix where each column represents a measurement or variable of interest for each observation which correspond to the rows of the dataset. After loading the tidyverse suite of packages, we use the read_csv() function to load the heart_disease dataset from yesterday:\n\nlibrary(tidyverse)\n\n── Attaching core tidyverse packages ──────────────────────── tidyverse 2.0.0 ──\n✔ dplyr     1.1.4     ✔ readr     2.1.5\n✔ forcats   1.0.0     ✔ stringr   1.5.1\n✔ ggplot2   3.5.1     ✔ tibble    3.2.1\n✔ lubridate 1.9.3     ✔ tidyr     1.3.1\n✔ purrr     1.0.2     \n── Conflicts ────────────────────────────────────────── tidyverse_conflicts() ──\n✖ dplyr::filter() masks stats::filter()\n✖ dplyr::lag()    masks stats::lag()\nℹ Use the conflicted package (&lt;http://conflicted.r-lib.org/&gt;) to force all conflicts to become errors\n\nheart_disease &lt;- read_csv(\"https://raw.githubusercontent.com/36-SURE/36-SURE.github.io/main/data/heart_disease.csv\")\n\nRows: 788 Columns: 10\n── Column specification ────────────────────────────────────────────────────────\nDelimiter: \",\"\nchr (1): Gender\ndbl (9): Cost, Age, Interventions, Drugs, ERVisit, Complications, Comorbidit...\n\nℹ Use `spec()` to retrieve the full column specification for this data.\nℹ Specify the column types or set `show_col_types = FALSE` to quiet this message.\n\n\nBy default, read_csv() reads in the dataset as a tbl (aka tibble) object instead of a data.frame object. You can read about the differences here, but it’s not that meaningful for purposes.\nWe can use the functions slice_head() and slice_tail() to view a sample of the data. Use the slice_head() function to view the first 6 rows, then use the slice_tail() function to view the last 3 rows:\n\n# INSERT CODE HERE\n\nView the dimensions of the data with dim():\n\n# INSERT CODE HERE\n\nQuickly view summary statistics for all variables with the summary() function:\n\n# Uncomment the following code by deleting the # at the front\n# summary(heart_disease)\n\nView the data structure types with str():\n\n# str(heart_disease)\n\nWhat’s the difference between the output from the two functions?\nYou can find a description of the dataset here."
  },
  {
    "objectID": "health/labs/02-wrangling.html#reading-and-previewing-data",
    "href": "health/labs/02-wrangling.html#reading-and-previewing-data",
    "title": "Lab: data wrangling",
    "section": "",
    "text": "Our data are usually stored as a .csv file and after loading a .csv file into RStudio, we will have a “data frame”. A data frame can be considered a special case of matrix where each column represents a measurement or variable of interest for each observation which correspond to the rows of the dataset. After loading the tidyverse suite of packages, we use the read_csv() function to load the heart_disease dataset from yesterday:\n\nlibrary(tidyverse)\n\n── Attaching core tidyverse packages ──────────────────────── tidyverse 2.0.0 ──\n✔ dplyr     1.1.4     ✔ readr     2.1.5\n✔ forcats   1.0.0     ✔ stringr   1.5.1\n✔ ggplot2   3.5.1     ✔ tibble    3.2.1\n✔ lubridate 1.9.3     ✔ tidyr     1.3.1\n✔ purrr     1.0.2     \n── Conflicts ────────────────────────────────────────── tidyverse_conflicts() ──\n✖ dplyr::filter() masks stats::filter()\n✖ dplyr::lag()    masks stats::lag()\nℹ Use the conflicted package (&lt;http://conflicted.r-lib.org/&gt;) to force all conflicts to become errors\n\nheart_disease &lt;- read_csv(\"https://raw.githubusercontent.com/36-SURE/36-SURE.github.io/main/data/heart_disease.csv\")\n\nRows: 788 Columns: 10\n── Column specification ────────────────────────────────────────────────────────\nDelimiter: \",\"\nchr (1): Gender\ndbl (9): Cost, Age, Interventions, Drugs, ERVisit, Complications, Comorbidit...\n\nℹ Use `spec()` to retrieve the full column specification for this data.\nℹ Specify the column types or set `show_col_types = FALSE` to quiet this message.\n\n\nBy default, read_csv() reads in the dataset as a tbl (aka tibble) object instead of a data.frame object. You can read about the differences here, but it’s not that meaningful for purposes.\nWe can use the functions slice_head() and slice_tail() to view a sample of the data. Use the slice_head() function to view the first 6 rows, then use the slice_tail() function to view the last 3 rows:\n\n# INSERT CODE HERE\n\nView the dimensions of the data with dim():\n\n# INSERT CODE HERE\n\nQuickly view summary statistics for all variables with the summary() function:\n\n# Uncomment the following code by deleting the # at the front\n# summary(heart_disease)\n\nView the data structure types with str():\n\n# str(heart_disease)\n\nWhat’s the difference between the output from the two functions?\nYou can find a description of the dataset here."
  },
  {
    "objectID": "health/labs/02-wrangling.html#data-manipulation-with-dplyr",
    "href": "health/labs/02-wrangling.html#data-manipulation-with-dplyr",
    "title": "Lab: data wrangling",
    "section": "Data manipulation with dplyr",
    "text": "Data manipulation with dplyr\nAn easier way to manipulate the data frame is through the dplyr package, which is in the tidyverse suite of packages. The operations we can do include: selecting specific columns, filtering for rows, re-ordering rows, adding new columns and summarizing data. The “split-apply-combine” concept can be achieved by dplyr.\n\nSelecting columns with select()\nThe function select() can be use to select certain column with the column names. First create a new table called heart_disease_ad that only contains the Age and Drugs columns:\n\n# INSERT CODE HERE\n\nTo select all columns except a specific column, use the - (subtraction) operator. For example, view the output from uncommenting the following line of code:\n\n# select(heart_disease, -Interventions)\n\nTo select a range of columns by name (that are in consecutive order), use the : (colon) operator. For example, view the output from uncommenting the following line of code:\n\n# select(heart_disease, Drugs:Duration)\n\nTo select all columns that start with certain character strings, use the function starts_with(). Other matching options are:\n\nends_with(): select columns that end with a character string\ncontains(): select columns that contain a character string\nmatches(): select columns that match a regular expression\none_of(): select columns names that are from a group of names\n\n\n# Uncomment the following lines of code\n# select(heart_disease, starts_with(\"Com\"))\n# select(heart_disease, contains(\"er\"))\n\n\n\nExtracting rows using filter()\nWe can also extract the rows/observations that satisfy certain criteria. Try extracting the rows with Cost of more than 400:\n\n# INSERT CODE HERE\n\nWe can also filter on multiple criteria. Subset the rows with Age above 60 and the gender is “Male”:\n\n# INSERT CODE HERE\n\n\n\nArranging rows using arrange()\nTo arrange the data frame by a specific order we need to use the function arrange(). The default is by increasing order and the desc() will provide the decreasing order. First arrange the heart_disease table by Duration in ascending order:\n\n# INSERT CODE HERE\n\nNext by descending order:\n\n# INSERT CODE HERE\n\nTry combining a pipeline of select(), filter(), and arrange() steps together with the |&gt; operator by:\n\nSelecting the Age, Cost, ERVisit, and Duration columns,\nFilter to select only rows with Age above 60,\nSort by Duration in descending order\n\n\n# INSERT CODE HERE\n\n\n\nCreating new columns using mutate()\nSometimes the data does not include the variable that we are interested in and we need to manipulate the current variables to add new variables into the data frame. Create a new column cost_per_day by taking the Cost and dividing by Duration (reassign this output to the heart_disease table following the commented code chunk so this column is added to the table):\n\n# heart_disease &lt;- heart_disease |&gt;\n#   mutate(INSERT CODE HERE)\n\n\n\nCreating summaries with summarize()\nTo create summary statistics for a given column in the data frame, we can use summarize() function. Compute the mean, min, and max amount of Cost:\n\n# INSERT CODE HERE\n\nThe advantage of summarize() is more obvious if we combine it with group_by(), the group operators. Try to group_by() the Gender column first and then compute the same summary statistics for Cost:\n\n# INSERT CODE HERE"
  },
  {
    "objectID": "health/labs/03-visualization.html",
    "href": "health/labs/03-visualization.html",
    "title": "Lab: data visualization",
    "section": "",
    "text": "Let’s start again by reading in the data from yesterday using the read_csv() function after loading the tidyverse:\n\nlibrary(tidyverse)\n\n── Attaching core tidyverse packages ──────────────────────── tidyverse 2.0.0 ──\n✔ dplyr     1.1.4     ✔ readr     2.1.5\n✔ forcats   1.0.0     ✔ stringr   1.5.1\n✔ ggplot2   3.5.1     ✔ tibble    3.2.1\n✔ lubridate 1.9.3     ✔ tidyr     1.3.1\n✔ purrr     1.0.2     \n── Conflicts ────────────────────────────────────────── tidyverse_conflicts() ──\n✖ dplyr::filter() masks stats::filter()\n✖ dplyr::lag()    masks stats::lag()\nℹ Use the conflicted package (&lt;http://conflicted.r-lib.org/&gt;) to force all conflicts to become errors\n\nheart_disease &lt;- read_csv(\"https://raw.githubusercontent.com/36-SURE/36-SURE.github.io/main/data/heart_disease.csv\")\n\nRows: 788 Columns: 10\n── Column specification ────────────────────────────────────────────────────────\nDelimiter: \",\"\nchr (1): Gender\ndbl (9): Cost, Age, Interventions, Drugs, ERVisit, Complications, Comorbidit...\n\nℹ Use `spec()` to retrieve the full column specification for this data.\nℹ Specify the column types or set `show_col_types = FALSE` to quiet this message."
  },
  {
    "objectID": "health/labs/03-visualization.html#reading-in-data",
    "href": "health/labs/03-visualization.html#reading-in-data",
    "title": "Lab: data visualization",
    "section": "",
    "text": "Let’s start again by reading in the data from yesterday using the read_csv() function after loading the tidyverse:\n\nlibrary(tidyverse)\n\n── Attaching core tidyverse packages ──────────────────────── tidyverse 2.0.0 ──\n✔ dplyr     1.1.4     ✔ readr     2.1.5\n✔ forcats   1.0.0     ✔ stringr   1.5.1\n✔ ggplot2   3.5.1     ✔ tibble    3.2.1\n✔ lubridate 1.9.3     ✔ tidyr     1.3.1\n✔ purrr     1.0.2     \n── Conflicts ────────────────────────────────────────── tidyverse_conflicts() ──\n✖ dplyr::filter() masks stats::filter()\n✖ dplyr::lag()    masks stats::lag()\nℹ Use the conflicted package (&lt;http://conflicted.r-lib.org/&gt;) to force all conflicts to become errors\n\nheart_disease &lt;- read_csv(\"https://raw.githubusercontent.com/36-SURE/36-SURE.github.io/main/data/heart_disease.csv\")\n\nRows: 788 Columns: 10\n── Column specification ────────────────────────────────────────────────────────\nDelimiter: \",\"\nchr (1): Gender\ndbl (9): Cost, Age, Interventions, Drugs, ERVisit, Complications, Comorbidit...\n\nℹ Use `spec()` to retrieve the full column specification for this data.\nℹ Specify the column types or set `show_col_types = FALSE` to quiet this message."
  },
  {
    "objectID": "health/labs/03-visualization.html#previewing-the-data",
    "href": "health/labs/03-visualization.html#previewing-the-data",
    "title": "Lab: data visualization",
    "section": "Previewing the data",
    "text": "Previewing the data\nWrite code that displays the column names of heart_disease. Also, look at the first six rows of your dataset to get an idea of what these variables look like. Which variables are quantitative, and which are categorical?\n\n# INSERT CODE HERE\n\nAs it turns out, even though Drugs and Complications appear to be quantitative - they are actually categorical variables. Specifically, Drugs represents the categorized number of drugs prescribed: 0 if none, 1 if one, 2 if more than one; Complications indicates whether or not the subscriber had complications: 1 if yes, 0 if no. To address this issue for our plots, we can manually recode the variables as factors. For instance, we can modify the Complications variable using a simple if-else statement:\n\nheart_disease &lt;- heart_disease |&gt;\n  mutate(Complications = ifelse(Complications == 0, \"No\", \"Yes\"))\n\nThis is a quick fix to the binary indicator variable since, by default, R orders factor variables in alphabetical order. In this case, “No” is before “Yes” because “N” is before “Y”. We may not want variables in alphabetical order however - we will see how to change this in lecture.\nNext, to update the Drugs variable we will use the fct_recode() function which allows us to manually change the labels of a factor variable:\n\nheart_disease &lt;- heart_disease |&gt;\n  mutate(Drugs = fct_recode(as.factor(Drugs), \n                            \"None\" = \"0\", \"One\" = \"1\", \"More than one\" = \"2\"))\n\nWhy did we have to specify as.factor(Drugs) first then place the numbers in quotation marks?"
  },
  {
    "objectID": "health/labs/03-visualization.html#always-make-a-bar-chart",
    "href": "health/labs/03-visualization.html#always-make-a-bar-chart",
    "title": "Lab: data visualization",
    "section": "Always make a bar chart…",
    "text": "Always make a bar chart…\nNow we’ll use the ggplot() function to create a bar chart of the Drugs variable. To make things easier, we provide the code for you to do this below; just uncomment the code and run it to create the bar chart. In what follows, you must answer some questions about the code and plot.\n\n# Create the bar chart of Drugs:\n# heart_disease |&gt;\n#   ggplot(aes(x = Drugs)) +\n#   geom_bar(fill = \"darkblue\") +\n#   labs(title = \"Number of patients by number of drugs\",\n#        x = \"Number of drugs\", \n#        y = \"Number of patients\")\n\nAnswer the following questions about the code and plot:\n\nIn general, ggplot() code takes the following format: ggplot(blank1, aes(x = blank2)). Looking at the above code, what kind of R object should blank1 be, and what should blank2 be?\nWhat do you think the line geom_bar(fill = \"darkblue\") does?\nWhat do you think the remaining lines of code do (contained in labs())?"
  },
  {
    "objectID": "health/labs/03-visualization.html#more-area-plots-but-bar-charts-are-better",
    "href": "health/labs/03-visualization.html#more-area-plots-but-bar-charts-are-better",
    "title": "Lab: data visualization",
    "section": "More area plots (but bar charts are better!)",
    "text": "More area plots (but bar charts are better!)\nNow we’ll make a few other area plots:\n\nspine chart\npie chart\nrose diagram\n\nYour goal for this part is to create each of these plots. These plots can be created by copy-and-pasting the bar chart code from above and modifying it slightly. Follow these directions to create each of these plots:\n\nspine chart: First, copy-and-paste the bar chart code from above. Then, delete the fill = \"darkblue\" within geom_bar(). Finally, within ggplot(), replace aes(x = Drugs) with aes(x = \"\", fill = Drugs). Also, change the labels in labs() if necessary.\n\n\n# PUT YOUR SPINE CHART CODE HERE\n\n\npie chart: First, copy-and-paste the spine chart code you just made. Then, after geom_bar(), “add” coord_polar(\"y\"). Be sure to put plus signs before and after coord_polar(\"y\"). Also, change the labels in labs() if necessary.\n\n\n# PUT YOUR PIE CHART CODE HERE\n\n\nrose diagram: First, copy-and-paste your original bar chart code. Then, after geom_bar(fill = \"darkblue\"), “add” coord_polar() + scale_y_sqrt(). Be sure to put plus signs before and after coord_polar() + scale_y_sqrt(). Also, change the labels in labs() if necessary. After you make the rose diagram: In 1-2 sentences, what do you think scale_y_sqrt() does, and what is a benefit to including scale_y_sqrt() when making the rose diagram?\n\n\n# PUT YOUR ROSE DIAGRAM CODE HERE"
  },
  {
    "objectID": "health/labs/03-visualization.html#notes-on-colors-in-plots",
    "href": "health/labs/03-visualization.html#notes-on-colors-in-plots",
    "title": "Lab: data visualization",
    "section": "Notes on colors in plots",
    "text": "Notes on colors in plots\nThree types of color scales to work with:\n\nQualitative: distinguishing discrete items that don’t have an order (nominal categorical). Colors should be distinct and equal with none standing out unless otherwise desired for emphasis.\n\n\nDo NOT use a discrete scale on a continuous variable\n\n\nSequential: when data values are mapped to one shade, e.g., for an ordered categorical variable or low to high continuous variable\n\n\nDo NOT use a sequential scale on an unordered variable\n\n\nDivergent: think of it as two sequential scales with a natural midpoint midpoint could represent 0 (assuming +/- values) or 50% if your data spans the full scale\n\n\nDo NOT use a divergent scale on data without natural midpoint\n\n\nOptions for ggplot2 colors\nThe default color scheme is pretty bad to put it bluntly, but ggplot2 has ColorBrewer built in which makes it easy to customize your color scales. For instance, we can make a scatterplot with Cost on the y-axis and Duration on the x-axis and using the geom_point() layer with each point colored by Drugs:\n\nheart_disease |&gt;\n  ggplot(aes(x = Duration, y = Cost, color = Drugs)) +\n  geom_point(alpha = 0.5) +\n  labs(x = \"Duration\", \n       y = \"Cost\",\n       color = \"Number of drugs\") +\n  theme_light()\n\n\n\n\nWhat does alpha change? We can change the color plot for this plot using scale_color_brewer() function:\n\nheart_disease |&gt;\n  ggplot(aes(x = Duration, y = Cost, color = Drugs)) +\n  geom_point(alpha = 0.5) +\n  scale_color_brewer(palette = \"Set2\") +\n  labs(x = \"Duration\", \n       y = \"Cost\",\n       color = \"Number of drugs\") +\n  theme_light()\n\n\n\n\nWhich do you prefer, the default palette or this new one? You can check out more color palettes here.\nSomething you should keep in mind is to pick a color-blind friendly palette. One simple way to do this is by using the ggthemes package (you need to install it first before running this code!) which has color-blind friendly palettes included:\n\nheart_disease |&gt;\n  ggplot(aes(x = Duration, y = Cost, color = Drugs)) +\n  geom_point(alpha = 0.5) +\n  # call the function directly from the package using `::` instead of library(ggthemes)\n  ggthemes::scale_color_colorblind() +\n  labs(x = \"Duration\", \n       y = \"Cost\",\n       color = \"Number of drugs\") +\n  theme_light()\n\n\n\n\nIn terms of displaying color from low to high, the viridis scales are excellent choices (and are also color-blind friendly!). For instance, we can map another quantitative variable (Interventions) to the color:\n\nheart_disease |&gt;\n  ggplot(aes(x = Duration, y = Cost, color = Interventions)) +\n  geom_point(alpha = 0.5) +\n  scale_color_viridis_c() +\n  labs(x = \"Duration\", \n       y = \"Cost\",\n       color = \"Interventions\") +\n  theme_light()\n\n\n\n\nWhat does this reveal about the plot? What happens if you delete scale_color_viridis_c() + from above? Which do you prefer?"
  },
  {
    "objectID": "health/labs/03-visualization.html#notes-on-themes",
    "href": "health/labs/03-visualization.html#notes-on-themes",
    "title": "Lab: data visualization",
    "section": "Notes on themes",
    "text": "Notes on themes\nYou might have noticed above have various changes to the theme of plots for customization. You will constantly be changing the theme of your plots to optimize the display. Fortunately, there are a number of built-in themes you can use to start with rather than the default theme_gray():\n\nheart_disease |&gt;\n  ggplot(aes(x = Duration, y = Cost, color = Interventions)) +\n  geom_point(alpha = 0.5) +\n  scale_color_viridis_c() +\n  labs(x = \"Duration\", \n       y = \"Cost\",\n       color = \"Interventions\") +\n  theme_gray()\n\n\n\n\nFor instance, Quang’s go-to theme is theme_light()\n\nheart_disease |&gt;\n  ggplot(aes(x = Duration, y = Cost, color = Interventions)) +\n  geom_point(alpha = 0.5) +\n  scale_color_viridis_c() +\n  labs(x = \"Duration\", \n       y = \"Cost\",\n       color = \"Interventions\") +\n  theme_light()\n\n\n\n\nThere are options such as theme_minimal():\n\nheart_disease |&gt;\n  ggplot(aes(x = Duration, y = Cost, color = Interventions)) +\n  geom_point(alpha = 0.5) +\n  scale_color_viridis_c() +\n  labs(x = \"Duration\", \n       y = \"Cost\",\n       color = \"Interventions\") +\n  theme_minimal()\n\n\n\n\nor theme_classic():\n\nheart_disease |&gt;\n  ggplot(aes(x = Duration, y = Cost, color = Interventions)) +\n  geom_point(alpha = 0.5) +\n  scale_color_viridis_c() +\n  labs(x = \"Duration\", \n       y = \"Cost\",\n       color = \"Interventions\") +\n  theme_classic()\n\n\n\n\nor theme_bw():\n\nheart_disease |&gt;\n  ggplot(aes(x = Duration, y = Cost, color = Interventions)) +\n  geom_point(alpha = 0.5) +\n  scale_color_viridis_c() +\n  labs(x = \"Duration\", \n       y = \"Cost\",\n       color = \"Interventions\") +\n  theme_bw()\n\n\n\n\nThere are also packages with popular, such as the ggthemes package which includes, for example, theme_economist():\n\nlibrary(ggthemes)\nheart_disease |&gt;\n  ggplot(aes(x = Duration, y = Cost, color = Interventions)) +\n  geom_point(alpha = 0.5) +\n  scale_color_viridis_c() +\n  labs(x = \"Duration\", \n       y = \"Cost\",\n       color = \"Interventions\") +\n  theme_economist()\n\n\n\n\nand theme_fivethirtyeight(), to name a couple:\n\nheart_disease |&gt;\n  ggplot(aes(x = Duration, y = Cost, color = Interventions)) +\n  geom_point(alpha = 0.5) +\n  scale_color_viridis_c() +\n  labs(x = \"Duration\", \n       y = \"Cost\",\n       color = \"Interventions\") +\n  theme_fivethirtyeight()\n\n\n\n\nWith any theme you have picked, you can then modify specific components directly using the theme() layer. There are many aspects of the plot’s theme to modify, such as my decision to move the legend to the bottom of the figure, drop the legend title, and increase the font size for the y-axis:\n\nheart_disease |&gt;\n  ggplot(aes(x = Duration, y = Cost, color = Interventions)) +\n  geom_point(alpha = 0.5) +\n  scale_color_viridis_c() +\n  labs(x = \"Duration\", \n       y = \"Cost\",\n       title = \"Joint distribution of patients' duration and cost\",\n       color = \"Interventions\") +\n  theme_light() +\n  theme(legend.position = \"bottom\",\n        legend.title = element_blank(),\n        axis.text.y = element_text(size = 14),\n        axis.text.x = element_text(size = 6))\n\n\n\n\nIf you’re tired of explicitly customizing every plot in the same way all the time, then you should make a custom theme. It’s quite easy to make a custom theme for ggplot2 and of course there are an incredible number of ways to customize your theme. Below, we modify theme_bw() using the %+replace% argument to a new customized theme named theme_cus() - which is stored as a function:\n\ntheme_cus &lt;- function() {\n  # start with the base font size\n  theme_bw(base_size = 10) %+replace%\n    theme(\n      panel.background  = element_blank(),\n      plot.background = element_rect(fill = \"transparent\", color = NA), \n      legend.position = \"bottom\",\n      legend.background = element_rect(fill = \"transparent\", color = NA),\n      legend.key = element_rect(fill = \"transparent\", color = NA),\n      axis.ticks = element_blank(),\n      panel.grid.major = element_line(color = \"grey90\", linewidth = 0.3), \n      panel.grid.minor = element_blank(),\n      plot.title = element_text(size = 15, hjust = 0, vjust = 0.5, face = \"bold\", \n                                margin = margin(b = 0.2, unit = \"cm\")),\n      plot.subtitle = element_text(size = 12, hjust = 0, vjust = 0.5, \n                                   margin = margin(b = 0.2, unit = \"cm\")),\n      plot.caption = element_text(size = 7, hjust = 1, face = \"italic\", \n                                  margin = margin(t = 0.1, unit = \"cm\")),\n      axis.text.x = element_text(size = 13),\n      axis.text.y = element_text(size = 13)\n    )\n}\n\nCreate the plot from before with this theme:\n\nheart_disease |&gt;\n  ggplot(aes(x = Duration, y = Cost,\n             color = Interventions)) +\n  geom_point(alpha = 0.5) +\n  scale_color_viridis_c() +\n  labs(x = \"Duration\", \n       y = \"Cost\",\n       title = \"Joint distribution of patients' duration and cost\",\n       color = \"Interventions\") +\n  theme_cus()"
  },
  {
    "objectID": "health/labs/05-regression.html",
    "href": "health/labs/05-regression.html",
    "title": "Lab: linear regression",
    "section": "",
    "text": "Execute the following code chunk to load the heart disease dataset we worked with before in previous labs\n\nlibrary(tidyverse)\nheart_disease &lt;- read_csv(\"https://raw.githubusercontent.com/36-SURE/36-SURE.github.io/main/data/heart_disease.csv\")\n\nThis dataset consists of 788 heart disease patients (608 women, 180 men). Your goal is to predict the Cost column, which corresponds to the patient’s total cost of claims by subscriber (i.e., Cost is the response variable). You have access to the following explanatory variables:\n\nAge: Age of subscriber (years)\nGender: Gender of subscriber\nInterventions: Total number of interventions or procedures carried out\nDrugs: Categorized number of drugs prescribed: 0 if none, 1 if one, 2 if more than one\nERVisit: Number of emergency room visits\nComplications: Whether or not the subscriber had complications: 1 if yes, 0 if no\nComorbidities: Number of other diseases that the subscriber had\nDuration: Number of days of duration of treatment condition"
  },
  {
    "objectID": "health/labs/05-regression.html#data",
    "href": "health/labs/05-regression.html#data",
    "title": "Lab: linear regression",
    "section": "",
    "text": "Execute the following code chunk to load the heart disease dataset we worked with before in previous labs\n\nlibrary(tidyverse)\nheart_disease &lt;- read_csv(\"https://raw.githubusercontent.com/36-SURE/36-SURE.github.io/main/data/heart_disease.csv\")\n\nThis dataset consists of 788 heart disease patients (608 women, 180 men). Your goal is to predict the Cost column, which corresponds to the patient’s total cost of claims by subscriber (i.e., Cost is the response variable). You have access to the following explanatory variables:\n\nAge: Age of subscriber (years)\nGender: Gender of subscriber\nInterventions: Total number of interventions or procedures carried out\nDrugs: Categorized number of drugs prescribed: 0 if none, 1 if one, 2 if more than one\nERVisit: Number of emergency room visits\nComplications: Whether or not the subscriber had complications: 1 if yes, 0 if no\nComorbidities: Number of other diseases that the subscriber had\nDuration: Number of days of duration of treatment condition"
  },
  {
    "objectID": "health/labs/05-regression.html#eda",
    "href": "health/labs/05-regression.html#eda",
    "title": "Lab: linear regression",
    "section": "1. EDA",
    "text": "1. EDA\nSpend time exploring the dataset, to visually assess which of the explanatory variables listed above is most associated with our response Cost. Create scatterplots between the response and each continuous explanatory variable (either Interventions, ERVist, Comorbidities, or Duration). Does any of the relationships appear to be linear? Describe the direction and strength of the association between the explanatory and response variables.\nIn your opinion, which of the possible continuous explanatory variables displays the strongest relationship with cost?"
  },
  {
    "objectID": "health/labs/05-regression.html#fit-a-simple-linear-model",
    "href": "health/labs/05-regression.html#fit-a-simple-linear-model",
    "title": "Lab: linear regression",
    "section": "2. Fit a simple linear model",
    "text": "2. Fit a simple linear model\nNow that you’ve performed some EDA, it’s time to actually fit some linear models to the data. Start the variable you think displays the strongest relationship with the response variable. Update the following code by replacing INSERT_VARIABLE with your selected variable, and run to fit the model:\n\ninit_cost_lm &lt;- lm(Cost ~ INSERT_VARIABLE, data = heart_disease)\n\nBefore check out the summary() of this model, you need to check the diagnostics to see if it meets the necessary assumptions. To do this you can try running plot(init_nba_lm) in the console (what happens?). Equivalently, another way to make the same plots but with ggplot2 perks is with the ggfortify package by running the following code:\n\nlibrary(ggfortify) # install.packages(\"ggfortify\")\ninit_cost_lm |&gt; \n  autoplot() +\n  theme_light()\n\nThe first plot is residuals vs. fitted: this plot should NOT display any clear patterns in the data, no obvious outliers, and be symmetric around the horizontal line at zero. The smooth line provided is just for reference to see how the residual average changes. Do you see any obvious patterns in your plot for this model?\nThe second plot is a Q-Q plot (see page 93 for more details). Without getting too much into the math behind them, the closer the observations are to the dashed reference line, the better your model fit is. It is bad for the observations to diverge from the dashed line in a systematic way; that means we are violating the assumption of normality discussed in lecture. How do your points look relative to the dashed reference line?\nThe third plot looks at the square root of the absolute value of the standardized residuals. We want to check for homoskedascity of errors (equal, constant variance). If we did have constant variance, what would we expect to see? What does your plot look like?\nThe fourth plot is residuals vs. leverage which helps us identify influential points. Leverage quantifies the influence the observed response for a particular observation has on its predicted value, i.e. if the leverage is small then the observed response has a small role in the value of its predicted response, while a large leverage indicates the observed response plays a large role in the predicted response. It’s a value between 0 and 1, where the sum of all leverage values equals the number of coefficients (including the intercept). Specifically the leverage for observation \\(i\\) is computed as \\[\nh_{ii} = \\frac{1}{n} + \\frac{(x_i - \\bar{x})^2}{\\sum_i^n (x_i - \\bar{x})^2}\n\\] where \\(\\bar{x}\\) is the average value for variable \\(x\\) across all observations. See page 191 for more details on leverage and the regression hat matrix. We’re looking for points in the upper right or lower right corners, where dashed lines for Cook’s distance values would indicate potential outlier points that are displaying too much influence on the model results. Do you observed any such influential points in upper or lower right corners?\nWhat is your final assessment of the diagnostics, do you believe all assumptions are met? Any potential outlier observations to remove?"
  },
  {
    "objectID": "health/labs/05-regression.html#transform-the-cost-variable",
    "href": "health/labs/05-regression.html#transform-the-cost-variable",
    "title": "Lab: linear regression",
    "section": "3. Transform the Cost variable",
    "text": "3. Transform the Cost variable\nAn obvious result from looking at the residual diagnostics above is that we are clearly violating the normality assumption. Why do you think we’re violating this assumption? (HINT: Display a histogram of the Cost variable.)\nOne way of addressing this concern is to apply a transformation to the response variable, in this case Cost. A common transformation for any type of dollar amount is to use the log() transformation. Run the following code chunk to create a new log_cost variable that we will use for the remainder of the lab.\n\nheart_disease &lt;- heart_disease |&gt; \n  mutate(log_cost = log(Cost + 1))\n\nWhy did we need to + 1 before taking the log()? (HINT: Look at the minimum of Cost.) Now make another histogram, this time for the new log_cost variable. What happened to the distribution?"
  },
  {
    "objectID": "health/labs/05-regression.html#assess-the-model-summary",
    "href": "health/labs/05-regression.html#assess-the-model-summary",
    "title": "Lab: linear regression",
    "section": "4. Assess the model summary",
    "text": "4. Assess the model summary\nNow fit the same model as before using the following code chunk. Update the following code by replacing INSERT_VARIABLE with your selected variable, and run to fit the model:\n\nlog_cost_lm &lt;- lm(log_cost ~ INSERT_VARIABLE, data = heart_disease)\n\nFollowing the example in lecture, interpret the results from the summary() function on your initial model. Do you think there is sufficient evidence to reject the null hypothesis that the coefficient is 0? What is the interpretation of the \\(R^2\\) value? Compare the square root of the raw (unadjusted) \\(R^2\\) of your linear model to the correlation between that explanatory variable and the response using the cor() function. What do you notice?\nTo assess the fit of a linear model, we can also plot the predicted values vs the actual values, to see how closely our predictions align with reality, and to decide whether our model is making any systematic errors. Execute the following code chunk to show the actual log(Cost) against our model’s predictions:\n\nheart_disease |&gt;\n  mutate(model_preds = predict(log_cost_lm)) |&gt;\n  ggplot(aes(x = model_preds, y = log_cost)) +\n  geom_point(alpha = 0.75) +\n  geom_abline(slope = 1, intercept = 0, linetype = \"dashed\", color = \"red\") +\n  labs(x = \"Predictions\", y = \"Observed log(Cost + 1)\") +\n  theme_light()\n\n\n5. Include multiple covariates\nRepeat steps 2 and 3 above but including more than one variable in your model. You can easily do this in the lm() function by adding another variable to the formula with the + operator as so (but just replace the INSERT_VARIABLE parts):\n\nmulti_cost_lm &lt;- lm(log_cost ~ INSERT_VARIABLE_1 + INSERT_VARIABLE_2, data = heart_disease)\n\nExperiment with different sets of the continuous variables What sets of continuous variables do you think models log(Cost) best? (Remember to use the Adjusted \\(R^2\\) when comparing models that have different numbers of variables).\nBeware of collinearity! Load the car library (install it if necessary!) and use the vif() function to check for possible (multi)collinearity. The vif() function computes the variance inflation factor (VIF) for predictor \\(x_j\\) with \\(j \\in 1,\\dots, p\\) as \\[\n\\text{VIF}_j = \\frac{1}{1 - R^2_j}\n\\] where \\(R^2_j\\) is the \\(R^2\\) from a variable with variable \\(x_j\\) as the response and the other \\(p-1\\) predictors as the explanatory variables. VIF values close to 1 indicate the variable is not correlated with other predictors, while VIF values over 5 indicate strong presence of collinearity. If present, remove a variable with VIF over 5, and redo the fit. Repeat this process until the vif() outputs are all less than 5. The follow code chunk displays an example of using this function:\n\nlibrary(car) # install.packages(\"car\")\nvif(multi_cost_lm)"
  },
  {
    "objectID": "health/labs/05-regression.html#linear-model-with-one-categorical-variable",
    "href": "health/labs/05-regression.html#linear-model-with-one-categorical-variable",
    "title": "Lab: linear regression",
    "section": "6. Linear model with one categorical variable",
    "text": "6. Linear model with one categorical variable\nRun the following code to fit a model using only the Gender variable:\n\ngender_cost_lm &lt;- lm(log_cost ~ Gender, data = heart_disease)\n\nNext, use the following code to first create a column called model_preds containing the predictions of the model above, to display the predictions of this model against the actual log_cast, but facet by the patient’s gender:\n\nheart_disease |&gt;\n  mutate(model_preds = predict(gender_cost_lm)) |&gt;\n  ggplot(aes(x = log_cost, y = model_preds)) +\n  geom_point(alpha = 0.5) +\n  facet_wrap(~ Gender, ncol = 2) +\n  labs(x = \"Actual log(Cost + 1)\", \n       y = \"Predicted log(Cost + 1)\") +\n  theme_light()\n\nAs the figure above, we are changing the intercept of our regression line by including a categorical variable. To make this more clear, view the output of the summary:\n\nsummary(gender_cost_lm)\n\nNotice how only one coefficient is provided in addition to the intercept. This is because, by default, R turns the categorical variables of \\(m\\) levels (e.g., we have 2 genders in this dataset) into \\(m - 1\\) indicator variables (binary with values of 1 if in that level versus 0 if not that level) for different categories relative to a baseline level. In this example, R has created an indicator for one gender: Male. By default, R will use alphabetical order to determine the baseline category, which in this example is the gender Female. The values for the coefficient estimates indicate the expected change in the response variable relative to the baseline. In other words, the intercept term gives us the baseline’s average y, e.g. the average log(Cost) for male patients. This matches what you displayed in the predictions against observed log_cost scatterplots by Gender above.\nBeware the default baseline R picks for categorical variables! We typically want to choose the baseline level to be the group with the most observations. In this example, Female has the most number of observations so the default was appropriate. But in general, we can change the reference level by modifying the factor levels of the categorical variables (similar to how we reorder things in ggplot2). For example, we can use the following code to modify the Gender variable so that Male is the baseline (we use fct_relevel() to update Gender so that Male is the first factor level - and we do not need to modify the order of the remaining levels):\n\nheart_disease &lt;- heart_disease |&gt;\n  mutate(Gender = fct_relevel(Gender, \"Male\")) \n\nRefit the linear regression model using Gender above, how has the summary changed?\nAfter you refit the model above, change the reference level back to Female with the following code:\n\nheart_disease &lt;- heart_disease |&gt;\n  mutate(Gender = fct_relevel(Gender, \"Female\")) \n\n\n7. Linear model with one categorical and one continuous variable\nPick a single continuous variable from yesterday, use it to replace INSERT_VARIABLE below, then run the code to fit a model with the Gender included:\n\nx_gender_cost_lm &lt;- lm(log_cost ~ Gender + INSERT_VARIABLE, data = heart_disease)\n\nCreate scatterplots with your predictions on the y-axis, your INSERT_VARIABLE on the x-asis, and color by Gender. What do you observe?\n\n\n8. Collapsing categorical variables\nAnother categorical we have access to is the Drugs variable, which is currently coded as numeric. We can first use the fct_recode() function to modify the Drugs variable so that the integers are relabeled:\n\nheart_disease &lt;- heart_disease |&gt;\n  mutate(Drugs = fct_recode(as.factor(Drugs), \"None\" = \"0\", \"One\" = \"1\", \"&gt; One\" = \"2\"))\n\nRun the following code to fit a model using only the Drugs variable:\n\ndrugs_cost_lm &lt;- lm(log_cost ~ Drugs, data = heart_disease)\n\nRepeat the same from above that you considered for the Gender variable, viewing the predictions faceted by Drugs and assess the summary() output. Do you think an appropriate reference level was used? (HINT: Use the table() function on the Drugs variable to view the overall frequency of each level and determine if the most frequent level was used as the reference.)\nGiven the similar values, we may decide to collapse the level of One and &gt; One into a single level &gt;= One. We can easily collapse the levels together into a smaller number of categories using fct_collapse():\n\nheart_disease &lt;- heart_disease |&gt;\n  mutate(drugs_group = fct_collapse(Drugs, \"None\" = \"None\", \"&gt;= One\" = c(\"One\", \"&gt; One\"))) \n\nRefit the model with this new drugs_group variable, but assign it to a different name, e.g. drugs_group_cost_lm. What changed in the summary?"
  },
  {
    "objectID": "health/labs/05-regression.html#interactions",
    "href": "health/labs/05-regression.html#interactions",
    "title": "Lab: linear regression",
    "section": "9. Interactions",
    "text": "9. Interactions\nRemember with ggplot2 you can directly compute and plot the results from running linear regression using geom_smooth() or stat_smooth() and specifying that method = \"lm\". Try running the following code (replace INSERT_VARIABLE) to generate the linear regression fits with geom_smooth versus your own model’s predictions (note the different y mapping for the point versus smooth layers):\n\nheart_disease |&gt;\n  mutate(model_preds = predict(x_gender_cost_lm)) |&gt;\n  ggplot(aes(x = INSERT_VARIABLE, color = Gender)) +\n  geom_point(aes(y = model_preds), alpha = 0.5) +\n  geom_smooth(aes(y = log_cost), method = \"lm\") \n  facet_wrap(~ Gender, ncol = 3) +\n  labs(x = \"INSERT YOUR LABEL HERE\", \n       y = \"Predicted log(Cost + 1)\") +\n  theme_light()\n\nThe geom_smooth() regression lines do NOT match! This is because ggplot2 is fitting separate regressions for each position, meaning the slope for the continuous variable on the x-axis is changing for each position. We can match the output of the geom_smooth() results with interactions. We can use interaction terms to build more complex models. Interaction terms allow for a different linear model to be fit for each category; that is, they allow for different slopes across different categories. If we believe relationships between continuous variables, and outcomes, differ across categories, we can use interaction terms to better model these relationships.\nTo fit a model with an interaction term between two variables, include the interaction via the * operator like so:\n\ngender_int_cost_lm &lt;- lm(log_cost ~ Gender * INSERT_VARIABLE, data = heart_disease)\n\nReplace the predictions in the previous plot’s mutate code with this interaction model’s predictions. How do they compare to the results from geom_smooth() now?\nYou can model interactions between any type of variables using the * operator, feel free to experiment on your different possible continuous variables.\n\n10. Polynomials\nAnother way to increase the explanatory power of your model is to include transformations of continuous variables. For instance you can directly create a column that is a square of a variable with mutate() and then fit the regression with the original variable and its squared term:\n\nheart_disease &lt;- heart_disease |&gt;\n  mutate(duration_squared = Duration ^ 2)\nsquared_duration_lm &lt;- lm(log_cost ~ Duration + duration_squared, data = heart_disease)\nsummary(squared_duration_lm)\n\nWhat are some difficulties with interpreting this model fit? View the predictions for this model or other covariates you squared.\n\n\n6. Training and testing\nAs we’ve seen, using transformations such as higher-order polynomials may decrease the interpretability and increase the potential for overfitting associated with our models; however, they can also dramatically improve the explanatory power.\nWe need a way for making sure our more complicated models have not overly fit to the noise present in our data. Another way of saying this is that a good model should generalize to a different sample than the one on which it was fit. This intuition motivates the idea of training/testing. We split our data into two parts, use one part – the training set – to fit our models, and the other part – the testing set – to evaluate our models. Any model which happens to fit to the noise present in our training data should perform poorly on our testing data.\nThe first thing we will need to do is split our sample. Run the following code chunk to divide our data into two halves, which we will refer to as a training set and a test set. Briefly summarize what each line in the code chunk is doing.\n\nheart_train &lt;- heart_disease |&gt; \n  slice_sample(prop = 0.5, replace = FALSE)\nheart_test &lt;- heart_disease |&gt; \n  anti_join(heart_train)\n\nWe will now compare three candidate models for predicting log_cost using Gender and Duration. We will fit these models on the training data only, ignoring the testing data for the moment. Run the below two code chunks to create two candidate models:\n\n# model with interaction term\ncandidate_model_1 &lt;- lm(log_cost ~ Gender * poly(Duration, 2, raw = TRUE), \n                        data = heart_train)\n\n\n# model with no interaction term\ncandidate_model_2 &lt;- lm(log_cost ~ Gender + poly(Duration, 2, raw = TRUE), \n                        data = heart_train)\n\n(Note: The poly() function is useful for getting higher-order polynomial transformations of variables.)\nUsing summary(), which of these models has more explanatory power according to the training data? Which of the models is less likely to overfit?\nFit another model to predict log_cost using a different set of variables or polynomials.\nNow that we’ve built our candidate models, we will evaluate them on our test set, using the criterion of mean squared error (MSE). Run the following code chunk to compute, on the test set, the MSE of predictions given by the first model compared to the actual log_cost.\n\nmodel_1_preds &lt;- predict(candidate_model_1, newdata = heart_test)\nmodel_1_mse &lt;- mean((model_1_preds - heart_test$log_cost) ^ 2)\n\nDo this for each of your candidate models. Compare the MSE on the test set, which model performed best (in terms of lowest test MSE)?"
  },
  {
    "objectID": "health/labs/01-intro.html",
    "href": "health/labs/01-intro.html",
    "title": "Lab: getting started with R",
    "section": "",
    "text": "NOTE: To preview this file, click the “Render” button in RStudio."
  },
  {
    "objectID": "health/labs/01-intro.html#installing-r-and-rstudio",
    "href": "health/labs/01-intro.html#installing-r-and-rstudio",
    "title": "Lab: getting started with R",
    "section": "Installing R and RStudio",
    "text": "Installing R and RStudio\n(Skip this part if you’ve already installed R and RStudio.)\nTo install R (latest release: 4.4.0), visit https://www.r-project.org and choose your system. Click on the download R link in the middle of the page under “Getting Started.” Download and install the installer files (executable, pkg, etc) that correspond to your system.\nAlthough you can use R without any integrated development environment (IDE), you will need to install RStudio, by far the most popular IDE for R, for this summer. Basically, it makes your life with R much easier and we will be using it throughout the program. To install RStudio, visit https://posit.co/download/rstudio-desktop and choose your system. The installer is preferred. If you have RStudio installed but not the latest version, just download the latest installer and install."
  },
  {
    "objectID": "health/labs/01-intro.html#typical-workflow",
    "href": "health/labs/01-intro.html#typical-workflow",
    "title": "Lab: getting started with R",
    "section": "Typical workflow",
    "text": "Typical workflow\n\nWriting R scripts\nYou can type R commands directly into the Console (lower left pane), but this can become quite tedious and annoying when your work becomes more complex. Instead, you can code in R Scripts. An R Script is a file type which R recognizes as storing R commands and is saved as a .R file. R Scripts are useful as we can edit our code before sending it to be run in the console.\nIn RStudio, to open a new R Script: File &gt; New File &gt; R Script.\n\n\nUsing Quarto\nAn Quarto file is a dynamic document for writing reproducible reports and communicating results. It contains the reproducible source code along with the narration that a reader needs to understand your work.\nThere are three important elements to a Quarto file:\n\nA YAML header at the top (surrounded by ---)\nChunks of R code surrounded by ```\nText mixed with simple text formatting like ## Heading and italics\n\n(Note that this file itself is a Quarto document.)\nIf you are familiar with the LaTeX syntax, math mode works like a charm in almost the same way:\n\\[\nf (x) = \\frac{1}{\\sqrt{2\\pi}} \\exp \\left( - \\frac{x^2}{2} \\right)\n\\]\nA chunk of embedded R code is the following:\n\n# R code here\nprint(\"Hello World\")\n\n[1] \"Hello World\"\n\n\nAll the lab documents will be Quarto files so you need to know how to render and convert them into a reader-friendly documents. We recommend to render as html file but if you have LaTeX installed, you can change the format to pdf.\nFor more details on Quarto, see the comprehensive manual online and the Quarto chapter of R for Data Science (2e). See also the guide on Markdown Basics for more on Markdown syntax. For code chunk options, see this guide."
  },
  {
    "objectID": "health/labs/01-intro.html#installing-r-packages",
    "href": "health/labs/01-intro.html#installing-r-packages",
    "title": "Lab: getting started with R",
    "section": "Installing R packages",
    "text": "Installing R packages\nR performs a wide variety of functions, such as data manipulation, modeling, and visualization. The extensive code base beyond the built-in functions are managed by packages created from numerous statisticians and developers. The Comprehensive R Archive Network (CRAN) manages the open-source distribution and the quality control of the R packages.\nTo install an R package, using the function install.packages and put the package name in the parentheses and the quote. While this is preferred, for those using RStudio, you can also go to “Tools” then “Install Packages” and then input the package name.\n\ninstall.packages(\"tidyverse\")\n\nImportant: NEVER install new packages in a code block in a .qmd file. That is, the install.packages() function should NEVER be in your code chunks (unless they are commented out using #). The library() function, however, will be used throughout your code: The library() function loads packages only after they are installed.\nIf in any time you get a message says: “Do you want to install from sources the package which needs compilation?” Choose “No” will tend to bring less troubles. (Note: This happens when the bleeding-edge version package is available, but not yet compiled for each OS distribution. In many case, you can just proceed without the source compilation.)\nEach package only needs to be installed once. Whenever you want to use functions defined in the package, you need to load the package with the command:\n\nlibrary(tidyverse)\n\nHere is a list of packages that we may need (but not limited to) in the following lectures and/or labs. Make sure you can install all of them. If you fail to install any package, please update R and RStudio first and check the error message for any other packages that need to install first.\n\nlibrary(tidyverse)\nlibrary(devtools)\nlibrary(ranger)\nlibrary(glmnet)"
  },
  {
    "objectID": "health/labs/01-intro.html#basic-data-type-and-operators",
    "href": "health/labs/01-intro.html#basic-data-type-and-operators",
    "title": "Lab: getting started with R",
    "section": "Basic data type and operators",
    "text": "Basic data type and operators\n\nData type: vector\nThe basic unit of R is a vector. A vector is a collection of values of the same type and the type could be:\n\nnumeric (double/integer number): digits with optional decimal point\n\n\nv1 &lt;- c(1, 5, 8.3, 0.02, 99999)\ntypeof(v1)\n\n[1] \"double\"\n\n\n\ncharacter: a string (or word) in double or single quotes, “…” or ’…’.\n\n\nv2 &lt;- c(\"apple\", \"banana\", \"3 chairs\", \"dimension1\", \"&gt;-&lt;\")\ntypeof(v2)\n\n[1] \"character\"\n\n\n\nlogical: TRUE and FALSE\n\n\nv3 &lt;- c(TRUE, FALSE, FALSE)\ntypeof(v3)\n\n[1] \"logical\"\n\n\nNote: Oftentimes, factor is used to encode a character vector into unique numeric vector.\n\nplayer_type &lt;- c(\"Batter\", \"Batter\", \"Hitter\", \"Batter\", \"Hitter\")\nplayer_type &lt;- factor(player_type)\nstr(player_type)\n\n Factor w/ 2 levels \"Batter\",\"Hitter\": 1 1 2 1 2\n\ntypeof(player_type)\n\n[1] \"integer\"\n\n\n\n\nData type: lists\nVector can store only single data type:\n\ntypeof(c(1, TRUE, \"apple\"))\n\n[1] \"character\"\n\n\nList is a vector of vectors which can store different data types of vectors:\n\nroster &lt;- list(\n  name = c(\"Quang\", \"Akshay\", \"Nick\", \"Princess\", \"Yuchen\", \"JungHo\", \"Daven\"),\n  role = c(\"Instructor\", \"TA\", \"TA\", \"TA\", \"TA\", \"TA\", \"TA\"),\n  is_TA = c(FALSE, TRUE, TRUE, TRUE, TRUE, TRUE, TRUE)\n)\nstr(roster)\n\nList of 3\n $ name : chr [1:7] \"Quang\" \"Akshay\" \"Nick\" \"Princess\" ...\n $ role : chr [1:7] \"Instructor\" \"TA\" \"TA\" \"TA\" ...\n $ is_TA: logi [1:7] FALSE TRUE TRUE TRUE TRUE TRUE ...\n\n\nR uses a specific type of list, data frame, containing the same number of rows with unique row names.\n\nstr(iris)\n\n'data.frame':   150 obs. of  5 variables:\n $ Sepal.Length: num  5.1 4.9 4.7 4.6 5 5.4 4.6 5 4.4 4.9 ...\n $ Sepal.Width : num  3.5 3 3.2 3.1 3.6 3.9 3.4 3.4 2.9 3.1 ...\n $ Petal.Length: num  1.4 1.4 1.3 1.5 1.4 1.7 1.4 1.5 1.4 1.5 ...\n $ Petal.Width : num  0.2 0.2 0.2 0.2 0.2 0.4 0.3 0.2 0.2 0.1 ...\n $ Species     : Factor w/ 3 levels \"setosa\",\"versicolor\",..: 1 1 1 1 1 1 1 1 1 1 ...\n\ntypeof(iris)\n\n[1] \"list\"\n\n\n\n\nOperators\nWe can perform element-wise actions on vectors through the operators:\n\narithmetic: +, -, *, /, ^ (for integer division, %/% is quotient, %% is remainder)\n\n\nv1 &lt;- c(1,2,3)\nv2 &lt;- c(4,5,6)\n\nv1 + v2\n\n[1] 5 7 9\n\nv1 * v2\n\n[1]  4 10 18\n\nv2 %% v1\n\n[1] 0 1 0\n\n\n\nrelation: &gt;, &gt;=, &lt; ,&lt;=, ==, !=\n\n\n5 &gt; 4\n\n[1] TRUE\n\n5 &lt;= 4\n\n[1] FALSE\n\n33 == 22\n\n[1] FALSE\n\n33 != 22\n\n[1] TRUE\n\n\n\nlogic: ! (not), & (and), | (or)\n\n\n(5 &gt; 6) | (2 &lt; 3)\n\n[1] TRUE\n\n(5 &gt; 6) & (2 &lt; 3)\n\n[1] FALSE\n\n!(5 &gt; 6) & (2 &lt; 3)\n\n[1] TRUE\n\n\n\nsequence: i:j (: operator, i and j are any two arbitrary numbers)\n\n\n1:5\n\n[1] 1 2 3 4 5\n\n5:1\n\n[1] 5 4 3 2 1\n\n-1:-5\n\n[1] -1 -2 -3 -4 -5\n\n-1:5\n\n[1] -1  0  1  2  3  4  5"
  },
  {
    "objectID": "health/labs/01-intro.html#loading-.csv-files",
    "href": "health/labs/01-intro.html#loading-.csv-files",
    "title": "Lab: getting started with R",
    "section": "Loading .csv files",
    "text": "Loading .csv files\nMost of the data provided to you are in .csv format. In the code chunk below, we use the read_csv() function (from the readr package, part of the tidyverse) to load a dataset that is saved in a folder located in the SURE GitHub repository. In quotations, insert the file path where the dataset is located, which in this case is online. However, typically you’ll save .csv files locally first and put them in an organized folder to access later.\n\nlibrary(tidyverse)\nheart_disease &lt;- read_csv(\"https://raw.githubusercontent.com/36-SURE/36-SURE.github.io/main/data/heart_disease.csv\")\nhead(heart_disease)"
  },
  {
    "objectID": "health/labs/01-intro.html#looking-for-help",
    "href": "health/labs/01-intro.html#looking-for-help",
    "title": "Lab: getting started with R",
    "section": "Looking for help",
    "text": "Looking for help\nIf you have any R problem, the best step is to use the help() function (or equivalently the ?). For example,\n\nhelp(str)\nhelp(lm)\n\nOr you can use the command ?…\n\n?str\n?lm\n\nDouble question marks can lead to a more general search.\n\n??predict\n\nYou should ALWAYS consult the R help documentation first before attempting to google around (or ask ChatGPT) for a solution."
  },
  {
    "objectID": "health/labs/01-intro.html#exercises",
    "href": "health/labs/01-intro.html#exercises",
    "title": "Lab: getting started with R",
    "section": "Exercises",
    "text": "Exercises\n\nCreate four vectors, v1 and v2 are numeric vectors, v3 is a character vector and v4 is a logic vector. Make sure the length of v1 and v2 are the same. (Hint: a way to check the length is to use the function length())\n\n\n# R code here\n\n\nPreform add, minus, product and division on v1 and v2.\n\n\n# R code here\n\n\nCreate four statements with both relation and logic operators, that 2 of them return TRUE and 2 of them return FALSE.\n\n\n# R code here\n\n\nCreate 2 sequences with length 20, one in an increasing order and the other in a decreasing order.\n\n\n# R code here\n\n\nThe following gapminder dataset contains health and income outcomes for 184 countries from 1960 to 2016 from the , accessed via the dslabs package. How many of the rows in the dataset are from the Caribbean (coded as Caribbean in region)? How about Eastern Europe? Can you summarize the counts for all regions? (Hint: table())\n\n\nlibrary(dslabs)\ndata(gapminder)\n\n# R code here"
  },
  {
    "objectID": "health/labs/01-intro.html#text-formatting-in-quarto",
    "href": "health/labs/01-intro.html#text-formatting-in-quarto",
    "title": "Lab: getting started with R",
    "section": "Text formatting in Quarto",
    "text": "Text formatting in Quarto\nThere are a lot of ways to format text in a Quarto document, e.g., italics and bold (just scan through this .qmd file to see how this was done). See this guide for more tips/tricks. In particular, check out the Markdown Basics and other guides under Authoring. See also this guide on R code chunk options.\nAs you’ll see throughout this summer (and especially with your project), well-formatted .html files can be a great way to showcase data science results to the public online. (Check out the project showcase from 2023 and 2022.)"
  },
  {
    "objectID": "health/labs/01-intro.html#customizing-rstudio",
    "href": "health/labs/01-intro.html#customizing-rstudio",
    "title": "Lab: getting started with R",
    "section": "Customizing RStudio",
    "text": "Customizing RStudio\nRStudio theme\nRStudio can be customized with different themes. To explore built-in themes,\n\nNavigate to the menu bar at the top of your screen\nChoose Tools &gt; Global Options &gt; Appearance\nChange your RStudio theme under Editor theme\n\n(FYI, Quang uses the Tomorrow Night Bright theme.)\nNote that within the Appearance tab, there are also options for changing your Editor font, Editor font size, etc.\nRStudio panes\nWithin RStudio, there are several panes (e.g., Console, Help, Environment, History, Plots, etc.). To customize, go to Tools &gt; Global Options &gt; Pane Layout, and arrange the panes as you see fit.\nFeel free to explore other options within the Tools &gt; Global Option menu."
  },
  {
    "objectID": "health/labs/04-github.html",
    "href": "health/labs/04-github.html",
    "title": "Using GitHub for project collaboration",
    "section": "",
    "text": "The purpose of this lab is to help you set up Git and GitHub on your computer, and use them to collaborate on your EDA project."
  },
  {
    "objectID": "health/labs/04-github.html#goal",
    "href": "health/labs/04-github.html#goal",
    "title": "Using GitHub for project collaboration",
    "section": "",
    "text": "The purpose of this lab is to help you set up Git and GitHub on your computer, and use them to collaborate on your EDA project."
  },
  {
    "objectID": "health/labs/04-github.html#task-0-github-registration-and-git-installation",
    "href": "health/labs/04-github.html#task-0-github-registration-and-git-installation",
    "title": "Using GitHub for project collaboration",
    "section": "Task 0: GitHub registration and Git installation",
    "text": "Task 0: GitHub registration and Git installation\n(Note: You were already being asked to complete these prior to the start of the program. Proceed to Task 1 if you already completed this task.)\nGitHub account. Register for a (free) GitHub account at https://github.com.  (if you already have a GitHub account, feel free to ignore this)\nDownload Git.  (if Git is already installed on your computer, the following instructions still hold for updating Git to its latest version)\n(Windows)\n\nGo to https://git-scm.com/download/win\nNavigate to “Click here to download” on the first line and click on it\nFollow the installation instructions\n\n(macOS)\n\nOpen the Terminal app on your computer (Finder \\(\\rightarrow\\) Applications \\(\\rightarrow\\) Terminal)\nGo to https://brew.sh and copy/paste the chunk under “Install Homebrew” to the Terminal\nOnce Homebrew is installed, type this into the Terminal: brew install git"
  },
  {
    "objectID": "health/labs/04-github.html#task-1-git-configuration",
    "href": "health/labs/04-github.html#task-1-git-configuration",
    "title": "Using GitHub for project collaboration",
    "section": "Task 1: Git configuration",
    "text": "Task 1: Git configuration\n\nMake sure you’ve already (i) created a GitHub account and (ii) installed Git on your computer\nYou then need to configure Git. This can be done directly in R:\n\n\n# uncomment and run the following line to install the usethis package\n# install.packages(\"usethis\")\nusethis::use_git_config(user.name = \"Your Name\", user.email = \"your-github@email.address\")\n\n\nUse your full name for the user.name field and the same email as your GitHub account for user.email\nYou then need to create a personal access token for authentication as follows:\n\n\nusethis::create_github_token()\n\n\nThis will direct you to the GitHub site on your browser (you may have to log in). On this site:\n\nUnder “Note”, type in some description for this token (e.g., “SURE 2024 GitHub token”)\nFor “Expiration”, set an expiration date for this token (e.g., 90 days) or make it permanent (i.e. choose “No expiration” if you don’t want to deal with this again in the future)\nUnder “Select scopes”, recommended scopes will be pre-selected. Stick with these for now.\n\nNext, click on “Generate token”\nCopy the token to your clipboard (or leave the browser window open, so you can come back to copy the token later)\nIn RStudio, run the following to get a prompt where you can paste your token:\n\n\n# uncomment and run the following line to install the gitcreds package\n# install.packages(\"gitcreds\")\ngitcreds::gitcreds_set()"
  },
  {
    "objectID": "health/labs/04-github.html#task-2-eda-project-collaboration-with-git-and-github",
    "href": "health/labs/04-github.html#task-2-eda-project-collaboration-with-git-and-github",
    "title": "Using GitHub for project collaboration",
    "section": "Task 2: EDA project collaboration with Git and GitHub",
    "text": "Task 2: EDA project collaboration with Git and GitHub\nMake sure every group member finishes Task 1 before proceeding to Task 2, which requires a group effort.\n\nStep 1: Create an EDA project repository on GitHub(Required for ONE group member only)\nEach group should elect ONE person to create a GitHub repository for the EDA project. This repository is to be shared among all group members.\nThe elected group member should do the following to create a new GitHub repository:\n\nAfter you’ve signed in to GitHub, go to https://github.com/new\nName the repository (give it a meaningful name)\nGive the repository a description (don’t leave this blank although this is optional)\nDecide whether to keep the repository public or private (for now, keep it public, so that we can review your code)\nClick on “Initialize this repository with a README”. For now, there’s no need to “Add .gitignore” or “Choose a license”\nClick on “Create Repository”\n\nNext, to add the rest of your group to the repository:\n\nGo to browser page of the GitHub repository you just created and click on “Settings”\nNavigate to the left sidebar and click on “Collaborators”\nClick on “Add people” (under Manage access). Enter the GitHub username for the other group members.\n\n\n\nStep 2: Clone the remote repository to your local computer(Required for ALL group members)\n\nEveryone (except the member responsible for creating the repository) should each get an invitation sent to the email associated with your GitHub account\nCheck your email and accept the invitation\nGo to the browser page for the EDA project GitHub repository (created in Step 1)\nClick on Code (in the same line as “Go to file”). Under HTTPS, copy the URL\nIn RStudio, click on File &gt; New Project.... Next, click on “Version Control” and then on “Git”. Paste the URL you just copied into “Repository URL”\nType the name for the folder on your computer associated with this repository into Project directory name\n\nYou can choose whatever name you want, but it is recommended to give a name similar to the repository name on GitHub\n\nMake sure “Create project as subdirectory of:” points to where you want to locate this new folder\nClick on “Create Project”\nAt this point, you should find that the “Files” pane (in the bottom right of RStudio) is listing the files in your local repository.\n\n\n\nStep 3: Modify the repository (Required for ALL group members)\nEach group member should create their own “sandbox” folder locally on their own computer as follows:\n\nNavigate to the Files pane in RStudio and click on “Folder” to create a new folder (for the new folder name, use your last name.)\nIn RStudio, open a new file (could be anything - e.g. R Script, Quarto document, etc.). Fill the file with some code/comments/etc. (This is just for illustration purpose, to show how you can add a file to GitHub from your computer)\nSave the file inside the folder you just created (with your last name as the folder name). At this point, this file should show up in the “Git” pane (in the top right of RStudio)\nCheck the box under “Staged” in the Git pane to stage the file for a commit\nClick on “Commit” in the Git pane\nIn the new window that opens, add a “Commit message”, then click on the “Commit” button\nClick on “Push” to push your changes from your local repository to the shared remote repository on GitHub\n\n\n\nStep 4: Update the local repository (Required for ALL group members)\n\nFirst, make sure that everyone in your group have completed Step 3\nIn RStudio, navigate to the Git pane and click on “Pull”. A new window will pop up. Once everything is finished running, close the window.\nAt this point, you should find that your Files pane in RStudio is listing the folders that your group members have created, in addition to your own folder\n\nThis task is know as git pull, which updates the local repository to match that content of a shared remote repository\n\n\n\n\nStep 5: Start your EDA project\nIf you encountered no errors then you can feel free to start brainstorming your EDA project with your group.\nFor this project, we ask you to create/update/save files within your own sandbox folder (that you created in Step 3). This will help mitigate the risk of running into trouble when pushing your files to GitHub, especially for those who are new to Git and GitHub. This also allows us to easily review your code.\n\n\n\n\n\n\nImportant notes\n\n\n\nThe GitHub procedure for any project collaboration is\n\nPull new changes\nMake changes on your computer (e.g. create new files, update existing files)\nCommit your local changes (Note: this step may be repeated)\nPull again to avoid merge conflicts\nPush your commit(s) to GitHub\n\nAdvices: Make small, frequent commits. ALWAYS pull before you push.\n\n\nAsk us for help if you run into any issues or have any questions."
  },
  {
    "objectID": "health/eda/maternal.html#overview",
    "href": "health/eda/maternal.html#overview",
    "title": "EDA project: maternal healthcare disparities",
    "section": "Overview",
    "text": "Overview\nThis project will be released on Thursday, June 6 and conclude with an 8-minute presentation on Tuesday, June 18 during lecture time.\nStudents will be randomly placed into groups of three and each group will be randomly assigned a dataset.\nThe goal of this project is to practice understanding the structure of a dataset, and to practice generating and evaluating hypotheses using fundamental EDA and data visualization techniques."
  },
  {
    "objectID": "health/eda/maternal.html#deliverables",
    "href": "health/eda/maternal.html#deliverables",
    "title": "EDA project: maternal healthcare disparities",
    "section": "Deliverables",
    "text": "Deliverables\nEach group is expected to make slides to accompany the 8-minute presentation.\nThe presentation should feature the following:\n\nOverview of the structure of your dataset\nThree questions/hypotheses you are interested in exploring\nThree data visualizations exploring the questions, at least two of which must be multivariate. Each visualization must be in a different format from the other two, and you must have at least one categorical and one continuous visualization\nOne clustering analysis\nConclusions for the hypotheses based on your EDA and data visualizations"
  },
  {
    "objectID": "health/eda/maternal.html#timeline",
    "href": "health/eda/maternal.html#timeline",
    "title": "EDA project: maternal healthcare disparities",
    "section": "Timeline",
    "text": "Timeline\nThere will be two submission deadlines:\nThursday, June 13 at 5pm ET - Each student will push their individual code for the project thus far to GitHub for review. We will then provide feedback.\nMonday, June 17 at 5pm ET - Slides and full code must be completed and ready for presentation. Send your slides to Quang (quang@stat.cmu.edu). All code must be written in R; but the slides may be created in any software. Take advantage of examples from lectures, but also feel free to explore online resources that may be relevant. (But be sure to always consult the R help documentation first before attempting to google around or ask ChatGPT.)"
  },
  {
    "objectID": "health/eda/maternal.html#data",
    "href": "health/eda/maternal.html#data",
    "title": "EDA project: maternal healthcare disparities",
    "section": "Data",
    "text": "Data\nThis dataset contains information on maternal healthcare disparities. The Centers for Disease Control and Prevention WONDER program helps track information related to birth records, parent demographics and risk factors, pregnancy history and pre-natal care characteristics. This data source could help identify combinations of risk factors more commonly associated with adverse outcomes which could then be utilized to develop better pre-natal care programs or targeted interventions to reduce disparities and improve patient outcomes across all ethnicity.\nThe dataset is a sample of data from the CDC Wonder database for available birth records from 2019 that has been aggregated by state and a few conditions (the number of prior births now deceased and whether the mother smoked or had pre-pregnancy diabetes or pre-pregnancy hypertension). For example, the first row corresponds to the set of births that were born to women in Alabama who had no prior births deceased, smoked, and had both diabetes and hypertension pre-pregnancy. There were 12 such births, and the following variables (e.g. mother’s age) describe the mothers/infants in that set of 12.\n\nState: name of the state\nPriorBirthsNowDeceased: number of prior births now deceased\nTobaccoUse: whether the mother uses tobacco products\nPrePregnancyDiabetes: whether the mother had diabetes prior to becoming pregnant\nPrePregnancyHypertension: whether the mother had hypertension prior to becoming pregnant\nBirths: number of births in that state with a defined combination of the previous four conditions (PriorBirthsNowDeceased, TobaccoUse, PrePregnancyDiabetes, PrePregnancyHypertension)\nAverageMotherAge: average mother’s age for the corresponding group of births\nAverageBirthWeight: average birth weight in grams for the corresponding group of births\nAveragePrePregnancyBMI: average pre-pregnancy BMI of the mother for the corresponding group of births\nAverageNumberPrenatalVisits: average number of prenatal visits of the mother for the corresponding group of births\nAverageIntervalSinceLastBirth: average length of time since the last birth for the corresponding group of births"
  },
  {
    "objectID": "health/eda/maternal.html#starter-code",
    "href": "health/eda/maternal.html#starter-code",
    "title": "EDA project: maternal healthcare disparities",
    "section": "Starter code",
    "text": "Starter code\n\nlibrary(tidyverse)\nmaternal &lt;- read_csv(\"https://raw.githubusercontent.com/36-SURE/36-SURE.github.io/main/data/maternal.csv\")"
  },
  {
    "objectID": "health/eda/covid_hospitalizations.html#overview",
    "href": "health/eda/covid_hospitalizations.html#overview",
    "title": "EDA project: COVID hospitalizations in Pennsylvania",
    "section": "Overview",
    "text": "Overview\nThis project will be released on Thursday, June 6 and conclude with an 8-minute presentation on Tuesday, June 18 during lecture time.\nStudents will be randomly placed into groups of three and each group will be randomly assigned a dataset.\nThe goal of this project is to practice understanding the structure of a dataset, and to practice generating and evaluating hypotheses using fundamental EDA and data visualization techniques."
  },
  {
    "objectID": "health/eda/covid_hospitalizations.html#deliverables",
    "href": "health/eda/covid_hospitalizations.html#deliverables",
    "title": "EDA project: COVID hospitalizations in Pennsylvania",
    "section": "Deliverables",
    "text": "Deliverables\nEach group is expected to make slides to accompany the 8-minute presentation.\nThe presentation should feature the following:\n\nOverview of the structure of your dataset\nThree questions/hypotheses you are interested in exploring\nThree data visualizations exploring the questions, at least two of which must be multivariate. Each visualization must be in a different format from the other two, and you must have at least one categorical and one continuous visualization\nOne clustering analysis\nConclusions for the hypotheses based on your EDA and data visualizations"
  },
  {
    "objectID": "health/eda/covid_hospitalizations.html#timeline",
    "href": "health/eda/covid_hospitalizations.html#timeline",
    "title": "EDA project: COVID hospitalizations in Pennsylvania",
    "section": "Timeline",
    "text": "Timeline\nThere will be two submission deadlines:\nThursday, June 13 at 5pm ET - Each student will push their individual code for the project thus far to GitHub for review. We will then provide feedback.\nMonday, June 17 at 5pm ET - Slides and full code must be completed and ready for presentation. Send your slides to Quang (quang@stat.cmu.edu). All code must be written in R; but the slides may be created in any software. Take advantage of examples from lectures, but also feel free to explore online resources that may be relevant. (But be sure to always consult the R help documentation first before attempting to google around or ask ChatGPT.)"
  },
  {
    "objectID": "health/eda/covid_hospitalizations.html#data",
    "href": "health/eda/covid_hospitalizations.html#data",
    "title": "EDA project: COVID hospitalizations in Pennsylvania",
    "section": "Data",
    "text": "Data\nThis dataset contains county-level hospitalization information on related to COVID-19 patient in Pennsylvania. with the dates ranging from April 1, 2020 to December 31, 2020. The data are available online at the Open Data Pennsylvania website. More information about the hospitalization data can be found here.\nEach row in the dataset corresponds to a county in Pennsylvania on a given date (between April 1, 2020 and December 31, 2020 - note that missing data are present for some of the rows) and the columns are:\n\ncounty: name of county\ndate: date\n\nicu_avail: adult ICU beds available\n\nicu_total: adult ICU beds total\n\nmed_avail: medical/surgical beds available\n\nmed_total: medical/surgical beds total\nped_avail: pediatrics beds available\n\nped_total: pediatrics beds total\n\npic_avail: pediatrics ICU beds available\n\npic_total: pediatrics ICU beds total\n\ncovid_patients: COVID-19 patients hospitalized\ncovid_vents: COVID-19 patients on ventilators\n\nvents_use: total ventilators in use\n\nvents: total ventilators\n\nicu_avail_mean: adult ICU beds available, 14-day average\nicu_total_mean: adult ICU beds total, 14-day average\n\nmed_avail_mean: medical/surgical beds available, 14-day average\nmed_total_mean: medical/surgical beds total, 14-day average\nped_avail_mean: pediatric beds available, 14-day average\nped_total_mean: pediatric beds total, 14-day average\npic_avail_mean: pediatric ICU beds available, 14-day average\npic_total_mean: pediatric ICU beds total, 14-day average\ncovid_patients_mean: COVID-19 patients hospitalized, 14-day average\ncovid_vents_mean: COVID-19 patients on ventilators, 14-day average\nvents_use_mean: total ventilators in use, 14-day average\nvents_mean: total ventilators, 14-day average\n\nicu_percent: adult ICU beds, percent available\n\nmed_percent: medical/surgical beds, percent available\n\nped_percent: pediatric beds, percent available\npic_percent: pediatric ICU beds, percent available\n\ncovid_icu: COVID patients in intensive care unit (ICU)\ncovid_icu_mean: the mean for COVID patients in intensive care unit (ICU), 14-day average.\n\ncounty_fips: a county’s 5-digit code (read more here)\nlongitude: a longitude generic point within the county\nlatitude: a latitude generic point within the county"
  },
  {
    "objectID": "health/eda/covid_hospitalizations.html#starter-code",
    "href": "health/eda/covid_hospitalizations.html#starter-code",
    "title": "EDA project: COVID hospitalizations in Pennsylvania",
    "section": "Starter code",
    "text": "Starter code\n\nlibrary(tidyverse)\ncovid_hospitalizations &lt;- read_csv(\"https://raw.githubusercontent.com/36-SURE/36-SURE.github.io/main/data/covid_hospitalizations.csv\")"
  },
  {
    "objectID": "health/eda/other/utah.html#overview",
    "href": "health/eda/other/utah.html#overview",
    "title": "EDA project: healthcare indicators in Utah",
    "section": "Overview",
    "text": "Overview\nThis project will be released on Thursday, June 6 and conclude with a 10-minute presentation on Tuesday, June 18 during lecture time.\nStudents will be randomly placed into groups of three and each group will be randomly assigned a dataset.\nThe goal of this project is to practice understanding the structure of a dataset, and to practice generating and evaluating hypotheses using fundamental EDA and data visualization techniques."
  },
  {
    "objectID": "health/eda/other/utah.html#deliverables",
    "href": "health/eda/other/utah.html#deliverables",
    "title": "EDA project: healthcare indicators in Utah",
    "section": "Deliverables",
    "text": "Deliverables\nEach group is expected to make slides to accompany the 10-minute presentation.\nThe presentation should feature the following:\n\nOverview of the structure of your dataset\nThree questions/hypotheses you are interested in exploring\nThree data visualizations exploring the questions, at least two of which must be multivariate. Each visualization must be in a different format from the other two, and you must have at least one categorical and one continuous visualization\nOne clustering analysis\nConclusions reached for the hypotheses based on your EDA and data visualizations"
  },
  {
    "objectID": "health/eda/other/utah.html#timeline",
    "href": "health/eda/other/utah.html#timeline",
    "title": "EDA project: healthcare indicators in Utah",
    "section": "Timeline",
    "text": "Timeline\nThere will be two submission deadlines:\nThursday, June 13 at 5pm ET - Each student will push their individual code for the project thus far to GitHub for review. We will then provide feedback.\nMonday, June 17 at 5pm ET - Slides and full code must be completed and ready for presentation. Send your slides to Quang (quang@stat.cmu.edu). All code must be written in R; but the slides may be created in any software. Take advantage of examples from lectures, but also feel free to explore online resources that may be relevant."
  },
  {
    "objectID": "health/eda/other/utah.html#data",
    "href": "health/eda/other/utah.html#data",
    "title": "EDA project: healthcare indicators in Utah",
    "section": "Data",
    "text": "Data\nThis dataset contains information on health indicators for 29 counties in Utah in 2014. The data are available online at the State of Utah Open Data Catalog website. The provided dataset is a cleaned version of the original data (which can be found here).\nEach row in the dataset corresponds to a county in Utah and the columns are:\n\nCounty: name of the county in Utah\nPopulation: county population\nPercentUnder18: percent of county population that is under 18\nPercentOver65: percent of county population that is 65 and over\nDiabeticRate: diabetic rate\nHIVRate: hive rate\nPrematureMortalityRate: premature mortality rate\nInfantMortalityRate: infant mortality rate\nChildMortalityRate: child mortality rate\nLimitedAccessToFood: limited access to food %\nFoodInsecure: % food insecure\nMotorDeathRate: motor vehicle mortality rate\nDrugDeathRate: drug poisoning mortality rate\nUninsured: % uninsured\nUninsuredChildren: % uninsured children\nHealthCareCosts: cost of healthcare\nCouldNotSeeDr: % could not see doctor due to cost\nMedianIncome: median household income\nChildrenFreeLunch: % children eligible for free lunch\nHomicideRate: homicide rate"
  },
  {
    "objectID": "health/eda/other/utah.html#starter-code",
    "href": "health/eda/other/utah.html#starter-code",
    "title": "EDA project: healthcare indicators in Utah",
    "section": "Starter code",
    "text": "Starter code\n\nlibrary(tidyverse)\nutah_health &lt;- read_csv(\"https://raw.githubusercontent.com/36-SURE/36-SURE.github.io/main/data/utah_health.csv\")"
  },
  {
    "objectID": "health/eda/other/hepatitis.html#overview",
    "href": "health/eda/other/hepatitis.html#overview",
    "title": "EDA project: hepatitis",
    "section": "Overview",
    "text": "Overview\nThis project will be released on Thursday, June 6 and conclude with a 10-minute presentation on Tuesday, June 18 during lecture time.\nStudents will be randomly placed into groups of three and each group will be randomly assigned a dataset.\nThe goal of this project is to practice understanding the structure of a dataset, and to practice generating and evaluating hypotheses using fundamental EDA and data visualization techniques."
  },
  {
    "objectID": "health/eda/other/hepatitis.html#deliverables",
    "href": "health/eda/other/hepatitis.html#deliverables",
    "title": "EDA project: hepatitis",
    "section": "Deliverables",
    "text": "Deliverables\nEach group is expected to make slides to accompany the 10-minute presentation.\nThe presentation should feature the following:\n\nOverview of the structure of your dataset\nThree questions/hypotheses you are interested in exploring\nThree data visualizations exploring the questions, at least two of which must be multivariate. Each visualization must be in a different format from the other two, and you must have at least one categorical and one continuous visualization\nOne clustering analysis\nConclusions reached for the hypotheses based on your EDA and data visualizations"
  },
  {
    "objectID": "health/eda/other/hepatitis.html#timeline",
    "href": "health/eda/other/hepatitis.html#timeline",
    "title": "EDA project: hepatitis",
    "section": "Timeline",
    "text": "Timeline\nThere will be two submission deadlines:\nThursday, June 13 at 5pm ET - Each student will push their individual code for the project thus far to GitHub for review. We will then provide feedback.\nMonday, June 17 at 5pm ET - Slides and full code must be completed and ready for presentation. Send your slides to Quang (quang@stat.cmu.edu). All code must be written in R; but the slides may be created in any software. Take advantage of examples from lectures, but also feel free to explore online resources that may be relevant."
  },
  {
    "objectID": "health/eda/other/hepatitis.html#data",
    "href": "health/eda/other/hepatitis.html#data",
    "title": "EDA project: hepatitis",
    "section": "Data",
    "text": "Data\nThis datasets contains laboratory measurements of blood donors and Hepatitis C patients. The data were obtained from the UCI Machine Learning Repository, with the original source coming from the article Using machine learning techniques to generate laboratory diagnostic pathways—a case study, published in Journal of Laboratory and Precision Medicine.\nEach row in the dataset represents a patient and the columns are:\n\n...1: patient ID number\nCategory: diagnosis (0=Blood Donor, 0s=suspect Blood Donor, 1=Hepatitis, 2=Fibrosis, 3=Cirrhosis)\nAge: age (in years)\nSex: sex (male or female)\nALB: albumin blood test\nALP: alkaline phosphatase\nALT: alanine transaminase\nAST: aspartate transaminase\nBIL: bilirubin\nCHE: acetylcholinesterase\nCHOL: cholesterol\nCREA: creatinine\nGGT: gamma-glutamyl transferase\nPROT: proteins"
  },
  {
    "objectID": "health/eda/other/hepatitis.html#starter-code",
    "href": "health/eda/other/hepatitis.html#starter-code",
    "title": "EDA project: hepatitis",
    "section": "Starter code",
    "text": "Starter code\n\nlibrary(tidyverse)\nhepatitis &lt;- read_csv(\"https://raw.githubusercontent.com/36-SURE/36-SURE.github.io/main/data/hepatitis.csv\")"
  },
  {
    "objectID": "health/eda/hospitals.html#overview",
    "href": "health/eda/hospitals.html#overview",
    "title": "EDA project: hospital ratings",
    "section": "Overview",
    "text": "Overview\nThis project will be released on Thursday, June 6 and conclude with an 8-minute presentation on Tuesday, June 18 during lecture time.\nStudents will be randomly placed into groups of three and each group will be randomly assigned a dataset.\nThe goal of this project is to practice understanding the structure of a dataset, and to practice generating and evaluating hypotheses using fundamental EDA and data visualization techniques."
  },
  {
    "objectID": "health/eda/hospitals.html#deliverables",
    "href": "health/eda/hospitals.html#deliverables",
    "title": "EDA project: hospital ratings",
    "section": "Deliverables",
    "text": "Deliverables\nEach group is expected to make slides to accompany the 8-minute presentation.\nThe presentation should feature the following:\n\nOverview of the structure of your dataset\nThree questions/hypotheses you are interested in exploring\nThree data visualizations exploring the questions, at least two of which must be multivariate. Each visualization must be in a different format from the other two, and you must have at least one categorical and one continuous visualization\nOne clustering analysis\nConclusions for the hypotheses based on your EDA and data visualizations"
  },
  {
    "objectID": "health/eda/hospitals.html#timeline",
    "href": "health/eda/hospitals.html#timeline",
    "title": "EDA project: hospital ratings",
    "section": "Timeline",
    "text": "Timeline\nThere will be two submission deadlines:\nThursday, June 13 at 5pm ET - Each student will push their individual code for the project thus far to GitHub for review. We will then provide feedback.\nMonday, June 17 at 5pm ET - Slides and full code must be completed and ready for presentation. Send your slides to Quang (quang@stat.cmu.edu). All code must be written in R; but the slides may be created in any software. Take advantage of examples from lectures, but also feel free to explore online resources that may be relevant. (But be sure to always consult the R help documentation first before attempting to google around or ask ChatGPT.)"
  },
  {
    "objectID": "health/eda/hospitals.html#data",
    "href": "health/eda/hospitals.html#data",
    "title": "EDA project: hospital ratings",
    "section": "Data",
    "text": "Data\nThis dataset contains information on hospital ratings curated by the CORGIS Dataset Project. The goal of this project was to: “allow consumers to directly compare across hospitals performance measure information related to heart attack, emergency department care, preventive care, stroke care, and other conditions. The data is part of an Administration-wide effort to increase the availability and accessibility of information on quality, utilization, and costs for effective, informed decision-making.” The original source of data can be found here.\nEach row of the dataset corresponds to a single hospital and the columns with definitions borrowed from the online glossary are:\n\nFacility.Name: name of the hospital\nFacility.City: city in which the hospital is located\nFacility.State: two letter capitalized abbreviation of the State in which the hospital is located\nFacility.Type: type of organization operating the hospital: one of Government, Private, Proprietary, Church, or Unknown\nRating.Overall: overall rating between 1 and 5 stars, with 5 stars being the highest rating; -1 represents no rating\nRating.Mortality: Above, Same, Below, or Unknown comparison to national hospital mortality\nRating.Safety: Above, Same, Below, or Unknown comparison to national hospital safety\nRating.Readmission: Above, Same, Below, or Unknown comparison to national hospital readmission\nRating.Experience: Above, Same, Below, or Unknown comparison to national hospital patience experience\nRating.Effectiveness: Above, Same, Below, or Unknown comparison to national hospital effectiveness of care\nRating.Timeliness: Above, Same, Below, or Unknown comparison to national hospital timeliness of care\nRating.Imaging: Above, Same, Below, or Unknown comparison to national hospital effective use of imaging\nProcedure.Heart Attack.Cost: Average cost of care for heart attacks\nProcedure.Heart Attack.Quality: Lower, Average, Worse, or Unknown comparison to national quality of care for heart attacks\nProcedure.Heart Attack.Value: Lower, Average, Worse, or Unknown comparison to national cost of care for heart attacks\nProcedure.Heart Failure.Cost: Average cost of care for heart failure\nProcedure.Heart Failure.Quality: Lower, Average, Worse, or Unknown comparison to national quality of care for heart failures\nProcedure.Heart Failure.Value: Lower, Average, Worse, or Unknown comparison to national cost of care for heart failures\nProcedure.Pneumonia.Cost: Average cost of care for pneumonia\nProcedure.Pneumonia.Quality: Lower, Average, Worse, or Unknown comparison to national quality of care for pneumonia\nProcedure.Pneumonia.Value: Lower, Average, Worse, or Unknown comparison to national cost of care for pneumonia\nProcedure.Hip Knee.Cost: Average cost of care for hip or knee conditions\nProcedure.Hip Knee.Quality: Lower, Average, Worse, or Unknown comparison to national quality of care for hip or knee conditions\nProcedure.Hip Knee.Value: Lower, Average, Worse, or Unknown comparison to national cost of care for hip or knee conditions"
  },
  {
    "objectID": "health/eda/hospitals.html#starter-code",
    "href": "health/eda/hospitals.html#starter-code",
    "title": "EDA project: hospital ratings",
    "section": "Starter code",
    "text": "Starter code\n\nlibrary(tidyverse)\nhospitals &lt;- read_csv(\"https://raw.githubusercontent.com/36-SURE/36-SURE.github.io/main/data/hospitals.csv\")"
  },
  {
    "objectID": "health/eda/nurses.html#overview",
    "href": "health/eda/nurses.html#overview",
    "title": "EDA project: registered nurses",
    "section": "Overview",
    "text": "Overview\nThis project will be released on Thursday, June 6 and conclude with an 8-minute presentation on Tuesday, June 18 during lecture time.\nStudents will be randomly placed into groups of three and each group will be randomly assigned a dataset.\nThe goal of this project is to practice understanding the structure of a dataset, and to practice generating and evaluating hypotheses using fundamental EDA and data visualization techniques."
  },
  {
    "objectID": "health/eda/nurses.html#deliverables",
    "href": "health/eda/nurses.html#deliverables",
    "title": "EDA project: registered nurses",
    "section": "Deliverables",
    "text": "Deliverables\nEach group is expected to make slides to accompany the 8-minute presentation.\nThe presentation should feature the following:\n\nOverview of the structure of your dataset\nThree questions/hypotheses you are interested in exploring\nThree data visualizations exploring the questions, at least two of which must be multivariate. Each visualization must be in a different format from the other two, and you must have at least one categorical and one continuous visualization\nOne clustering analysis\nConclusions for the hypotheses based on your EDA and data visualizations"
  },
  {
    "objectID": "health/eda/nurses.html#timeline",
    "href": "health/eda/nurses.html#timeline",
    "title": "EDA project: registered nurses",
    "section": "Timeline",
    "text": "Timeline\nThere will be two submission deadlines:\nThursday, June 13 at 5pm ET - Each student will push their individual code for the project thus far to GitHub for review. We will then provide feedback.\nMonday, June 17 at 5pm ET - Slides and full code must be completed and ready for presentation. Send your slides to Quang (quang@stat.cmu.edu). All code must be written in R; but the slides may be created in any software. Take advantage of examples from lectures, but also feel free to explore online resources that may be relevant. (But be sure to always consult the R help documentation first before attempting to google around or ask ChatGPT.)"
  },
  {
    "objectID": "health/eda/nurses.html#data",
    "href": "health/eda/nurses.html#data",
    "title": "EDA project: registered nurses",
    "section": "Data",
    "text": "Data\nThis dataset contains registered nursing labor stats for every year between 1998 and 2020 across the US states and territories. The data came from the TidyTuesday project with original source from Data.World.\nEach row in the dataset corresponds to a US state/territory and the columns are:\n\nState: name of the US state or territory\nYear: year\nTotal Employed RN: estimated total employment rounded to the nearest 10 (excludes self-employed)\nEmployed Standard Error (%): percent relative standard error (PRSE) for the employment estimate. PRSE is a measure of sampling error, expressed as a percentage of the corresponding estimate. Sampling error occurs when values for a population are estimated from a sample survey of the population, rather than calculated from data for all members of the population. Estimates with lower PRSEs are typically more precise in the presence of sampling error.\nHourly Wage Avg: mean hourly wage\nHourly Wage Median: hourly median wage (or the 50th percentile)\nAnnual Salary Avg: mean annual wage\nAnnual Salary Median: annual median wage (or the 50th percentile)\nWage/Salary standard error (%): percent relative standard error (PRSE) for the mean wage estimate\nHourly 10th Percentile: hourly 10th percentile wage\nHourly 25th Percentile: hourly 25th percentile wage\nHourly 75th Percentile: hourly 75th percentile wage\nHourly 90th Percentile: hourly 90th percentile wage\nAnnual 10th Percentile: annual 10th percentile wage\nAnnual 25th Percentile: annual 25th percentile wage\nAnnual 75th Percentile: annual 75th percentile wage\nAnnual 90th Percentile: annual 90th percentile wage\nLocation Quotient: the location quotient represents the ratio of an occupation’s share of employment in a given area to that occupation’s share of employment in the U.S. as a whole. For example, an occupation that makes up 10 percent of employment in a specific metropolitan area compared with 2 percent of U.S. employment would have a location quotient of 5 for the area in question. Only available for the state, metropolitan area, and nonmetropolitan area estimates; otherwise, this column is blank.\nTotal Employed (National)_Aggregate: total employment (national)\nTotal Employed (Healthcare, National)_Aggregate: total employment (healthcare, national)\nTotal Employed (Healthcare, State)_Aggregate: total employment (healthcare, state)\nYearly Total Employed (State)_Aggregate: yearly total employment (state)"
  },
  {
    "objectID": "health/eda/nurses.html#starter-code",
    "href": "health/eda/nurses.html#starter-code",
    "title": "EDA project: registered nurses",
    "section": "Starter code",
    "text": "Starter code\n\nlibrary(tidyverse)\nnurses &lt;- read_csv(\"https://raw.githubusercontent.com/36-SURE/36-SURE.github.io/main/data/nurses.csv\")"
  },
  {
    "objectID": "health/eda/covid_cases_deaths.html#overview",
    "href": "health/eda/covid_cases_deaths.html#overview",
    "title": "EDA project: COVID cases and deaths in Pennsylvania",
    "section": "Overview",
    "text": "Overview\nThis project will be released on Thursday, June 6 and conclude with an 8-minute presentation on Tuesday, June 18 during lecture time.\nStudents will be randomly placed into groups of three and each group will be randomly assigned a dataset.\nThe goal of this project is to practice understanding the structure of a dataset, and to practice generating and evaluating hypotheses using fundamental EDA and data visualization techniques."
  },
  {
    "objectID": "health/eda/covid_cases_deaths.html#deliverables",
    "href": "health/eda/covid_cases_deaths.html#deliverables",
    "title": "EDA project: COVID cases and deaths in Pennsylvania",
    "section": "Deliverables",
    "text": "Deliverables\nEach group is expected to make slides to accompany the 8-minute presentation.\nThe presentation should feature the following:\n\nOverview of the structure of your dataset\nThree questions/hypotheses you are interested in exploring\nThree data visualizations exploring the questions, at least two of which must be multivariate. Each visualization must be in a different format from the other two, and you must have at least one categorical and one continuous visualization\nOne clustering analysis\nConclusions for the hypotheses based on your EDA and data visualizations"
  },
  {
    "objectID": "health/eda/covid_cases_deaths.html#timeline",
    "href": "health/eda/covid_cases_deaths.html#timeline",
    "title": "EDA project: COVID cases and deaths in Pennsylvania",
    "section": "Timeline",
    "text": "Timeline\nThere will be two submission deadlines:\nThursday, June 13 at 5pm ET - Each student will push their individual code for the project thus far to GitHub for review. We will then provide feedback.\nMonday, June 17 at 5pm ET - Slides and full code must be completed and ready for presentation. Send your slides to Quang (quang@stat.cmu.edu). All code must be written in R; but the slides may be created in any software. Take advantage of examples from lectures, but also feel free to explore online resources that may be relevant. (But be sure to always consult the R help documentation first before attempting to google around or ask ChatGPT.)"
  },
  {
    "objectID": "health/eda/covid_cases_deaths.html#data",
    "href": "health/eda/covid_cases_deaths.html#data",
    "title": "EDA project: COVID cases and deaths in Pennsylvania",
    "section": "Data",
    "text": "Data\nThis dataset contains county-level information on the number of COVID-related cases and deaths for Pennsylvania. In particular, daily case and death counts, averages, and rates are provided for all counties in Pennsylvania, with the dates ranging from April 1, 2020 to December 31, 2020. The data are available online at the Open Data Pennsylvania website. More information about the cases and deaths data can be found here and here.\nEach row in the dataset corresponds to a county in Pennsylvania on a given date (between April 1, 2020 and December 31, 2020) and the columns are:\n\ncounty: name of county\ndate: date\ncases: number of new confirmed and probable cases first reported to the Department of Health on that date\ncases_avg_new: rolling 7-day average of new confirmed and probable case\ncases_cume: cumulative confirmed and probable cases reported through that date\n\ncases_rate: number of new confirmed and probable cases reported that date per 100,000 population\ncases_avg_new_rate: rolling 7-day average of new confirmed and probable cases per 100,000 population\ncases_cume_rate: number of cumulative confirmed and probable cases reported through that date per 100,000 population\n\ndeaths: count of new deaths\n\ndeaths_avg_new: new deaths, 7-day average\ndeaths_cume: cumulative count of deaths\n\npopulation: population (as of 2019)\n\ndeaths_rate: new deaths per 100,000 population\ndeaths_avg_new_rate: new deaths per 100,000 population, 7-day average\ndeaths_cume_rate: cumulative count of deaths per 100,000 population\nlongitude: a longitude generic point within the county\nlatitude: a latitude generic point within the county"
  },
  {
    "objectID": "health/eda/covid_cases_deaths.html#starter-code",
    "href": "health/eda/covid_cases_deaths.html#starter-code",
    "title": "EDA project: COVID cases and deaths in Pennsylvania",
    "section": "Starter code",
    "text": "Starter code\n\nlibrary(tidyverse)\ncovid_cases_deaths &lt;- read_csv(\"https://raw.githubusercontent.com/36-SURE/36-SURE.github.io/main/data/covid_cases_deaths.csv\")"
  },
  {
    "objectID": "health/eda/prescriptions.html#overview",
    "href": "health/eda/prescriptions.html#overview",
    "title": "EDA project: opioid prescriptions and claims",
    "section": "Overview",
    "text": "Overview\nThis project will be released on Thursday, June 6 and conclude with an 8-minute presentation on Tuesday, June 18 during lecture time.\nStudents will be randomly placed into groups of three and each group will be randomly assigned a dataset.\nThe goal of this project is to practice understanding the structure of a dataset, and to practice generating and evaluating hypotheses using fundamental EDA and data visualization techniques."
  },
  {
    "objectID": "health/eda/prescriptions.html#deliverables",
    "href": "health/eda/prescriptions.html#deliverables",
    "title": "EDA project: opioid prescriptions and claims",
    "section": "Deliverables",
    "text": "Deliverables\nEach group is expected to make slides to accompany the 8-minute presentation.\nThe presentation should feature the following:\n\nOverview of the structure of your dataset\nThree questions/hypotheses you are interested in exploring\nThree data visualizations exploring the questions, at least two of which must be multivariate. Each visualization must be in a different format from the other two, and you must have at least one categorical and one continuous visualization\nOne clustering analysis\nConclusions for the hypotheses based on your EDA and data visualizations"
  },
  {
    "objectID": "health/eda/prescriptions.html#timeline",
    "href": "health/eda/prescriptions.html#timeline",
    "title": "EDA project: opioid prescriptions and claims",
    "section": "Timeline",
    "text": "Timeline\nThere will be two submission deadlines:\nThursday, June 13 at 5pm ET - Each student will push their individual code for the project thus far to GitHub for review. We will then provide feedback.\nMonday, June 17 at 5pm ET - Slides and full code must be completed and ready for presentation. Send your slides to Quang (quang@stat.cmu.edu). All code must be written in R; but the slides may be created in any software. Take advantage of examples from lectures, but also feel free to explore online resources that may be relevant. (But be sure to always consult the R help documentation first before attempting to google around or ask ChatGPT.)"
  },
  {
    "objectID": "health/eda/prescriptions.html#data",
    "href": "health/eda/prescriptions.html#data",
    "title": "EDA project: opioid prescriptions and claims",
    "section": "Data",
    "text": "Data\nThis dataset contains information on Medicare Part D Prescription Claims. Under the Medicare Part D Prescription Drug program, information is tracked for opioids and other drugs prescribed by physicians and other health care providers including the number of prescriptions dispensed (original prescriptions and refills), the total drug cost, beneficiary demographics (65+), related claims information, as well as information about the physician/provider such as their specialization and location.\nThe sample of data is proportionally sampled across the states (e.g. 5% from each state), and includes the following columns:\n\nNPI: national provider identifier for the performing provider on the claim\nLastName: provider last name\nFirstName: rovider first name\nCity: city where the provider is located\nState: state where the provider is located\nSpecialty: specialty of the provider derived from the Medicare code reported on the claims\nBrandName: brand name of the drug filled\nGenericName: generic name/chemical ingredient of the drug filled\nNumberClaims: number of Medicare Part D claims filled (includes original prescriptions and refills)\nNumber30DayFills: aggregated number of Medicare Part D standardized 30-day fills (number of days supplied dived by 30; if &lt; 1.0, bottom-coded as 1.0; if &gt; 12.0, top-coded as 12.0\nNumberDaysSupply: aggregated number of day’s supply for which the drug is dispersed\nTotalDrugCost: aggregated drug cost paid for all associated claims\nNumberMedicareBeneficiaries: total number of unique Medicare Part D beneficiaries with at least one claim for the drug\nNumberClaims65Older: number of Medicare Part D claims for beneficiaries age 65 and older\nNumber30DayFills65Older: number of Medicare Part D standardized 30-day fills for beneficiaries age 65 and older (see Number30DayFills for standardized definition)\nTotalDrugCost65Older: aggregated total drug cost paid for all associated claims for beneficiaries age 65 and older\nNumberDaysSupply65Older: aggregated number of day’s supply for which this drug was dispensed, for beneficiaries age 65 and older\nNumberMedicareBeneficiaries65Older: number of unique Medicare Part D beneficiaries age 65 and older with at least one claim for the drug\nType: type of drug used: Brand or Generic\nOpioidFlag: whether the drug is an opioid or not an opioid\nSpecialtyCateg: provider specialty in broader categories (see Specialty variable)"
  },
  {
    "objectID": "health/eda/prescriptions.html#starter-code",
    "href": "health/eda/prescriptions.html#starter-code",
    "title": "EDA project: opioid prescriptions and claims",
    "section": "Starter code",
    "text": "Starter code\n\nlibrary(tidyverse)\nprescriptions &lt;- read_csv(\"https://raw.githubusercontent.com/36-SURE/36-SURE.github.io/main/data/prescriptions.csv\")"
  },
  {
    "objectID": "health/project/report-template.html#introduction",
    "href": "health/project/report-template.html#introduction",
    "title": "Title",
    "section": "Introduction",
    "text": "Introduction\nDescribe the problem and why it is important."
  },
  {
    "objectID": "health/project/report-template.html#data",
    "href": "health/project/report-template.html#data",
    "title": "Title",
    "section": "Data",
    "text": "Data\nDescribe the data you’re using in detail, where you accessed it, along with relevant exploratory data analysis (EDA). You should also include descriptions of any major data pre-processing/cleaning steps."
  },
  {
    "objectID": "health/project/report-template.html#methods",
    "href": "health/project/report-template.html#methods",
    "title": "Title",
    "section": "Methods",
    "text": "Methods\nDescribe the modeling techniques you chose, their assumptions, justifications for why they are appropriate for the problem, and your plan for comparison/evaluation approaches."
  },
  {
    "objectID": "health/project/report-template.html#results",
    "href": "health/project/report-template.html#results",
    "title": "Title",
    "section": "Results",
    "text": "Results\nDescribe your results. This can include tables and plots showing your results, as well as text describing how your models worked and the appropriate interpretations of the relevant output. (Note: Don’t just write out the textbook interpretations of all model coefficients! Instead, interpret the output that is relevant for your question of interest that is framed in the introduction)"
  },
  {
    "objectID": "health/project/report-template.html#discussion",
    "href": "health/project/report-template.html#discussion",
    "title": "Title",
    "section": "Discussion",
    "text": "Discussion\nGive your conclusions and summarize what you have learned with regards to your question of interest. Are there any limitations with the approaches you used? What do you think are the next steps to follow-up your project?"
  },
  {
    "objectID": "health/project/report-template.html#appendix-a-quick-tutorial",
    "href": "health/project/report-template.html#appendix-a-quick-tutorial",
    "title": "Title",
    "section": "Appendix: A quick tutorial",
    "text": "Appendix: A quick tutorial\n(Feel free to remove this section when you submit)\nThis a Quarto document. To learn more about Quarto see https://quarto.org. You can use the Render button to see what it looks like in HTML.\n\nText formatting\nText can be bolded with double asterisks and italicized with single asterisks. Monospace text, such as for short code snippets, uses backticks. (Note these are different from quotation marks or apostrophes.) Links are written like this.\nBulleted lists can be written with asterisks:\n\nEach item starts on a new line with an asterisk.\nItems should start on the beginning of the line.\nLeave blank lines after the end of the list so the list does not continue.\n\nMathematics can be written with LaTeX syntax using dollar signs. For instance, using single dollar signs we can write inline math: (-b \\pm \\sqrt{b^2 - 4ac})/2a.\nTo write math in “display style”, i.e. displayed on its own line centered on the page, we use double dollar signs: \nx^2 + y^2 = 1\n\n\n\nCode blocks\nCode blocks are evaluated sequentially when you hit Render. As the code runs, R prints out which block is running, so naming blocks is useful if you want to know which one takes a long time. After the block name, you can specify chunk options. For example, echo controls whether the code is printed in the document. By default, output is printed in the document in monospace:\n\n\n                   mpg cyl disp  hp drat    wt  qsec vs am gear carb\nMazda RX4         21.0   6  160 110 3.90 2.620 16.46  0  1    4    4\nMazda RX4 Wag     21.0   6  160 110 3.90 2.875 17.02  0  1    4    4\nDatsun 710        22.8   4  108  93 3.85 2.320 18.61  1  1    4    1\nHornet 4 Drive    21.4   6  258 110 3.08 3.215 19.44  1  0    3    1\nHornet Sportabout 18.7   8  360 175 3.15 3.440 17.02  0  0    3    2\nValiant           18.1   6  225 105 2.76 3.460 20.22  1  0    3    1\n\n\nChunk options can also be written inside the code block, which is helpful for really long options, as we’ll see soon.\n\n\n                   mpg cyl disp  hp drat    wt  qsec vs am gear carb\nMazda RX4         21.0   6  160 110 3.90 2.620 16.46  0  1    4    4\nMazda RX4 Wag     21.0   6  160 110 3.90 2.875 17.02  0  1    4    4\nDatsun 710        22.8   4  108  93 3.85 2.320 18.61  1  1    4    1\nHornet 4 Drive    21.4   6  258 110 3.08 3.215 19.44  1  0    3    1\nHornet Sportabout 18.7   8  360 175 3.15 3.440 17.02  0  0    3    2\nValiant           18.1   6  225 105 2.76 3.460 20.22  1  0    3    1\n\n\n\n\nFigures\nIf a code block produces a plot or figure, this figure will automatically be inserted inline in the report. That is, it will be inserted exactly where the code block is.\n\n\n\n\n\nThis is a caption. It should explain what’s in the figure and what’s interesting about it. For instance: There is a negative, strong linear correlation between miles per gallon and horsepower for US cars in the 1970s.\n\n\n\n\nNotice the use of fig-width and fig-height to control the figure’s size (in inches). These control the sizes given to R when it generates the plot, so R proportionally adjusts the font sizes to be large enough.\n\n\nTables\nUse the knitr::kable() function to print tables as HTML:\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nmpg\ncyl\ndisp\nhp\ndrat\nwt\nqsec\nvs\nam\ngear\ncarb\n\n\n\n\nMazda RX4\n21.0\n6\n160\n110\n3.90\n2.620\n16.46\n0\n1\n4\n4\n\n\nMazda RX4 Wag\n21.0\n6\n160\n110\n3.90\n2.875\n17.02\n0\n1\n4\n4\n\n\nDatsun 710\n22.8\n4\n108\n93\n3.85\n2.320\n18.61\n1\n1\n4\n1\n\n\nHornet 4 Drive\n21.4\n6\n258\n110\n3.08\n3.215\n19.44\n1\n0\n3\n1\n\n\nHornet Sportabout\n18.7\n8\n360\n175\n3.15\n3.440\n17.02\n0\n0\n3\n2\n\n\n\n\n\nWe can summarize model results with a table. For instance, suppose we fit a linear regression model:\n\nmodel1 &lt;- lm(mpg ~ disp + hp + drat, data = mtcars)\n\nIt is not appropriate to simply print summary(model1) into the report. If we want the reader to understand what models we have fit and what their results are, we should provide a nicely formatted table. A simple option is to use the tidy() function from the broom package to get a data frame of the model fit, and simply report that as a table.\n\n\n\nPredicting fuel economy using vehicle features. \n\n\nTerm\nEstimate\nSE\nt\np\n\n\n\n\n(Intercept)\n19.34\n6.37\n3.04\n0.01\n\n\ndisp\n-0.02\n0.01\n-2.05\n0.05\n\n\nhp\n-0.03\n0.01\n-2.34\n0.03\n\n\ndrat\n2.71\n1.49\n1.83\n0.08"
  },
  {
    "objectID": "health/project/description.html",
    "href": "health/project/description.html",
    "title": "Project Guidelines",
    "section": "",
    "text": "Each student has been allocated into a project group (of up to 4 students). Each group has been assigned a specific project research topic. Your goal is to complete the required project deliverables and checkpoints, in accordance with the guidelines detailed in the remainder of this document."
  },
  {
    "objectID": "health/project/description.html#your-task",
    "href": "health/project/description.html#your-task",
    "title": "Project Guidelines",
    "section": "",
    "text": "Each student has been allocated into a project group (of up to 4 students). Each group has been assigned a specific project research topic. Your goal is to complete the required project deliverables and checkpoints, in accordance with the guidelines detailed in the remainder of this document."
  },
  {
    "objectID": "health/project/description.html#deliverables",
    "href": "health/project/description.html#deliverables",
    "title": "Project Guidelines",
    "section": "Deliverables",
    "text": "Deliverables\nThis project has the following three key deliverables DUE FRIDAY, JULY 19 AT 6PM ET.\n(NOTE: It is important to finish everything by the July 19 deadline, due to the Minneapolis trip the week of July 22.)\n\n1. Report\n[template] (right click and choose “Save Link As…” to download)\nDUE FRIDAY, JULY 19 AT 6PM ET\nYour report should be written using Quarto and submitted as a rendered .html file. We recommend using an IDMRaD (Introduction, Data, Methods, Results and Discussion) report format, with details provided in the report template.\n\n\n2. Poster\n[template] (Google Slides link)\nDUE FRIDAY, JULY 19 AT 6PM ET\nYour poster should be submitted as a .pdf file. We will then make a printed copy for the poster session on the final day (July 26).\n\n\n3. Slides\nDUE FRIDAY, JULY 19 AT 6PM ET\nEach group will give an 8-minute presentation on the final day (July 26). The presentation should effectively have the same structure as your report with an introduction, followed by data description, an overview of methods, followed by results and discussion. Your slides may be created in any software, but we ony accept submissions in the form of a .pdf file, a Google Slides link, or a Quarto presentation (self-contained .html file or hosted online)."
  },
  {
    "objectID": "health/project/description.html#checkpoints",
    "href": "health/project/description.html#checkpoints",
    "title": "Project Guidelines",
    "section": "Checkpoints",
    "text": "Checkpoints\nCheckpoint 1: 5-minute presentation during lab on July 1\nNote: It is perfectly fine if you don’t have any results at this point\nYour first checkpoint presentation should be structured as follows.\n\nIntroduction (1 slide): Describe your project topic/question(s) and why it is important\nData (1 slide): Data description, any preliminary data pre-processing/cleaning steps\nEDA (2 slides max): 1–2 EDA plots related to your question(s) of interest\n\nDesign the slides using the assertion-evidence model\n\nMethods (1 slide): Early thoughts on methods/modeling strategy. Justify why it might be appropriate to answer your question(s) of interest\n\n\n\nPlan of action (1 slide): List all the steps needed to complete your project (be specific). Highlight the completed steps. What are the next steps?\n\nCheckpoint 2: 8-minute presentation during lab on July 12\nYour second checkpoint presentation should be structured as follows.\n\nIntroduction (1 slide): Describe your project topic/question(s) and why it is important\nData: (1 slide) Data description and any major data pre-processing/cleaning steps\nPlan of action (1 slide): List all the steps needed to complete your project (be specific). Highlight the completed steps.\nPresent the completed steps (5 slides max): methods, plots, findings, etc.\n\nDesign the slides using the assertion-evidence model\n\nPlan of action (1 slide, use the same one as before): what are the steps still to be completed?"
  },
  {
    "objectID": "health/project/description.html#project-topics",
    "href": "health/project/description.html#project-topics",
    "title": "Project Guidelines",
    "section": "Project Topics",
    "text": "Project Topics\n(Note: All five topics below contribute to an overall theme of “the impact of race, social and demographic factors on health, survival, and mortality.”)\nProject 1: Premature deaths (TA: Princess)\nDo income inequality, unemployment and high school completion rates affect the number of premature deaths of certain racial groups at the county level?\nProject 2: Preventable hospital stays (TA: Nick)\nDo income inequality, unemployment and high school completion rates affect the number of preventable hospital stays of certain racial groups at the county level?\nProject 3: Mental health (TA: Nick)\nDo the number of mental health professionals per county affect the number of poor mental health days?\nProject 4: Drug overdose and alcohol related deaths (TA: Princess)\nAre there demographic and social factors that are predictors of drug overdose and alcohol-related incidents (e.g., driving accidents)?\nProject 5: Influences on childhood outcomes (TA: Akshay)\nHow are juvenile healthcare outcomes impacted by adult health-related practices (e.g., smoking, drinking, diet)?"
  },
  {
    "objectID": "health/project/description.html#analysis",
    "href": "health/project/description.html#analysis",
    "title": "Project Guidelines",
    "section": "Analysis",
    "text": "Analysis\nYour analysis should focus on both:\nExploratory data analysis: Create visualizations to explore the underlying structure of the data and gain insights about distributions and relationships between variables. These should be ideally based on reasoned hypotheses.\nStatistical modeling: Demonstrate the use of statistical and machine learning modeling techniques. This may involve justifications for your choice of model (e.g., comparison with model specifications such as using different predictors, or with other methods), and then any relevant interpretation of the model with regards to your project’s topic. Depending on your project, the model(s) you rely on may be used for either an inference (i.e., interpreting coefficients) or prediction task. The model you choose just needs to be motivated by your question of interest."
  },
  {
    "objectID": "health/project/description.html#data",
    "href": "health/project/description.html#data",
    "title": "Project Guidelines",
    "section": "Data",
    "text": "Data\nRequired: County Health Rankings Data\n\nThe County Health Rankings Data—collected by the University of Wisconsin Population Health Institute—ranks every county in each state on their Health Outcomes and Health Factors.\nThis dataset also contains the measurements used to calculate the rankings for each county. More information can be found here.\nYou must (at minimum) use the 2024 ranking measures which can provide more insight into the most recent health ranking outcomes priorities. This can be used to better shape your project topic and related hypotheses.\nYour analysis should mainly be done at the entire United States scale (as feasible). However, you are welcome to focus on some specific counties/states to test more granular spatial hypotheses.\n\nOptional: additional suggestions and data sources\n\nConsider doing a temporal or trend analysis for your analyses, as the County Health Rankings Data are typically collected over time.\n\nFor predictive modeling, consider adding time-varying features or forecasting an outcome, with suitable uncertainty quantification.\n\nConsider merging the County Health Rankings Data with other publicly available datasets.\n\nExample sources include US Census data (accessed via the tidycensus R package) and COVID-19 data (can be accssed via the covidcast R package).\n\n\n(Note: All data used must be publicly available.)"
  },
  {
    "objectID": "health.html",
    "href": "health.html",
    "title": "Health",
    "section": "",
    "text": "Note: To download a .qmd file, right click and choose “Save Link As…”.\n\n\n\nDate\nTitle\nMaterials\n\n\n\n\nJune 3\nGetting started with R\n.html   .qmd\n\n\nJune 4\nData wrangling\n.html   .qmd\n\n\nJune 5\nData visualization\n.html   .qmd\n\n\nJune 6\nUsing GitHub for project collaboration\n.html"
  },
  {
    "objectID": "health.html#labs",
    "href": "health.html#labs",
    "title": "Health",
    "section": "",
    "text": "Note: To download a .qmd file, right click and choose “Save Link As…”.\n\n\n\nDate\nTitle\nMaterials\n\n\n\n\nJune 3\nGetting started with R\n.html   .qmd\n\n\nJune 4\nData wrangling\n.html   .qmd\n\n\nJune 5\nData visualization\n.html   .qmd\n\n\nJune 6\nUsing GitHub for project collaboration\n.html"
  },
  {
    "objectID": "health.html#eda-projects",
    "href": "health.html#eda-projects",
    "title": "Health",
    "section": "EDA projects",
    "text": "EDA projects\n\n\n\nProject 1\nHospital ratings\ndescription\n\n\nProject 2\nMaternal healthcare disparities\ndescription\n\n\nProject 3\nOpioid prescriptions and claims\ndescription\n\n\nProject 4\nRegistered nurses\ndescription\n\n\nProject 5\nCOVID cases and deaths\ndescription\n\n\nProject 6\nCOVID hospitalizations\ndescription"
  },
  {
    "objectID": "health.html#resources",
    "href": "health.html#resources",
    "title": "Health",
    "section": "Resources",
    "text": "Resources\n\nCMU Delphi\nPHIGHT COVID\nR for Public Health\nCRAN Task View: Epidemiology\nEpiverse and Epiverse-TRACE\nThe Epidemiologist R Handbook\nThe R4Epis project\nThe R Epidemics Consortium (RECON)\npharmaverse\nR/Pharma Conference\nR/Medicine workshops: 2023, 2022"
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "SURE 2024",
    "section": "",
    "text": "Welcome to the Summer Undergraduate Research Experience in Statistics (SURE) 2024, hosted by the Department of Statistics & Data Science at Carnegie Mellon University. This program focuses on statistics and data science methodology with applications in healthcare and sports analytics.\nOn this site, the Lectures tab contains all lecture slides. The Health and Sports tabs contain materials for health and sports labs, respectively."
  },
  {
    "objectID": "sports/labs/02-wrangling.html",
    "href": "sports/labs/02-wrangling.html",
    "title": "Lab: data wrangling",
    "section": "",
    "text": "Our data are usually stored as a .csv file and after loading a .csv file into RStudio, we will have a “data frame”. A data frame can be considered a special case of matrix where each column represents a measurement or variable of interest for each observation which correspond to the rows of the dataset. After loading the tidyverse suite of packages, we use the read_csv() function to load the 2024 NBA regular season stats dataset from yesterday:\n\nlibrary(tidyverse)\n\n── Attaching core tidyverse packages ──────────────────────── tidyverse 2.0.0 ──\n✔ dplyr     1.1.4     ✔ readr     2.1.5\n✔ forcats   1.0.0     ✔ stringr   1.5.1\n✔ ggplot2   3.5.1     ✔ tibble    3.2.1\n✔ lubridate 1.9.3     ✔ tidyr     1.3.1\n✔ purrr     1.0.2     \n── Conflicts ────────────────────────────────────────── tidyverse_conflicts() ──\n✖ dplyr::filter() masks stats::filter()\n✖ dplyr::lag()    masks stats::lag()\nℹ Use the conflicted package (&lt;http://conflicted.r-lib.org/&gt;) to force all conflicts to become errors\n\nnba_stats &lt;- read_csv(\"https://raw.githubusercontent.com/36-SURE/36-SURE.github.io/main/data/nba_stats.csv\")\n\nRows: 657 Columns: 23\n── Column specification ────────────────────────────────────────────────────────\nDelimiter: \",\"\nchr  (3): player, position, team\ndbl (20): age, games, games_started, minutes_played, field_goals, field_goal...\n\nℹ Use `spec()` to retrieve the full column specification for this data.\nℹ Specify the column types or set `show_col_types = FALSE` to quiet this message.\n\n\nBy default, read_csv() reads in the dataset as a tbl (aka tibble) object instead of a data.frame object. You can read about the differences here, but it’s not that meaningful for purposes.\nWe can use the functions slice_head() and slice_tail() to view a sample of the data. Use the slice_head() function to view the first 6 rows, then use the slice_tail() function to view the last 3 rows:\n\n# INSERT CODE HERE\n\nView the dimensions of the data with dim():\n\n# INSERT CODE HERE\n\nQuickly view summary statistics for all variables with the summary() function:\n\n# Uncomment the following code by deleting the # at the front\n# summary(nba_stats)\n\nView the data structure types with str():\n\n# str(nba_stats)\n\nWhat’s the difference between the output from the two functions?"
  },
  {
    "objectID": "sports/labs/02-wrangling.html#reading-and-previewing-data",
    "href": "sports/labs/02-wrangling.html#reading-and-previewing-data",
    "title": "Lab: data wrangling",
    "section": "",
    "text": "Our data are usually stored as a .csv file and after loading a .csv file into RStudio, we will have a “data frame”. A data frame can be considered a special case of matrix where each column represents a measurement or variable of interest for each observation which correspond to the rows of the dataset. After loading the tidyverse suite of packages, we use the read_csv() function to load the 2024 NBA regular season stats dataset from yesterday:\n\nlibrary(tidyverse)\n\n── Attaching core tidyverse packages ──────────────────────── tidyverse 2.0.0 ──\n✔ dplyr     1.1.4     ✔ readr     2.1.5\n✔ forcats   1.0.0     ✔ stringr   1.5.1\n✔ ggplot2   3.5.1     ✔ tibble    3.2.1\n✔ lubridate 1.9.3     ✔ tidyr     1.3.1\n✔ purrr     1.0.2     \n── Conflicts ────────────────────────────────────────── tidyverse_conflicts() ──\n✖ dplyr::filter() masks stats::filter()\n✖ dplyr::lag()    masks stats::lag()\nℹ Use the conflicted package (&lt;http://conflicted.r-lib.org/&gt;) to force all conflicts to become errors\n\nnba_stats &lt;- read_csv(\"https://raw.githubusercontent.com/36-SURE/36-SURE.github.io/main/data/nba_stats.csv\")\n\nRows: 657 Columns: 23\n── Column specification ────────────────────────────────────────────────────────\nDelimiter: \",\"\nchr  (3): player, position, team\ndbl (20): age, games, games_started, minutes_played, field_goals, field_goal...\n\nℹ Use `spec()` to retrieve the full column specification for this data.\nℹ Specify the column types or set `show_col_types = FALSE` to quiet this message.\n\n\nBy default, read_csv() reads in the dataset as a tbl (aka tibble) object instead of a data.frame object. You can read about the differences here, but it’s not that meaningful for purposes.\nWe can use the functions slice_head() and slice_tail() to view a sample of the data. Use the slice_head() function to view the first 6 rows, then use the slice_tail() function to view the last 3 rows:\n\n# INSERT CODE HERE\n\nView the dimensions of the data with dim():\n\n# INSERT CODE HERE\n\nQuickly view summary statistics for all variables with the summary() function:\n\n# Uncomment the following code by deleting the # at the front\n# summary(nba_stats)\n\nView the data structure types with str():\n\n# str(nba_stats)\n\nWhat’s the difference between the output from the two functions?"
  },
  {
    "objectID": "sports/labs/02-wrangling.html#data-manipulation-with-dplyr",
    "href": "sports/labs/02-wrangling.html#data-manipulation-with-dplyr",
    "title": "Lab: data wrangling",
    "section": "Data manipulation with dplyr",
    "text": "Data manipulation with dplyr\nAn easier way to manipulate the data frame is through the dplyr package, which is in the tidyverse suite of packages. The operations we can do include: selecting specific columns, filtering for rows, re-ordering rows, adding new columns and summarizing data. The “split-apply-combine” concept can be achieved by dplyr.\n\nSelecting columns with select()\nThe function select() can be use to select certain column with the column names. First create a new table called nba_stats_pg that only contains the player and games columns:\n\n# INSERT CODE HERE\n\nTo select all columns except a specific column, use the - (subtraction) operator. For example, view the output from uncommenting the following line of code:\n\n# select(nba_stats, -player)\n\nTo select a range of columns by name (that are in consecutive order), use the : (colon) operator. For example, view the output from uncommenting the following line of code:\n\n# select(nba_stats, player:games)\n\nTo select all columns that start with certain character strings, use the function starts_with(). Other matching options are:\n\nends_with(): select columns that end with a character string\ncontains(): select columns that contain a character string\nmatches(): select columns that match a regular expression\none_of(): select columns names that are from a group of names\n\n\n# Uncomment the following lines of code\n# select(nba_stats, starts_with(\"three\"))\n# select(nba_stats, contains(\"throw\"))\n\n\n\nExtracting rows using filter()\nWe can also extract the rows/observations that satisfy certain criteria. Try extracting the rows with more than 500 assists:\n\n# INSERT CODE HERE\n\nWe can also filter on multiple criteria. Subset the rows with age above 30 and the team is either “HOU” or “GSW”:\n\n# INSERT CODE HERE\n\n\n\nArranging rows using arrange()\nTo arrange the data frame by a specific order we need to use the function arrange(). The default is by increasing order and the desc() function will provide the decreasing order. First arrange the nba_stats table by personal_fouls in ascending order:\n\n# INSERT CODE HERE\n\nNext by descending order:\n\n# INSERT CODE HERE\n\nTry combining a pipeline of select(), filter(), and arrange() steps together with the |&gt; operator by:\n\nSelecting the player, team, age, and games columns,\nFilter to select only rows with games above 50,\nSort by age in descending order\n\n\n# INSERT CODE HERE\n\n\n\nCreating new columns using mutate()\nSometimes the data does not include the variable that we are interested in and we need to manipulate the current variables to add new variables into the data frame. Create a new column fouls_per_game by taking the personal_fouls and dividing by games (reassign this output to the nba_stats table following the commented code chunk so this column is added to the table):\n\n# nba_stats &lt;- nba_stats |&gt;\n#   mutate(INSERT CODE HERE)\n\n\n\nCreating summaries with summarize()\nTo create summary statistics for a given column in the data frame, we can use summarize() function. Compute the mean, min, and max number of assists:\n\n# INSERT CODE HERE\n\nThe advantage of summarize() is more obvious if we combine it with group_by(), the group operators. Since players at the different position tend to have very different statistics, first group_by() position and then compute the same summary statistics for assists:\n\n# INSERT CODE HERE"
  },
  {
    "objectID": "sports/labs/03-visualization.html",
    "href": "sports/labs/03-visualization.html",
    "title": "Lab: data visualization",
    "section": "",
    "text": "Let’s start again by reading in the data from yesterday using the read_csv() function after loading the tidyverse:\n\nlibrary(tidyverse)\n\n── Attaching core tidyverse packages ──────────────────────── tidyverse 2.0.0 ──\n✔ dplyr     1.1.4     ✔ readr     2.1.5\n✔ forcats   1.0.0     ✔ stringr   1.5.1\n✔ ggplot2   3.5.1     ✔ tibble    3.2.1\n✔ lubridate 1.9.3     ✔ tidyr     1.3.1\n✔ purrr     1.0.2     \n── Conflicts ────────────────────────────────────────── tidyverse_conflicts() ──\n✖ dplyr::filter() masks stats::filter()\n✖ dplyr::lag()    masks stats::lag()\nℹ Use the conflicted package (&lt;http://conflicted.r-lib.org/&gt;) to force all conflicts to become errors\n\nnba_stats &lt;- read_csv(\"https://raw.githubusercontent.com/36-SURE/36-SURE.github.io/main/data/nba_stats.csv\")\n\nRows: 657 Columns: 23\n── Column specification ────────────────────────────────────────────────────────\nDelimiter: \",\"\nchr  (3): player, position, team\ndbl (20): age, games, games_started, minutes_played, field_goals, field_goal...\n\nℹ Use `spec()` to retrieve the full column specification for this data.\nℹ Specify the column types or set `show_col_types = FALSE` to quiet this message."
  },
  {
    "objectID": "sports/labs/03-visualization.html#reading-in-data",
    "href": "sports/labs/03-visualization.html#reading-in-data",
    "title": "Lab: data visualization",
    "section": "",
    "text": "Let’s start again by reading in the data from yesterday using the read_csv() function after loading the tidyverse:\n\nlibrary(tidyverse)\n\n── Attaching core tidyverse packages ──────────────────────── tidyverse 2.0.0 ──\n✔ dplyr     1.1.4     ✔ readr     2.1.5\n✔ forcats   1.0.0     ✔ stringr   1.5.1\n✔ ggplot2   3.5.1     ✔ tibble    3.2.1\n✔ lubridate 1.9.3     ✔ tidyr     1.3.1\n✔ purrr     1.0.2     \n── Conflicts ────────────────────────────────────────── tidyverse_conflicts() ──\n✖ dplyr::filter() masks stats::filter()\n✖ dplyr::lag()    masks stats::lag()\nℹ Use the conflicted package (&lt;http://conflicted.r-lib.org/&gt;) to force all conflicts to become errors\n\nnba_stats &lt;- read_csv(\"https://raw.githubusercontent.com/36-SURE/36-SURE.github.io/main/data/nba_stats.csv\")\n\nRows: 657 Columns: 23\n── Column specification ────────────────────────────────────────────────────────\nDelimiter: \",\"\nchr  (3): player, position, team\ndbl (20): age, games, games_started, minutes_played, field_goals, field_goal...\n\nℹ Use `spec()` to retrieve the full column specification for this data.\nℹ Specify the column types or set `show_col_types = FALSE` to quiet this message."
  },
  {
    "objectID": "sports/labs/03-visualization.html#previewing-the-data",
    "href": "sports/labs/03-visualization.html#previewing-the-data",
    "title": "Lab: data visualization",
    "section": "Previewing the data",
    "text": "Previewing the data\nWrite code that displays the column names of nba_stats. Also, look at the first six rows of your dataset to get an idea of what these variables look like. Which variables are quantitative, and which are categorical?\n\n# INSERT CODE HERE"
  },
  {
    "objectID": "sports/labs/03-visualization.html#always-make-a-bar-chart",
    "href": "sports/labs/03-visualization.html#always-make-a-bar-chart",
    "title": "Lab: data visualization",
    "section": "Always make a bar chart…",
    "text": "Always make a bar chart…\nNow we’ll use the ggplot() function to create a bar chart of the position variable. To make things easier, we provide the code for you to do this below; just uncomment the code and run it to create the bar graph. In what follows, you must answer some questions about the code and plot.\n\n# Create the bar graph of position:\n# nba_stats |&gt;\n#   ggplot(aes(x = position)) +\n#   geom_bar(fill = \"darkblue\") +\n#   labs(title = \"Number of NBA players by position\",\n#        x = \"Position\",\n#        y = \"Number of players\",\n#        caption = \"Source: Basketball-Reference.com\")\n\nAnswer the following questions about the code and plot:\n\nIn general, ggplot() code takes the following format: ggplot(blank1, aes(x = blank2)). Looking at the above code, what kind of R object should blank1 be, and what should blank2 be?\nWhat do you think the line geom_bar(fill = \"darkblue\") does?\nWhat do you think the remaining lines of code do (contained in labs())?"
  },
  {
    "objectID": "sports/labs/03-visualization.html#more-area-plots-but-bar-charts-are-better",
    "href": "sports/labs/03-visualization.html#more-area-plots-but-bar-charts-are-better",
    "title": "Lab: data visualization",
    "section": "More area plots (but bar charts are better!)",
    "text": "More area plots (but bar charts are better!)\nNow we’ll make a few other area plots:\n\nspine chart\npie chart\nrose diagram\n\nYour goal for this part is to create each of these plots. These plots can be created by copy-and-pasting the bar chart code from above and modifying it slightly. Follow these directions to create each of these plots:\n\nspine chart: First, copy-and-paste the bar chart code from above. Then, delete the fill = \"darkblue\" within geom_bar(). Finally, within ggplot(), replace aes(x = position) with aes(x = \"\", fill = position). Also, change the labels in labs() if necessary.\n\n\n# PUT YOUR SPINE CHART CODE HERE\n\n\npie chart: First, copy-and-paste the spine chart code you just made. Then, after geom_bar(), “add” coord_polar(\"y\"). Be sure to put plus signs before and after coord_polar(\"y\"). Also, change the labels in labs() if necessary.\n\n\n# PUT YOUR PIE CHART CODE HERE\n\n\nrose diagram: First, copy-and-paste your original bar chart code. Then, after geom_bar(fill = \"darkblue\"), “add” coord_polar() + scale_y_sqrt(). Be sure to put plus signs before and after coord_polar() + scale_y_sqrt(). Also, change the labels in labs() if necessary. After you make the rose diagram: In 1-2 sentences, what do you think scale_y_sqrt() does, and what is a benefit to including scale_y_sqrt() when making the rose diagram?\n\n\n# PUT YOUR ROSE DIAGRAM CODE HERE"
  },
  {
    "objectID": "sports/labs/03-visualization.html#notes-on-colors-in-plots",
    "href": "sports/labs/03-visualization.html#notes-on-colors-in-plots",
    "title": "Lab: data visualization",
    "section": "Notes on colors in plots",
    "text": "Notes on colors in plots\nThree types of color scales to work with:\n\nQualitative: distinguishing discrete items that don’t have an order (nominal categorical). Colors should be distinct and equal with none standing out unless otherwise desired for emphasis.\n\n\nDo NOT use a discrete scale on a continuous variable\n\n\nSequential: when data values are mapped to one shade, e.g., for an ordered categorical variable or low to high continuous variable\n\n\nDo NOT use a sequential scale on an unordered variable\n\n\nDivergent: think of it as two sequential scales with a natural midpoint midpoint could represent 0 (assuming +/- values) or 50% if your data spans the full scale\n\n\nDo NOT use a divergent scale on data without natural midpoint\n\n\nOptions for ggplot2 colors\nThe default color scheme is pretty bad to put it bluntly, but ggplot2 has ColorBrewer built in which makes it easy to customize your color scales. For instance, we can make a scatterplot with three_pointers on the y-axis and offensive_rebounds on the x-axis and using the geom_point() layer with each point colored by position:\n\nnba_stats |&gt;\n  ggplot(aes(x = offensive_rebounds, y = three_pointers, color = position)) +\n  geom_point(alpha = 0.5) +\n  labs(x = \"Offensive rebounds\", \n       y = \"Three-pointers\",\n       color = \"Position\") +\n  theme_light()\n\n\n\n\nWhat does alpha change? We can change the color plot for this plot using scale_color_brewer() function:\n\nnba_stats |&gt;\n  ggplot(aes(x = offensive_rebounds, y = three_pointers,\n             color = position)) +\n  geom_point(alpha = 0.5) +\n  scale_color_brewer(palette = \"Set2\") +\n  labs(x = \"Offensive rebounds\", \n       y = \"Three-pointers\",\n       color = \"Position\") +\n  theme_light()\n\n\n\n\nWhich do you prefer, the default palette or this new one? You can check out more color palettes here.\nSomething you should keep in mind is to pick a color-blind friendly palette. One simple way to do this is by using the ggthemes package (you need to install it first before running this code!) which has color-blind friendly palettes included:\n\nnba_stats |&gt;\n  ggplot(aes(x = offensive_rebounds, y = three_pointers, color = position)) +\n  geom_point(alpha = 0.5) +\n  # call the function directly from the package using `::` instead of library(ggthemes)\n  ggthemes::scale_color_colorblind() +\n  labs(x = \"Offensive rebounds\", \n       y = \"Three-pointers\",\n       color = \"Position\") +\n  theme_light()\n\n\n\n\nIn terms of displaying color from low to high, the viridis scales are excellent choices (and are also color-blind friendly!). For instance, we can map another continuous variable (minutes_played) to the color:\n\nnba_stats |&gt;\n  ggplot(aes(x = offensive_rebounds, y = three_pointers, color = minutes_played)) +\n  geom_point(alpha = 0.5) +\n  scale_color_viridis_c() +\n  labs(x = \"Offensive rebounds\", \n       y = \"Three-pointers\",\n       color = \"Minutes played\") +\n  theme_light()\n\n\n\n\nWhat does this reveal about the plot? What happens if you delete scale_color_viridis_c() + from above? Which do you prefer?"
  },
  {
    "objectID": "sports/labs/03-visualization.html#notes-on-themes",
    "href": "sports/labs/03-visualization.html#notes-on-themes",
    "title": "Lab: data visualization",
    "section": "Notes on themes",
    "text": "Notes on themes\nYou might have noticed above have various changes to the theme of plots for customization. You will constantly be changing the theme of your plots to optimize the display. Fortunately, there are a number of built-in themes you can use to start with rather than the default theme_gray():\n\nnba_stats |&gt;\n  ggplot(aes(x = offensive_rebounds, y = three_pointers, color = minutes_played)) +\n  geom_point(alpha = 0.5) +\n  scale_color_viridis_c() +\n  labs(x = \"Offensive rebounds\", \n       y = \"Three-pointers\",\n       color = \"Minutes played\") +\n  theme_gray()\n\n\n\n\nFor instance, Quang’s go-to theme is theme_light()\n\nnba_stats |&gt;\n  ggplot(aes(x = offensive_rebounds, y = three_pointers, color = minutes_played)) +\n  geom_point(alpha = 0.5) +\n  scale_color_viridis_c() +\n  labs(x = \"Offensive rebounds\", \n       y = \"Three-pointers\",\n       color = \"Minutes played\") +\n  theme_light()\n\n\n\n\nThere are options such as theme_minimal():\n\nnba_stats |&gt;\n  ggplot(aes(x = offensive_rebounds, y = three_pointers, color = minutes_played)) +\n  geom_point(alpha = 0.5) +\n  scale_color_viridis_c() +\n  labs(x = \"Offensive rebounds\", \n       y = \"Three-pointers\",\n       color = \"Minutes played\") +\n  theme_minimal()\n\n\n\n\nor theme_classic():\n\nnba_stats |&gt;\n  ggplot(aes(x = offensive_rebounds, y = three_pointers, color = minutes_played)) +\n  geom_point(alpha = 0.5) +\n  scale_color_viridis_c() +\n  labs(x = \"Offensive rebounds\", \n       y = \"Three-pointers\",\n       color = \"Minutes played\") +\n  theme_classic()\n\n\n\n\nor theme_bw():\n\nnba_stats |&gt;\n  ggplot(aes(x = offensive_rebounds, y = three_pointers, color = minutes_played)) +\n  geom_point(alpha = 0.5) +\n  scale_color_viridis_c() +\n  labs(x = \"Offensive rebounds\", \n       y = \"Three-pointers\",\n       color = \"Minutes played\") +\n  theme_bw()\n\n\n\n\nThere are also packages with popular, such as the ggthemes package which includes, for example, theme_economist():\n\nlibrary(ggthemes)\nnba_stats |&gt;\n  ggplot(aes(x = offensive_rebounds, y = three_pointers, color = minutes_played)) +\n  geom_point(alpha = 0.5) +\n  scale_color_viridis_c() +\n  labs(x = \"Offensive rebounds\", \n       y = \"Three-pointers\",\n       color = \"Minutes played\") +\n  theme_economist()\n\n\n\n\nand theme_fivethirtyeight(), to name a couple:\n\nnba_stats |&gt;\n  ggplot(aes(x = offensive_rebounds, y = three_pointers, color = minutes_played)) +\n  geom_point(alpha = 0.5) +\n  scale_color_viridis_c() +\n  labs(x = \"Offensive rebounds\", \n       y = \"Three-pointers\",\n       color = \"Minutes played\") +\n  theme_fivethirtyeight()\n\n\n\n\nWith any theme you have picked, you can then modify specific components directly using the theme() layer. There are many aspects of the plot’s theme to modify, such as my decision to move the legend to the bottom of the figure, drop the legend title, and increase the font size for the y-axis:\n\nnba_stats |&gt;\n  ggplot(aes(x = offensive_rebounds, y = three_pointers, color = minutes_played)) +\n  geom_point(alpha = 0.5) +\n  scale_color_viridis_c() +\n  labs(x = \"Offensive rebounds\", \n       y = \"Three-pointers\",\n       title = \"Joint distribution of three-pointers and offensive rebounds\",\n       subtitle = \"NBA statistics from 2021-2022 season\",\n       color = \"Minutes played\") +\n  theme_light() +\n  theme(legend.position = \"bottom\",\n        legend.title = element_blank(),\n        axis.text.y = element_text(size = 14),\n        axis.text.x = element_text(size = 6))\n\n\n\n\nIf you’re tired of explicitly customizing every plot in the same way all the time, then you should make a custom theme. It’s quite easy to make a custom theme for ggplot2 and of course there are an incredible number of ways to customize your theme. Below, we modify theme_bw() using the %+replace% argument to a new customized theme named theme_cus() - which is stored as a function:\n\ntheme_cus &lt;- function() {\n  # start with the base font size\n  theme_bw(base_size = 10) %+replace%\n    theme(\n      panel.background  = element_blank(),\n      plot.background = element_rect(fill = \"transparent\", color = NA), \n      legend.position = \"bottom\",\n      legend.background = element_rect(fill = \"transparent\", color = NA),\n      legend.key = element_rect(fill = \"transparent\", color = NA),\n      axis.ticks = element_blank(),\n      panel.grid.major = element_line(color = \"grey90\", linewidth = 0.3), \n      panel.grid.minor = element_blank(),\n      plot.title = element_text(size = 15, hjust = 0, vjust = 0.5, face = \"bold\", \n                                margin = margin(b = 0.2, unit = \"cm\")),\n      plot.subtitle = element_text(size = 12, hjust = 0, vjust = 0.5, \n                                   margin = margin(b = 0.2, unit = \"cm\")),\n      plot.caption = element_text(size = 7, hjust = 1, face = \"italic\", \n                                  margin = margin(t = 0.1, unit = \"cm\")),\n      axis.text.x = element_text(size = 13),\n      axis.text.y = element_text(size = 13)\n    )\n}\n\nCreate the plot from before with this theme:\n\nnba_stats |&gt;\n  ggplot(aes(x = offensive_rebounds, y = three_pointers, color = minutes_played)) +\n  geom_point(alpha = 0.5) +\n  scale_color_viridis_c() +\n  labs(x = \"Offensive rebounds\", \n       y = \"Three-pointers\",\n       title = \"Joint distribution of three-pointers and offensive rebounds\",\n       subtitle = \"NBA statistics from 2021-2022 season\",\n       color = \"Minutes played\") +\n  theme_cus()"
  },
  {
    "objectID": "sports/labs/05-regression.html",
    "href": "sports/labs/05-regression.html",
    "title": "Lab: linear regression",
    "section": "",
    "text": "We continue to use the 2024 NBA regular season stats dataset from previous labs.\nIn the NBA, and more generally in team sports, a coach must make decisions about how many minutes each player should play. Typically, these decisions are informed by a player’s skills, along with other factors such as fatigue, matchups, etc. Our goal is to use measurements of a few (quantifiable) player attributes to predict the minutes per game a player plays. In particular, we will focus on the following data, measured over the 2024 NBA regular season for over 400 players:\n\nplayer: names of each player (not useful for modeling purposes, but just for reference)\nmin_per_game: our response variable, measuring the minutes per game a player played during the 2024 NBA regular season.\nfield_goal_percentage: potential (continuous) explanatory variable, calculated as (number of made field goals) / (number of field goals attempted).\nfree_throw_percentage: potential (continuous) explanatory variable, calculated as (number of made free throws) / (number of free throws attempted).\nthree_point_percentage: potential (continuous) explanatory variable, calculated as (number of made 3 point shots) / (number of 3 point shots attempted),\nage: potential (continuous/discrete) explanatory variable, player’s reported age for the 2024 season,\nposition: potential (categorical) explanatory variable, one of SG (shooting guard), PG (point guard), C (center), PF (power forward) or SF (small forward).\n\nExecute the following code chunk to (a) load the necessary data for this lab, (b) compute variables we will use in this lab, (c) remove players with missing data (just to simplify things), and (d) subset out players with low minute totals (fewer than 250 minutes played in a season):\n\nlibrary(tidyverse)\nnba_stats &lt;- read_csv(\"https://raw.githubusercontent.com/36-SURE/36-SURE.github.io/main/data/nba_stats.csv\")\n\nnba_stats &lt;- nba_stats |&gt;\n  # summarize player stats across multiple teams they played for\n  group_by(player) |&gt;\n  summarize(\n    age = first(age),\n    position = first(position),\n    games = sum(games, na.rm = TRUE),\n    minutes_played = sum(minutes_played, na.rm = TRUE),\n    field_goals = sum(field_goals, na.rm = TRUE),\n    field_goal_attempts = sum(field_goal_attempts, na.rm = TRUE),\n    three_pointers = sum(three_pointers, na.rm = TRUE),\n    three_point_attempts = sum(three_point_attempts, na.rm = TRUE),\n    free_throws = sum(free_throws, na.rm = TRUE),\n    free_throw_attempts = sum(free_throw_attempts, na.rm = TRUE)\n  ) |&gt;\n  ungroup() |&gt;\n  mutate(\n    field_goal_percentage = field_goals / field_goal_attempts,\n    three_point_percentage = three_pointers / three_point_attempts,\n    free_throw_percentage = free_throws / free_throw_attempts,\n    min_per_game = minutes_played / games\n  ) |&gt;\n  # remove rows with missing missing values\n  drop_na() |&gt;\n  filter(minutes_played &gt; 250)"
  },
  {
    "objectID": "sports/labs/05-regression.html#data",
    "href": "sports/labs/05-regression.html#data",
    "title": "Lab: linear regression",
    "section": "",
    "text": "We continue to use the 2024 NBA regular season stats dataset from previous labs.\nIn the NBA, and more generally in team sports, a coach must make decisions about how many minutes each player should play. Typically, these decisions are informed by a player’s skills, along with other factors such as fatigue, matchups, etc. Our goal is to use measurements of a few (quantifiable) player attributes to predict the minutes per game a player plays. In particular, we will focus on the following data, measured over the 2024 NBA regular season for over 400 players:\n\nplayer: names of each player (not useful for modeling purposes, but just for reference)\nmin_per_game: our response variable, measuring the minutes per game a player played during the 2024 NBA regular season.\nfield_goal_percentage: potential (continuous) explanatory variable, calculated as (number of made field goals) / (number of field goals attempted).\nfree_throw_percentage: potential (continuous) explanatory variable, calculated as (number of made free throws) / (number of free throws attempted).\nthree_point_percentage: potential (continuous) explanatory variable, calculated as (number of made 3 point shots) / (number of 3 point shots attempted),\nage: potential (continuous/discrete) explanatory variable, player’s reported age for the 2024 season,\nposition: potential (categorical) explanatory variable, one of SG (shooting guard), PG (point guard), C (center), PF (power forward) or SF (small forward).\n\nExecute the following code chunk to (a) load the necessary data for this lab, (b) compute variables we will use in this lab, (c) remove players with missing data (just to simplify things), and (d) subset out players with low minute totals (fewer than 250 minutes played in a season):\n\nlibrary(tidyverse)\nnba_stats &lt;- read_csv(\"https://raw.githubusercontent.com/36-SURE/36-SURE.github.io/main/data/nba_stats.csv\")\n\nnba_stats &lt;- nba_stats |&gt;\n  # summarize player stats across multiple teams they played for\n  group_by(player) |&gt;\n  summarize(\n    age = first(age),\n    position = first(position),\n    games = sum(games, na.rm = TRUE),\n    minutes_played = sum(minutes_played, na.rm = TRUE),\n    field_goals = sum(field_goals, na.rm = TRUE),\n    field_goal_attempts = sum(field_goal_attempts, na.rm = TRUE),\n    three_pointers = sum(three_pointers, na.rm = TRUE),\n    three_point_attempts = sum(three_point_attempts, na.rm = TRUE),\n    free_throws = sum(free_throws, na.rm = TRUE),\n    free_throw_attempts = sum(free_throw_attempts, na.rm = TRUE)\n  ) |&gt;\n  ungroup() |&gt;\n  mutate(\n    field_goal_percentage = field_goals / field_goal_attempts,\n    three_point_percentage = three_pointers / three_point_attempts,\n    free_throw_percentage = free_throws / free_throw_attempts,\n    min_per_game = minutes_played / games\n  ) |&gt;\n  # remove rows with missing missing values\n  drop_na() |&gt;\n  filter(minutes_played &gt; 250)"
  },
  {
    "objectID": "sports/labs/05-regression.html#eda",
    "href": "sports/labs/05-regression.html#eda",
    "title": "Lab: linear regression",
    "section": "1. EDA",
    "text": "1. EDA\nSpend time exploring the dataset, to visually assess which of the explanatory variables listed above is most associated with our response the minutes played per game (min_per_game). Create scatterplots between the response and each continuous explanatory variable. Does any of the relationships appear to be linear? Describe the direction and strength of the association between the explanatory and response variables.\nIn your opinion, which of the possible continuous explanatory variables displays the strongest relationship with minutes per game?\nCreate an appropriate visualization comparing the distribution of minutes per game by position. Do you think there is a relationship between minutes per game and position?"
  },
  {
    "objectID": "sports/labs/05-regression.html#fit-a-simple-linear-model",
    "href": "sports/labs/05-regression.html#fit-a-simple-linear-model",
    "title": "Lab: linear regression",
    "section": "2. Fit a simple linear model",
    "text": "2. Fit a simple linear model\nNow that you’ve performed some EDA, it’s time to actually fit some linear models to the data. Start the variable you think displays the strongest relationship with the response variable. Update the following code by replacing INSERT_VARIABLE with your selected variable, and run to fit the model:\n\ninit_nba_lm &lt;- lm(min_per_game ~ INSERT_VARIABLE, data = nba_stats)\n\nBefore check out the summary() of this model, you need to check the diagnostics to see if it meets the necessary assumptions. To do this you can try running plot(init_nba_lm) in the console (what happens?). Equivalently, another way to make the same plots but with ggplot2 perks is with the ggfortify package by running the following code:\n\nlibrary(ggfortify) # install.packages(\"ggfortify\")\ninit_nba_lm |&gt; \n  autoplot() +\n  theme_light()\n\nThe first plot is residuals vs. fitted: this plot should NOT display any clear patterns in the data, no obvious outliers, and be symmetric around the horizontal line at zero. The smooth line provided is just for reference to see how the residual average changes. Do you see any obvious patterns in your plot for this model?\nThe second plot is a Q-Q plot (see page 93 for more details). Without getting too much into the math behind them, the closer the observations are to the dashed reference line, the better your model fit is. It is bad for the observations to diverge from the dashed line in a systematic way; that means we are violating the assumption of normality discussed in lecture. How do your points look relative to the dashed reference line?\nThe third plot looks at the square root of the absolute value of the standardized residuals. We want to check for homoskedascity of errors (equal, constant variance). If we did have constant variance, what would we expect to see? What does your plot look like?\nThe fourth plot is residuals vs. leverage which helps us identify influential points. Leverage quantifies the influence the observed response for a particular observation has on its predicted value, i.e. if the leverage is small then the observed response has a small role in the value of its predicted response, while a large leverage indicates the observed response plays a large role in the predicted response. It’s a value between 0 and 1, where the sum of all leverage values equals the number of coefficients (including the intercept). Specifically the leverage for observation \\(i\\) is computed as \\[\nh_{ii} = \\frac{1}{n} + \\frac{(x_i - \\bar{x})^2}{\\sum_i^n (x_i - \\bar{x})^2}\n\\] where \\(\\bar{x}\\) is the average value for variable \\(x\\) across all observations. See page 191 for more details on leverage and the regression hat matrix. We’re looking for points in the upper right or lower right corners, where dashed lines for Cook’s distance values would indicate potential outlier points that are displaying too much influence on the model results. Do you observed any such influential points in upper or lower right corners?\nWhat is your final assessment of the diagnostics, do you believe all assumptions are met? Any potential outlier observations to remove?"
  },
  {
    "objectID": "sports/labs/05-regression.html#assess-the-model-summary",
    "href": "sports/labs/05-regression.html#assess-the-model-summary",
    "title": "Lab: linear regression",
    "section": "3. Assess the model summary",
    "text": "3. Assess the model summary\nFollowing the example in lecture, interpret the results from the summary() function on your initial model. Do you think there is sufficient evidence to reject the null hypothesis that the coefficient is 0? What is the interpretation of the \\(R^2\\) value? Compare the square root of the raw (unadjusted) \\(R^2\\) of your linear model to the correlation between that explanatory variable and the response using the cor() function. What do you notice?\nTo assess the fit of a linear model, we can also plot the predicted values vs the actual values, to see how closely our predictions align with reality, and to decide whether our model is making any systematic errors. Execute the following code chunk to show the actual minutes per game against our model’s predictions:\n\nnba_stats |&gt;\n  mutate(init_preds = predict(init_nba_lm)) |&gt;\n  ggplot(aes(x = init_preds, y = min_per_game)) +\n  geom_point(alpha = 0.75) +\n  geom_abline(slope = 1, intercept = 0, linetype = \"dashed\", color = \"red\") +\n  labs(x = \"Predictions\", y = \"Observed minutes per game\") +\n  theme_light()"
  },
  {
    "objectID": "sports/labs/05-regression.html#include-multiple-covariates",
    "href": "sports/labs/05-regression.html#include-multiple-covariates",
    "title": "Lab: linear regression",
    "section": "4. Include multiple covariates",
    "text": "4. Include multiple covariates\nRepeat steps 2 and 3 above but including more than one variable in your model. You can easily do this in the lm() function by adding another variable to the formula with the + operator as so (but just replace the INSERT_VARIABLE parts):\n\nmulti_nba_lm &lt;- lm(min_per_game ~ INSERT_VARIABLE_1 + INSERT_VARIABLE_2, data = nba_stats)\n\nExperiment with different sets of the continuous variables. What sets of continuous variables do you think model minutes per game best? (Remember to use the Adjusted \\(R^2\\) when comparing models that have different numbers of variables).\nBeware of collinearity! Load the car library (install it if necessary!) and use the vif() function to check for possible (multi)collinearity. The vif() function computes the variance inflation factor (VIF) for predictor \\(x_j\\) with \\(j \\in 1,\\dots, p\\) as \\[\n\\text{VIF}_j = \\frac{1}{1 - R^2_j}\n\\] where \\(R^2_j\\) is the \\(R^2\\) from a variable with variable \\(x_j\\) as the response and the other \\(p-1\\) predictors as the explanatory variables. VIF values close to 1 indicate the variable is not correlated with other predictors, while VIF values over 5 indicate strong presence of collinearity. If present, remove a variable with VIF over 5, and redo the fit. Repeat this process until the vif() outputs are all less than 5. The follow code chunk displays an example of using this function:\n\nlibrary(car) # install.packages(\"car\")\nvif(multi_nba_lm)"
  },
  {
    "objectID": "sports/labs/05-regression.html#linear-model-with-one-categorical-variable",
    "href": "sports/labs/05-regression.html#linear-model-with-one-categorical-variable",
    "title": "Lab: linear regression",
    "section": "5. Linear model with one categorical variable",
    "text": "5. Linear model with one categorical variable\nRun the following code to fit a model using only the position variable:\n\npos_nba_lm &lt;- lm(min_per_game ~ position, data = nba_stats)\n\nNext, use the following code to first create a column called pos_preds containing the predictions of the model above, to display the predictions of this model against the actual observed minutes per game, but facet by the player’s position:\n\nnba_stats |&gt;\n  mutate(pos_preds = predict(pos_nba_lm)) |&gt;\n  ggplot(aes(x = min_per_game, y = pos_preds)) +\n  geom_point(alpha = 0.5) +\n  facet_wrap(~ position, ncol = 3) +\n  labs(x = \"Actual minutes per game\", \n       y = \"Predicted minutes per game\") +\n  theme_light()\n\nAs the figure above, we are changing the intercept of our regression line by including a categorical variable. To make this more clear, view the output of the summary:\n\nsummary(pos_nba_lm)\n\nNotice how only four coefficients are provided in addition to the intercept. This is because, by default, R turns the categorical variables of \\(m\\) levels (e.g. we have 5 positions in this dataset) into \\(m - 1\\) indicator variables (binary with values of 1 if in that level versus 0 if not that level) for different categories relative to a baseline level. In this example, R has created an indicator for four positions: PF, PG, SF, and SG. By default, R will use alphabetical order to determine the baseline category, which in this example the center position C. The values for the coefficient estimates indicate the expected change in the response variable relative to the baseline. In other words, the intercept term gives us the baseline’s average y, e.g. the average minutes per game for centers. This matches what you displayed in the predictions against observed minutes/game scatterplots by position above.\nBeware the default baseline R picks for categorical variables! We typically want to choose the baseline level to be the group with the most observations. In this example, each position has a similar number of observations so the results are reasonable. But in general, we can change the reference level by modifying the factor levels of the categorical variables (similar to how we reorder things in ggplot2). For example, after viewing table(nba_stats$position) we see how the SG position has the most observations. We can use the following code to modify the position variable so that SG is the baseline (we can use fct_relevel() to update position so that SG is the first factor level - and we do not need to modify the order of the remaining levels):\n\nnba_stats &lt;- nba_stats |&gt;\n  mutate(position = fct_relevel(position, \"SG\")) \n\nRefit the linear regression model using position above, how has the summary changed?"
  },
  {
    "objectID": "sports/labs/05-regression.html#linear-model-with-one-categorical-and-one-continuous-variable",
    "href": "sports/labs/05-regression.html#linear-model-with-one-categorical-and-one-continuous-variable",
    "title": "Lab: linear regression",
    "section": "6. Linear model with one categorical and one continuous variable",
    "text": "6. Linear model with one categorical and one continuous variable\nPick a single continuous variable from yesterday, use it to replace INSERT_VARIABLE below, then run the code to fit a model with the position included:\n\nx_pos_nba_lm &lt;- lm(min_per_game ~ position + INSERT_VARIABLE, data = nba_stats)\n\nCreate scatterplots with your predictions on the y-axis, your INSERT_VARIABLE on the x-asis, and color by position. What do you observe?\nGiven similarities between different types of positions, we can easily collapse the positions together into a smaller number of categories using fct_collapse():\n\nnba_stats &lt;- nba_stats |&gt;\n  mutate(\n    position_group = fct_collapse(position, Guard = c(\"SG\", \"PG\"), Forward = c(\"SF\", \"PF\"), Center = \"C\")\n  ) \n\nRefit the model with this new position_group variable, but assign it to a different name, e.g. x_pos_group_nba_lm. What changed in the summary?"
  },
  {
    "objectID": "sports/labs/05-regression.html#interactions",
    "href": "sports/labs/05-regression.html#interactions",
    "title": "Lab: linear regression",
    "section": "7. Interactions",
    "text": "7. Interactions\nRemember with ggplot2 you can directly compute and plot the results from running linear regression using geom_smooth() or stat_smooth() and specifying that method = \"lm\". Try running the following code (replace INSERT_VARIABLE) to generate the linear regression fits with geom_smooth versus your own model’s predictions (note the different y mapping for the point versus smooth layers):\n\nnba_stats |&gt;\n  mutate(pos_preds = predict(x_pos_nba_lm)) |&gt;\n  ggplot(aes(x = INSERT_VARIABLE, color = position)) +\n  geom_point(aes(y = pos_preds), alpha = 0.5) +\n  geom_smooth(aes(y = min_per_game), method = \"lm\") \n  facet_wrap(~ position, ncol = 3) +\n  labs(x = \"INSERT YOUR LABEL HERE\", \n       y = \"Predicted minutes per game\") +\n  theme_light()\n\nThe geom_smooth() regression lines do NOT match! This is because ggplot2 is fitting separate regressions for each position, meaning the slope for the continuous variable on the x-axis is changing for each position. We can match the output of the geom_smooth() results with interactions. We can use interaction terms to build more complex models. Interaction terms allow for a different linear model to be fit for each category; that is, they allow for different slopes across different categories. If we believe relationships between continuous variables, and outcomes, differ across categories, we can use interaction terms to better model these relationships.\nTo fit a model with an interaction term between two variables, include the interaction via the * operator like so:\n\npos_int_nba_lm &lt;- lm(min_per_game ~ position * INSERT_VARIABLE, data = nba_stats)\n\nReplace the predictions in the previous plot’s mutate code with this interaction model’s predictions. How do they compare to the results from geom_smooth() now?\nYou can model interactions between any type of variables using the * operator, feel free to experiment on your different possible continuous variables."
  },
  {
    "objectID": "sports/labs/05-regression.html#polynomials",
    "href": "sports/labs/05-regression.html#polynomials",
    "title": "Lab: linear regression",
    "section": "8. Polynomials",
    "text": "8. Polynomials\nAnother way to increase the explanatory power of your model is to include transformations of continuous variables. For instance you can directly create a column that is a square of a variable with mutate() and then fit the regression with the original variable and its squared term:\n\nnba_stats &lt;- nba_stats |&gt;\n  mutate(fg_perc_squared = field_goal_percentage ^ 2)\nsquared_fg_lm &lt;- lm(min_per_game ~ field_goal_percentage + fg_perc_squared, data = nba_stats)\nsummary(squared_fg_lm)\n\nWhat are some difficulties with interpreting this model fit? View the predictions for this model or other covariates you squared."
  },
  {
    "objectID": "sports/labs/05-regression.html#training-and-testing",
    "href": "sports/labs/05-regression.html#training-and-testing",
    "title": "Lab: linear regression",
    "section": "9. Training and testing",
    "text": "9. Training and testing\nAs we’ve seen, using transformations such as higher-order polynomials may decrease the interpretability and increase the potential for overfitting associated with our models; however, they can also dramatically improve the explanatory power.\nWe need a way for making sure our more complicated models have not overly fit to the noise present in our data. Another way of saying this is that a good model should generalize to a different sample than the one on which it was fit. This intuition motivates the idea of training/testing. We split our data into two parts, use one part – the training set – to fit our models, and the other part – the testing set – to evaluate our models. Any model which happens to fit to the noise present in our training data should perform poorly on our testing data.\nThe first thing we will need to do is split our sample. Run the following code chunk to divide our data into two halves, which we will refer to as a training set and a test set. Briefly summarize what each line in the code chunk is doing.\n\nnba_train &lt;- nba_stats |&gt; \n  slice_sample(prop = 0.5, replace = FALSE)\nnba_test &lt;- nba_stats |&gt; \n  anti_join(nba_train)\n\nWe will now compare three candidate models for predicting minutes played using position and field goal percentage. We will fit these models on the training data only, ignoring the testing data for the moment. Run the below two code chunks to create two candidate models:\n\n# model with interaction terms\ncandidate_model_1 &lt;- lm(min_per_game ~ position * poly(field_goal_percentage, 2, raw = TRUE), \n                        data = nba_train)\n\n\n# model without interaction terms\ncandidate_model_2 &lt;- lm(min_per_game ~ position + poly(field_goal_percentage, 2, raw = TRUE), \n                        data = nba_train)\n\n(Note: The poly() function is useful for getting higher-order polynomial transformations of variables.)\nUsing summary(), which of these models has more explanatory power according to the training data? Which of the models is less likely to overfit?\nFit another model to predict minutes per game using a different set of variables or polynomials.\nNow that we’ve built our candidate models, we will evaluate them on our test set, using the criterion of mean squared error (MSE). Run the following code chunk to compute, on the test set, the MSE of predictions given by the first model compared to the actual minutes played.\n\nmodel_1_preds &lt;- predict(candidate_model_1, newdata = nba_test)\nmodel_1_mse &lt;- mean((model_1_preds - nba_test$min_per_game) ^ 2)\n\nDo this for each of your candidate models. Compare the MSE on the test set, which model performed best (in terms of lowest test MSE)?"
  },
  {
    "objectID": "sports/labs/06-regularization.html",
    "href": "sports/labs/06-regularization.html",
    "title": "Lab: regularization",
    "section": "",
    "text": "The purpose of this lab is to walk through the basics of a regularized adjusted plus-minus (RAPM) model to estimate the impact of basketball players when they are on the court, while adjusting for the quality of their teammates and opponents. We will use a dataset that is already in the wide design matrix form with indicator columns for every player that was observed during the regular season. You can find the script for building this dataset here, which relies on the hoopR package to obtain the data.\nThe following code chunk reads in the data, which is in a wide format.\n\n# library(tidyverse)\n# nba_rapm_data &lt;- read_csv(\"https://github.com/36-SURE/36-SURE.github.io/raw/main/data/nba_2223_season_rapm_data.csv.gz\")\n# nba_rapm_data\n\nIn this dataset, we have 32,358 unique shifts/stints with 539 players represented by the indicator variables (1 if on court for home team, -1 if on court for away team, and 0 if not on court). Additional context is captured by the following variables:\n\ngame_id: unique game ID\nstint_id: unique identifier within a game for a stint for particular combination of home and away lineup (in appearance of order, where 1 is the first stint in the game)\nn_pos: number of possessions (combined for both home and away) during the observed stint\nhome_points: number of points scored by the home team during the stint\naway_points: number of points scored by the away team during the stint\nminutes: length of the stint in terms of minutes played\nmargin: common response for RAPM models, defined as: (home_points - away_points) / n_pos * 100"
  },
  {
    "objectID": "sports/labs/06-regularization.html#introduction",
    "href": "sports/labs/06-regularization.html#introduction",
    "title": "Lab: regularization",
    "section": "",
    "text": "The purpose of this lab is to walk through the basics of a regularized adjusted plus-minus (RAPM) model to estimate the impact of basketball players when they are on the court, while adjusting for the quality of their teammates and opponents. We will use a dataset that is already in the wide design matrix form with indicator columns for every player that was observed during the regular season. You can find the script for building this dataset here, which relies on the hoopR package to obtain the data.\nThe following code chunk reads in the data, which is in a wide format.\n\n# library(tidyverse)\n# nba_rapm_data &lt;- read_csv(\"https://github.com/36-SURE/36-SURE.github.io/raw/main/data/nba_2223_season_rapm_data.csv.gz\")\n# nba_rapm_data\n\nIn this dataset, we have 32,358 unique shifts/stints with 539 players represented by the indicator variables (1 if on court for home team, -1 if on court for away team, and 0 if not on court). Additional context is captured by the following variables:\n\ngame_id: unique game ID\nstint_id: unique identifier within a game for a stint for particular combination of home and away lineup (in appearance of order, where 1 is the first stint in the game)\nn_pos: number of possessions (combined for both home and away) during the observed stint\nhome_points: number of points scored by the home team during the stint\naway_points: number of points scored by the away team during the stint\nminutes: length of the stint in terms of minutes played\nmargin: common response for RAPM models, defined as: (home_points - away_points) / n_pos * 100"
  },
  {
    "objectID": "sports/labs/06-regularization.html#adjusted-plus-minus-apm",
    "href": "sports/labs/06-regularization.html#adjusted-plus-minus-apm",
    "title": "Lab: regularization",
    "section": "Adjusted Plus-Minus (APM)",
    "text": "Adjusted Plus-Minus (APM)\nWe’ll first consider the classic Rosenbaum (2004) adjusted plus-minus (APM) model, which is weighted least-squares where:\n\nResponse variable is the score differential with respect to home team, i.e., home_points - away_points\nWeights are the number of posessions during the shift/stint, i.e., n_pos\n\nLet’s go ahead and fit this initial model (this might take a bit to run).\nFirst, compute the score differential as score_diff = home_points - away_points using mutate(). Append this new column to the data nba_rapm_data.\n\n# INSERT CODE HERE\n\nNext, create a new dataset named nba_apm_model_data that contains only the response score_diff, the weights n_pos, and the player columns.\n\n# INSERT CODE HERE\n\nNow, fit the model using the code below.\n\n# uncomment the code below\n# rosenbaum_model &lt;- lm(score_diff ~ 0 + . - n_pos, \n#                       data = INSERT CODE HERE,\n#                       weights = INSERT CODE HERE)\n# notice an intercept term is not included (that's what the 0 is there for)\n# `+ .` by itself means include everything as predictors\n# `+ . - n_pos` means include everything BUT n_pos \n\nWe’re not going to view the summary of this model since it is a bit of a mess. Instead, we’ll take advantage of the broom package to view the coefficients:\n\n# library(broom)\n# rosenbaum_coef &lt;- tidy(rosenbaum_model)\n# rosenbaum_coef\n\nObviously, in this current form we have no idea, we have no idea which player is which. Fortunately for you, there is another dataset with the names of the players to join using these IDs in the term column:\n\n# nba_player_table &lt;- read_csv(\"https://raw.githubusercontent.com/36-SURE/36-SURE.github.io/main/data/nba_2223_player_table.csv\")\n# nba_player_table\n\nYou’ll notice that this matches the number of rows as the rosenbaum_coef table. But we first need to modify the term column by removing the back-tick symbols and then converting the IDs to numeric values before joining:\n\n# rosenbaum_coef &lt;- rosenbaum_coef |&gt;\n#   mutate(term = as.numeric(str_remove_all(term, \"`\"))) |&gt; # convert term to numeric\n#   left_join(nba_player_table, by = c(\"term\" = \"player_id\")) # join to obtain player names\n\nWho are the top players based on this approach? Display the top 10 (in terms of estimate in the rosenbaum_coef data).\n\n# INSERT CODE HERE\n\nAnd the worst players? Display the bottom 10.\n\n# INSERT CODE HERE\n\nThese look like pretty extreme values, with the most extreme values observed by players that have limited playing time (upon searching their stats online).\nBefore we think about how to address these issues, let’s look at what happens if we make a slight tweak to our model by using the margin variable as the response instead.\nRepeat what we’ve done so far, but use margin in the original data nba_rapm_data as the response instead of score_diff.\n\n# INSERT CODE HERE\n\nDo we see player names that make sense? What do you notice about the magnitude for the coefficient estimates compared to the score differential model?\nLet’s quickly take a look at the distribution of the coefficients for the players. Display a histogram of estimate (for the new model with margin as the response). What do you notice about this distribution?\n\n# INSERT CODE HERE"
  },
  {
    "objectID": "sports/labs/06-regularization.html#regularized-adjusted-plus-minus-rapm",
    "href": "sports/labs/06-regularization.html#regularized-adjusted-plus-minus-rapm",
    "title": "Lab: regularization",
    "section": "Regularized Adjusted Plus-Minus (RAPM)",
    "text": "Regularized Adjusted Plus-Minus (RAPM)\n\nRidge RAPM\nIn order to address the common issues facing APM models, we can fit a RAPM model using ridge regression. The go-to approach for fitting ridge (as well as lasso and elastic-net models) is with the glmnet package. We can easily fit a ridge regression model with the RAPM design matrix. In order to tune the penalty \\(\\lambda\\), we will use the built-in cross-validation code with the cv.glmnet() function.\nFirst, grab only the player columns (i.e. the indicator variables in the original data), then convert to a matrix using as.matrix(), and store this as a new object named player_matrix.\n\n# INSERT CODE HERE\n\nNext, fit a ridge regression model with glmnet\n\n# library(glmnet)\n# help(cv.glmnet)\n\n# ridge with 10 fold cv, no intercept and no standardization\n# fit_ridge_cv &lt;- cv.glmnet(x = INSERT CODE HERE, \n#                           y = INSERT CODE HERE, \n#                           alpha = INSERT CODE HERE, \n#                           intercept = FALSE, \n#                           standardize = FALSE)\n\nWe can now view the penalty selection for this model:\n\n# plot(fit_ridge_cv)\n\nWe can also easily plot the path of the ridge regression shrinkage, to see how the coefficients are pulled towards 0 as the penalty increases. The following code chunk shows this full path:\n\n# plot(fit_ridge_cv$glmnet.fit, xvar = \"lambda\")\n\nUsing the broom package again, we can again make a tidy table of the coefficients for each player:\n\n# tidy_ridge_coef &lt;- tidy(fit_ridge_cv$glmnet.fit)\n# tidy_ridge_coef\n\nIf you look closely, this returns 100 rows for each player in the data - because it is returning the coefficient for each player at each value of the lambda penalty. We can filter to the values for the optimal choice of lambda based on the cross-validation results, and then join our player names as before:\n\n# rapm_ridge_coef &lt;- tidy_ridge_coef |&gt;\n#   filter(lambda == fit_ridge_cv$lambda.min) |&gt; # filter to the min lambda CV\n#   mutate(term = as.numeric(term)) |&gt;  # convert term to numeric\n#   left_join(INSERT CODE HERE) # join to obtain player names\n\nNow, display the top 10 players based on coefficient estimates. Does this list pass the eye test? (Who won the NBA MVP in 2023?)\n\n# INSERT CODE HERE\n\nAnd finally, let’s view the RAPM coefficient distribution (for comparison against the APM coefficients). Display a histogram of estimate from the model\n\n# INSERT CODE HERE\n\n\n\nLasso RAPM\nWe’ve just seen the benefits of using ridge regression to estimate player effects in the presence of collinearity. What happens if we use the lasso penalty instead of the ridge penalty?\nRepeat what we just did, but for a lasso regression model instead of ridge. (HINT: what do you need to set alpha = to?)\n\n# INSERT CODE HERE\n\nWho are the top 10 players based on lasso regression? Is this similar (player names and order) to what we got using ridge? Comment on how the lasso rankings and estimates compare to the ridge regression estimates."
  },
  {
    "objectID": "sports/labs/01-intro.html",
    "href": "sports/labs/01-intro.html",
    "title": "Lab: getting started with R",
    "section": "",
    "text": "NOTE: To preview this file, click the “Render” button in RStudio."
  },
  {
    "objectID": "sports/labs/01-intro.html#installing-r-and-rstudio",
    "href": "sports/labs/01-intro.html#installing-r-and-rstudio",
    "title": "Lab: getting started with R",
    "section": "Installing R and RStudio",
    "text": "Installing R and RStudio\n(Skip this part if you’ve already installed R and RStudio.)\nTo install R (latest release: 4.4.0), visit https://www.r-project.org and choose your system. Click on the download R link in the middle of the page under “Getting Started.” Download and install the installer files (executable, pkg, etc) that correspond to your system.\nAlthough you can use R without any integrated development environment (IDE), you will need to install RStudio, by far the most popular IDE for R, for this summer. Basically, it makes your life with R much easier and we will be using it throughout the program. To install RStudio, visit https://posit.co/download/rstudio-desktop and choose your system. The installer is preferred. If you have RStudio installed but not the latest version, just download the latest installer and install."
  },
  {
    "objectID": "sports/labs/01-intro.html#typical-workflow",
    "href": "sports/labs/01-intro.html#typical-workflow",
    "title": "Lab: getting started with R",
    "section": "Typical workflow",
    "text": "Typical workflow\n\nWriting R scripts\nYou can type R commands directly into the Console (lower left pane), but this can become quite tedious and annoying when your work becomes more complex. Instead, you can code in R Scripts. An R Script is a file type which R recognizes as storing R commands and is saved as a .R file. R Scripts are useful as we can edit our code before sending it to be run in the console.\nIn RStudio, to open a new R Script: File &gt; New File &gt; R Script.\n\n\nUsing Quarto\nAn Quarto file is a dynamic document for writing reproducible reports and communicating results. It contains the reproducible source code along with the narration that a reader needs to understand your work.\nThere are three important elements to a Quarto file:\n\nA YAML header at the top (surrounded by ---)\nChunks of R code surrounded by ```\nText mixed with simple text formatting like ## Heading and italics\n\n(Note that this file itself is a Quarto document.)\nIf you are familiar with the LaTeX syntax, math mode works like a charm in almost the same way:\n\\[\nf (x) = \\frac{1}{\\sqrt{2\\pi}} \\exp \\left( - \\frac{x^2}{2} \\right)\n\\]\nA chunk of embedded R code is the following:\n\n# R code here\nprint(\"Hello World\")\n\n[1] \"Hello World\"\n\n\nAll the lab documents will be Quarto files so you need to know how to render and convert them into a reader-friendly documents. We recommend to render as html file but if you have LaTeX installed, you can change the format to pdf.\nFor more details on Quarto, see the comprehensive manual online and the Quarto chapter of R for Data Science (2e). See also the guide on Markdown Basics for more on Markdown syntax. For code chunk options, see this guide."
  },
  {
    "objectID": "sports/labs/01-intro.html#installing-r-packages",
    "href": "sports/labs/01-intro.html#installing-r-packages",
    "title": "Lab: getting started with R",
    "section": "Installing R packages",
    "text": "Installing R packages\nR performs a wide variety of functions, such as data manipulation, modeling, and visualization. The extensive code base beyond the built-in functions are managed by packages created from numerous statisticians and developers. The Comprehensive R Archive Network (CRAN) manages the open-source distribution and the quality control of the R packages.\nTo install an R package, using the function install.packages and put the package name in the parentheses and the quote. While this is preferred, for those using RStudio, you can also go to “Tools” then “Install Packages” and then input the package name.\n\ninstall.packages(\"tidyverse\")\n\nImportant: NEVER install new packages in a code block in a .qmd file. That is, the install.packages() function should NEVER be in your code chunks (unless they are commented out using #). The library() function, however, will be used throughout your code: The library() function loads packages only after they are installed.\nIf in any time you get a message says: “Do you want to install from sources the package which needs compilation?” Choose “No” will tend to bring less troubles. (Note: This happens when the bleeding-edge version package is available, but not yet compiled for each OS distribution. In many case, you can just proceed without the source compilation.)\nEach package only needs to be installed once. Whenever you want to use functions defined in the package, you need to load the package with the command:\n\nlibrary(tidyverse)\n\nHere is a list of packages that we may need (but not limited to) in the following lectures and/or labs. Make sure you can install all of them. If you fail to install any package, please update R and RStudio first and check the error message for any other packages that need to install first.\n\nlibrary(tidyverse)\nlibrary(devtools)\nlibrary(ranger)\nlibrary(glmnet)"
  },
  {
    "objectID": "sports/labs/01-intro.html#basic-data-type-and-operators",
    "href": "sports/labs/01-intro.html#basic-data-type-and-operators",
    "title": "Lab: getting started with R",
    "section": "Basic data type and operators",
    "text": "Basic data type and operators\n\nData type: vector\nThe basic unit of R is a vector. A vector is a collection of values of the same type and the type could be:\n\nnumeric (double/integer number): digits with optional decimal point\n\n\nv1 &lt;- c(1, 5, 8.3, 0.02, 99999)\ntypeof(v1)\n\n[1] \"double\"\n\n\n\ncharacter: a string (or word) in double or single quotes, “…” or ’…’.\n\n\nv2 &lt;- c(\"apple\", \"banana\", \"3 chairs\", \"dimension1\", \"&gt;-&lt;\")\ntypeof(v2)\n\n[1] \"character\"\n\n\n\nlogical: TRUE and FALSE\n\n\nv3 &lt;- c(TRUE, FALSE, FALSE)\ntypeof(v3)\n\n[1] \"logical\"\n\n\nNote: Oftentimes, factor is used to encode a character vector into unique numeric vector.\n\nplayer_type &lt;- c(\"Batter\", \"Batter\", \"Hitter\", \"Batter\", \"Hitter\")\nplayer_type &lt;- factor(player_type)\nstr(player_type)\n\n Factor w/ 2 levels \"Batter\",\"Hitter\": 1 1 2 1 2\n\ntypeof(player_type)\n\n[1] \"integer\"\n\n\n\n\nData type: lists\nVector can store only single data type:\n\ntypeof(c(1, TRUE, \"apple\"))\n\n[1] \"character\"\n\n\nList is a vector of vectors which can store different data types of vectors:\n\nroster &lt;- list(\n  name = c(\"Quang\", \"Akshay\", \"Nick\", \"Princess\", \"Yuchen\", \"JungHo\", \"Daven\"),\n  role = c(\"Instructor\", \"TA\", \"TA\", \"TA\", \"TA\", \"TA\", \"TA\"),\n  is_TA = c(FALSE, TRUE, TRUE, TRUE, TRUE, TRUE, TRUE)\n)\nstr(roster)\n\nList of 3\n $ name : chr [1:7] \"Quang\" \"Akshay\" \"Nick\" \"Princess\" ...\n $ role : chr [1:7] \"Instructor\" \"TA\" \"TA\" \"TA\" ...\n $ is_TA: logi [1:7] FALSE TRUE TRUE TRUE TRUE TRUE ...\n\n\nR uses a specific type of list, data frame, containing the same number of rows with unique row names.\n\nstr(iris)\n\n'data.frame':   150 obs. of  5 variables:\n $ Sepal.Length: num  5.1 4.9 4.7 4.6 5 5.4 4.6 5 4.4 4.9 ...\n $ Sepal.Width : num  3.5 3 3.2 3.1 3.6 3.9 3.4 3.4 2.9 3.1 ...\n $ Petal.Length: num  1.4 1.4 1.3 1.5 1.4 1.7 1.4 1.5 1.4 1.5 ...\n $ Petal.Width : num  0.2 0.2 0.2 0.2 0.2 0.4 0.3 0.2 0.2 0.1 ...\n $ Species     : Factor w/ 3 levels \"setosa\",\"versicolor\",..: 1 1 1 1 1 1 1 1 1 1 ...\n\ntypeof(iris)\n\n[1] \"list\"\n\n\n\n\nOperators\nWe can perform element-wise actions on vectors through the operators:\n\narithmetic: +, -, *, /, ^ (for integer division, %/% is quotient, %% is remainder)\n\n\nv1 &lt;- c(1,2,3)\nv2 &lt;- c(4,5,6)\n\nv1 + v2\n\n[1] 5 7 9\n\nv1 * v2\n\n[1]  4 10 18\n\nv2 %% v1\n\n[1] 0 1 0\n\n\n\nrelation: &gt;, &gt;=, &lt; ,&lt;=, ==, !=\n\n\n5 &gt; 4\n\n[1] TRUE\n\n5 &lt;= 4\n\n[1] FALSE\n\n33 == 22\n\n[1] FALSE\n\n33 != 22\n\n[1] TRUE\n\n\n\nlogic: ! (not), & (and), | (or)\n\n\n(5 &gt; 6) | (2 &lt; 3)\n\n[1] TRUE\n\n(5 &gt; 6) & (2 &lt; 3)\n\n[1] FALSE\n\n!(5 &gt; 6) & (2 &lt; 3)\n\n[1] TRUE\n\n\n\nsequence: i:j (: operator, i and j are any two arbitrary numbers)\n\n\n1:5\n\n[1] 1 2 3 4 5\n\n5:1\n\n[1] 5 4 3 2 1\n\n-1:-5\n\n[1] -1 -2 -3 -4 -5\n\n-1:5\n\n[1] -1  0  1  2  3  4  5"
  },
  {
    "objectID": "sports/labs/01-intro.html#loading-.csv-files",
    "href": "sports/labs/01-intro.html#loading-.csv-files",
    "title": "Lab: getting started with R",
    "section": "Loading .csv files",
    "text": "Loading .csv files\nMost of the data provided to you are in .csv format. In the code chunk below, we use the read_csv() function (from the readr package, part of the tidyverse) to load a dataset that is saved in a folder located in the SURE GitHub repository. In quotations, insert the file path where the dataset is located, which in this case is online. However, typically you’ll save .csv files locally first and put them in an organized folder to access later.\n\nlibrary(tidyverse)\nnba_stats &lt;- read_csv(\"https://raw.githubusercontent.com/36-SURE/36-SURE.github.io/main/data/nba_stats.csv\")\nhead(nba_stats)"
  },
  {
    "objectID": "sports/labs/01-intro.html#looking-for-help",
    "href": "sports/labs/01-intro.html#looking-for-help",
    "title": "Lab: getting started with R",
    "section": "Looking for help",
    "text": "Looking for help\nIf you have any R problem, the best step is to use the help() function (or equivalently the ?). For example,\n\nhelp(str)\nhelp(lm)\n\nOr you can use the command ?…\n\n?str\n?lm\n\nDouble question marks can lead to a more general search.\n\n??predict\n\nYou should ALWAYS consult the R help documentation first before attempting to google around (or ask ChatGPT) for a solution."
  },
  {
    "objectID": "sports/labs/01-intro.html#exercises",
    "href": "sports/labs/01-intro.html#exercises",
    "title": "Lab: getting started with R",
    "section": "Exercises",
    "text": "Exercises\n\nCreate four vectors, v1 and v2 are numeric vectors, v3 is a character vector and v4 is a logic vector. Make sure the length of v1 and v2 are the same. (Hint: a way to check the length is to use the function length())\n\n\n# R code here\n\n\nPreform add, minus, product and division on v1 and v2.\n\n\n# R code here\n\n\nCreate four statements with both relation and logic operators, that 2 of them return TRUE and 2 of them return FALSE.\n\n\n# R code here\n\n\nCreate 2 sequences with length 20, one in an increasing order and the other in a decreasing order.\n\n\n# R code here\n\n\nThe following Batting dataset contains historical MLB statistics, accessed via the Lahman package (you will need to install it first!). How many of the players are in Double-A league (coded as AA in lgID)? How about American League (AL)? Can you summarize the counts for all leagues? (Hint: table())\n\n\nlibrary(Lahman)\ndata(Batting)\n\n# R code here"
  },
  {
    "objectID": "sports/labs/01-intro.html#text-formatting-in-quarto",
    "href": "sports/labs/01-intro.html#text-formatting-in-quarto",
    "title": "Lab: getting started with R",
    "section": "Text formatting in Quarto",
    "text": "Text formatting in Quarto\nThere are a lot of ways to format text in a Quarto document, e.g., italics and bold (just scan through this .qmd file to see how this was done). See this guide for more tips/tricks. In particular, check out the Markdown Basics and other guides under Authoring. See also this guide on R code chunk options.\nAs you’ll see throughout this summer (and especially with your project), well-formatted .html files can be a great way to showcase data science results to the public online. (Check out the project showcase from 2023 and 2022.)"
  },
  {
    "objectID": "sports/labs/01-intro.html#customizing-rstudio",
    "href": "sports/labs/01-intro.html#customizing-rstudio",
    "title": "Lab: getting started with R",
    "section": "Customizing RStudio",
    "text": "Customizing RStudio\nRStudio theme\nRStudio can be customized with different themes. To explore built-in themes,\n\nNavigate to the menu bar at the top of your screen\nChoose Tools &gt; Global Options &gt; Appearance\nChange your RStudio theme under Editor theme\n\n(FYI, Quang uses the Tomorrow Night Bright theme.)\nNote that within the Appearance tab, there are also options for changing your Editor font, Editor font size, etc.\nRStudio panes\nWithin RStudio, there are several panes (e.g., Console, Help, Environment, History, Plots, etc.). To customize, go to Tools &gt; Global Options &gt; Pane Layout, and arrange the panes as you see fit.\nFeel free to explore other options within the Tools &gt; Global Option menu."
  },
  {
    "objectID": "sports/labs/04-github.html",
    "href": "sports/labs/04-github.html",
    "title": "Using GitHub for project collaboration",
    "section": "",
    "text": "The purpose of this lab is to help you set up Git and GitHub on your computer, and use them to collaborate on your EDA project."
  },
  {
    "objectID": "sports/labs/04-github.html#goal",
    "href": "sports/labs/04-github.html#goal",
    "title": "Using GitHub for project collaboration",
    "section": "",
    "text": "The purpose of this lab is to help you set up Git and GitHub on your computer, and use them to collaborate on your EDA project."
  },
  {
    "objectID": "sports/labs/04-github.html#task-0-github-registration-and-git-installation",
    "href": "sports/labs/04-github.html#task-0-github-registration-and-git-installation",
    "title": "Using GitHub for project collaboration",
    "section": "Task 0: GitHub registration and Git installation",
    "text": "Task 0: GitHub registration and Git installation\n(Note: You were already being asked to complete these prior to the start of the program. Proceed to Task 1 if you already completed this task.)\nGitHub account. Register for a (free) GitHub account at https://github.com.  (if you already have a GitHub account, feel free to ignore this)\nDownload Git.  (if Git is already installed on your computer, the following instructions still hold for updating Git to its latest version)\n(Windows)\n\nGo to https://git-scm.com/download/win\nNavigate to “Click here to download” on the first line and click on it\nFollow the installation instructions\n\n(macOS)\n\nOpen the Terminal app on your computer (Finder \\(\\rightarrow\\) Applications \\(\\rightarrow\\) Terminal)\nGo to https://brew.sh and copy/paste the chunk under “Install Homebrew” to the Terminal\nOnce Homebrew is installed, type this into the Terminal: brew install git"
  },
  {
    "objectID": "sports/labs/04-github.html#task-1-git-configuration",
    "href": "sports/labs/04-github.html#task-1-git-configuration",
    "title": "Using GitHub for project collaboration",
    "section": "Task 1: Git configuration",
    "text": "Task 1: Git configuration\n\nMake sure you’ve already (i) created a GitHub account and (ii) installed Git on your computer\nYou then need to configure Git. This can be done directly in R:\n\n\n# uncomment and run the following line to install the usethis package\n# install.packages(\"usethis\")\nusethis::use_git_config(user.name = \"Your Name\", user.email = \"your-github@email.address\")\n\n\nUse your full name for the user.name field and the same email as your GitHub account for user.email\nYou then need to create a personal access token for authentication as follows:\n\n\nusethis::create_github_token()\n\n\nThis will direct you to the GitHub site on your browser (you may have to log in). On this site:\n\nUnder “Note”, type in some description for this token (e.g., “SURE 2024 GitHub token”)\nFor “Expiration”, set an expiration date for this token (e.g., 90 days) or make it permanent (i.e. choose “No expiration” if you don’t want to deal with this again in the future)\nUnder “Select scopes”, recommended scopes will be pre-selected. Stick with these for now.\n\nNext, click on “Generate token”\nCopy the token to your clipboard (or leave the browser window open, so you can come back to copy the token later)\nIn RStudio, run the following to get a prompt where you can paste your token:\n\n\n# uncomment and run the following line to install the gitcreds package\n# install.packages(\"gitcreds\")\ngitcreds::gitcreds_set()"
  },
  {
    "objectID": "sports/labs/04-github.html#task-2-eda-project-collaboration-with-git-and-github",
    "href": "sports/labs/04-github.html#task-2-eda-project-collaboration-with-git-and-github",
    "title": "Using GitHub for project collaboration",
    "section": "Task 2: EDA project collaboration with Git and GitHub",
    "text": "Task 2: EDA project collaboration with Git and GitHub\nMake sure every group member finishes Task 1 before proceeding to Task 2, which requires a group effort.\n\nStep 1: Create an EDA project repository on GitHub(Required for ONE group member only)\nEach group should elect ONE person to create a GitHub repository for the EDA project. This repository is to be shared among all group members.\nThe elected group member should do the following to create a new GitHub repository:\n\nAfter you’ve signed in to GitHub, go to https://github.com/new\nName the repository (give it a meaningful name)\nGive the repository a description (don’t leave this blank although this is optional)\nDecide whether to keep the repository public or private (for now, keep it public, so that we can review your code)\nClick on “Initialize this repository with a README”. For now, there’s no need to “Add .gitignore” or “Choose a license”\nClick on “Create Repository”\n\nNext, to add the rest of your group to the repository:\n\nGo to browser page of the GitHub repository you just created and click on “Settings”\nNavigate to the left sidebar and click on “Collaborators”\nClick on “Add people” (under Manage access). Enter the GitHub username for the other group members.\n\n\n\nStep 2: Clone the remote repository to your local computer(Required for ALL group members)\n\nEveryone (except the member responsible for creating the repository) should each get an invitation sent to the email associated with your GitHub account\nCheck your email and accept the invitation\nGo to the browser page for the EDA project GitHub repository (created in Step 1)\nClick on Code (in the same line as “Go to file”). Under HTTPS, copy the URL\nIn RStudio, click on File &gt; New Project.... Next, click on “Version Control” and then on “Git”. Paste the URL you just copied into “Repository URL”\nType the name for the folder on your computer associated with this repository into Project directory name\n\nYou can choose whatever name you want, but it is recommended to give a name similar to the repository name on GitHub\n\nMake sure “Create project as subdirectory of:” points to where you want to locate this new folder\nClick on “Create Project”\nAt this point, you should find that the “Files” pane (in the bottom right of RStudio) is listing the files in your local repository.\n\n\n\nStep 3: Modify the repository (Required for ALL group members)\nEach group member should create their own “sandbox” folder locally on their own computer as follows:\n\nNavigate to the Files pane in RStudio and click on “Folder” to create a new folder (for the new folder name, use your last name.)\nIn RStudio, open a new file (could be anything - e.g. R Script, Quarto document, etc.). Fill the file with some code/comments/etc. (This is just for illustration purpose, to show how you can add a file to GitHub from your computer)\nSave the file inside the folder you just created (with your last name as the folder name). At this point, this file should show up in the “Git” pane (in the top right of RStudio)\nCheck the box under “Staged” in the Git pane to stage the file for a commit\nClick on “Commit” in the Git pane\nIn the new window that opens, add a “Commit message”, then click on the “Commit” button\nClick on “Push” to push your changes from your local repository to the shared remote repository on GitHub\n\n\n\nStep 4: Update the local repository (Required for ALL group members)\n\nFirst, make sure that everyone in your group have completed Step 3\nIn RStudio, navigate to the Git pane and click on “Pull”. A new window will pop up. Once everything is finished running, close the window.\nAt this point, you should find that your Files pane in RStudio is listing the folders that your group members have created, in addition to your own folder\n\nThis task is know as git pull, which updates the local repository to match that content of a shared remote repository\n\n\n\n\nStep 5: Start your EDA project\nIf you encountered no errors then you can feel free to start brainstorming your EDA project with your group.\nFor this project, we ask you to create/update/save files within your own sandbox folder (that you created in Step 3). This will help mitigate the risk of running into trouble when pushing your files to GitHub, especially for those who are new to Git and GitHub. This also allows us to easily review your code.\n\n\n\n\n\n\nImportant notes\n\n\n\nThe GitHub procedure for any project collaboration is\n\nPull new changes\nMake changes on your computer (e.g. create new files, update existing files)\nCommit your local changes (Note: this step may be repeated)\nPull again to avoid merge conflicts\nPush your commit(s) to GitHub\n\nAdvices: Make small, frequent commits. ALWAYS pull before you push.\n\n\nAsk us for help if you run into any issues or have any questions."
  },
  {
    "objectID": "sports/eda/nhl.html#overview",
    "href": "sports/eda/nhl.html#overview",
    "title": "EDA project: NHL shooting",
    "section": "Overview",
    "text": "Overview\nThis project will be released on Thursday, June 6 and conclude with an 8-minute presentation on Tuesday, June 18 during lecture time.\nStudents will be randomly placed into groups of three and each group will be randomly assigned a dataset.\nThe goal of this project is to practice understanding the structure of a dataset, and to practice generating and evaluating hypotheses using fundamental EDA and data visualization techniques."
  },
  {
    "objectID": "sports/eda/nhl.html#deliverables",
    "href": "sports/eda/nhl.html#deliverables",
    "title": "EDA project: NHL shooting",
    "section": "Deliverables",
    "text": "Deliverables\nEach group is expected to make slides to accompany the 8-minute presentation.\nThe presentation should feature the following:\n\nOverview of the structure of your dataset\nThree questions/hypotheses you are interested in exploring\nThree data visualizations exploring the questions, at least two of which must be multivariate. Each visualization must be in a different format from the other two, and you must have at least one categorical and one continuous visualization\nOne clustering analysis\nConclusions for the hypotheses based on your EDA and data visualizations"
  },
  {
    "objectID": "sports/eda/nhl.html#timeline",
    "href": "sports/eda/nhl.html#timeline",
    "title": "EDA project: NHL shooting",
    "section": "Timeline",
    "text": "Timeline\nThere will be two submission deadlines:\nThursday, June 13 at 5pm ET - Each student will push their individual code for the project thus far to GitHub for review. We will then provide feedback.\nMonday, June 17 at 5pm ET - Slides and full code must be completed and ready for presentation. Send your slides to Quang (quang@stat.cmu.edu). All code must be written in R; but the slides may be created in any software. Take advantage of examples from lectures, but also feel free to explore online resources that may be relevant. (But be sure to always consult the R help documentation first before attempting to google around or ask ChatGPT.)"
  },
  {
    "objectID": "sports/eda/nhl.html#data",
    "href": "sports/eda/nhl.html#data",
    "title": "EDA project: NHL shooting",
    "section": "Data",
    "text": "Data\nThis dataset contains all shot attempts from the 2023 NHL playoffs, courtesy of MoneyPuck.com.\nEach row in the dataset corresponds to a shot attempt and the columns are:\n\nshooterPlayerId: player id of the skater taking the shot\nshooterName: first and Last name of the player taking the shot\nteam: team taking the shot\nshooterLeftRight: whether the shooter is a left or right shot\nshooterTimeOnIce: playing time in seconds that have passed since the shooter started their shift\nshooterTimeOnIceSinceFaceoff: minimum of the playing time in seconds since the last faceoff and the playing time that has passed since the shooter started their shift\nevent: whether the shot was a shot on goal (SHOT), goal, (GOAL), or missed the net (MISS)\nlocation: the zone the shot took place in\nshotType: type of shot\nshotAngle: angle of the shot in degrees, positive if the shot is from the left side of the ice.\nshotAnglePlusRebound: difference in angle between the previous shot and this shot if this shot is a rebound, is otherwise set to 0\nshotDistance: distance from the net of the shot in feet, net is defined as being at the (89,0) coordinates\nshotOnEmptyNet: whether the shot was on an empty net\nshotRebound: whether the shot is a rebound, i.e., if the last event was a shot and within 3 seconds of this shot\nshotRush: whether the shot was on a rush, i.e., ff the last event was in another zone and within 4 seconds\nshotWasOnGoal: whether the shot was on net - either a goal or a goalie save\nshotGeneratedRebound: whether the shot generated a rebound shot within 3 seconds of the this shot\nshotGoalieFroze: whether the goalie froze the puck within 1 second of the shot\narenaAdjustedShotDistance: shot distance adjusted for arena recording bias - uses the same methodology as War On Ice proposed by Schuckers and Curro\narenaAdjustedXCord: x coordinate of the arena adjusted shot location, always a positive number\narenaAdjustedYCord: y coordinate of the arena adjusted shot location\ngoalieIdForShot: player id for the goalie the shot is on\ngoalieNameForShot: first and Last name of the goalie the shot is on\nteamCode: team code of the shooting team\nisHomeTeam: whether the shooting team is the home team\nhomeSkatersOnIce: number of skaters on ice for the home team (does not count the goalie)\nawaySkatersOnIce: number of skaters on ice for the away team (does not count the goalie)\ngame_id: game id of the game the shot took place in\nhomeTeamCode: home team in the game\nawayTeamCode: away team in the game\nhomeTeamGoals: home team goals before the shot took place\nawayTeamGoals: away team goals before the shot took place\ntime: seconds into the game of the shot\nperiod: period of the game\n\nNote that a full glossary of the features available for NHL shot data can be found here."
  },
  {
    "objectID": "sports/eda/nhl.html#starter-code",
    "href": "sports/eda/nhl.html#starter-code",
    "title": "EDA project: NHL shooting",
    "section": "Starter code",
    "text": "Starter code\n\nlibrary(tidyverse)\nnhl_shots &lt;- read_csv(\"https://raw.githubusercontent.com/36-SURE/36-SURE.github.io/main/data/nhl_shots.csv\")\n\nIn case you’re curious, the code to build this dataset can be found below. (Note that the data were originally downloaded from the MoneyPuck site.)\n\n# download and unzip\n# https://peter-tanner.com/moneypuck/downloads/shots_2022.zip\nnhl_shots &lt;- read_csv(\"shots_2022.csv\") # might need to modify file path\n\nnhl_shots &lt;- nhl_shots |&gt; \n  filter(isPlayoffGame == 1) |&gt; \n  select(# shooter info\n         shooterPlayerId, shooterName, team, shooterLeftRight, \n         shooterTimeOnIce, shooterTimeOnIceSinceFaceoff,\n         # shot info\n         event, location, shotType, shotAngle, shotAnglePlusRebound, \n         shotDistance, shotOnEmptyNet, shotRebound, shotRush, \n         shotWasOnGoal, shotGeneratedRebound, shotGoalieFroze,\n         # arena-adjusted locations\n         arenaAdjustedShotDistance, arenaAdjustedXCord, arenaAdjustedYCord,\n         # goalie info\n         goalieIdForShot, goalieNameForShot,\n         # team context\n         teamCode, isHomeTeam, homeSkatersOnIce, awaySkatersOnIce,\n         # game context\n         game_id, homeTeamCode, awayTeamCode, homeTeamGoals, awayTeamGoals,\n         time, period)"
  },
  {
    "objectID": "sports/eda/other/ncaa_softball.html#overview",
    "href": "sports/eda/other/ncaa_softball.html#overview",
    "title": "EDA project: PHF shots",
    "section": "Overview",
    "text": "Overview\nThis project will be released on Thursday, June 6 and conclude with a 10-minute presentation (+ 2-3 minutes for Q&A) on Tuesday, June 18 (either during the morning session from x to x or in the afternoon from x to x).\nThe goal of this project is to practice understanding the structure of a dataset, and to practice generating and evaluating hypotheses using fundamental EDA and data visualization techniques."
  },
  {
    "objectID": "sports/eda/other/ncaa_softball.html#deliverables",
    "href": "sports/eda/other/ncaa_softball.html#deliverables",
    "title": "EDA project: PHF shots",
    "section": "Deliverables",
    "text": "Deliverables\nYour team is expected to make slides to accompany your 12-minute presentation.\n\nThe presentation should feature the following:\n\nOverview of the structure of your dataset\nThree questions/hypotheses you are interested in exploring\nThree data visualizations exploring the questions, at least two of which must be multivariate. Each visualization must be in a different format from the other two, and you must have at least one categorical and one continuous visualization\nOne clustering analysis\nConclusions reached for the hypotheses based on your EDA and data visualizations"
  },
  {
    "objectID": "sports/eda/other/ncaa_softball.html#timeline",
    "href": "sports/eda/other/ncaa_softball.html#timeline",
    "title": "EDA project: PHF shots",
    "section": "Timeline",
    "text": "Timeline\nThere will be two submission deadlines:\nThursday, June 13 at 5pm ET - Each student will push their individual code for the project thus far to their GitHub accounts for review. We will then provide feedback on the code submitted.\nMonday, June 17 at 11:59pm ET - Slides and full code must be completed and ready for presentation. Send your slides to Quang (quang@stat.cmu.edu). All code must be written in R. Take advantage of examples from lectures, but also feel free to explore material online that may be relevant!"
  },
  {
    "objectID": "sports/eda/other/ncaa_softball.html#data",
    "href": "sports/eda/other/ncaa_softball.html#data",
    "title": "EDA project: PHF shots",
    "section": "Data",
    "text": "Data\n\ngame_id: game ID\nteam: pitching team\nopponent: batting team\nplayer: pitcher\nip: innings pitched (number of innings a pitcher remains in a game)\nha: hits allowed\ner: earned runs\nbb: batted balls\nhb: hit by pitch\nso: strikeout\nbf: batters faced\n\nhr_a: home runs allowed\ngo: ground outs\nfo: fly outs\n\nseason: year of the season"
  },
  {
    "objectID": "sports/eda/other/ncaa_softball.html#starter-code",
    "href": "sports/eda/other/ncaa_softball.html#starter-code",
    "title": "EDA project: PHF shots",
    "section": "Starter code",
    "text": "Starter code\n\n# install.packages(\"devtools\")\ndevtools::install_github(\"tmking2002/softballR\")\nncaa_softball_pitching_2023 &lt;- softballR::load_ncaa_softball_playerbox(season = 2023, category = \"Pitching\")\n# scoreboard &lt;- softballR::load_ncaa_softball_scoreboard(season = 2023)\n# write_csv(ncaa_softball_pitching_2023, \"data/ncaa_softball_pitching_2023.csv\")"
  },
  {
    "objectID": "sports/eda/other/phf.html#overview",
    "href": "sports/eda/other/phf.html#overview",
    "title": "EDA project: PHF shots",
    "section": "Overview",
    "text": "Overview\nThis project will be released on Thursday, June 6 and conclude with a 10-minute presentation (+ 2-3 minutes for Q&A) on Tuesday, June 18 (either during the morning session from x to x or in the afternoon from x to x).\nThe goal of this project is to practice understanding the structure of a dataset, and to practice generating and evaluating hypotheses using fundamental EDA and data visualization techniques."
  },
  {
    "objectID": "sports/eda/other/phf.html#deliverables",
    "href": "sports/eda/other/phf.html#deliverables",
    "title": "EDA project: PHF shots",
    "section": "Deliverables",
    "text": "Deliverables\nYour team is expected to make slides to accompany your 12-minute presentation.\n\nThe presentation should feature the following:\n\nOverview of the structure of your dataset\nThree questions/hypotheses you are interested in exploring\nThree data visualizations exploring the questions, at least two of which must be multivariate. Each visualization must be in a different format from the other two, and you must have at least one categorical and one continuous visualization\nOne clustering analysis\nConclusions reached for the hypotheses based on your EDA and data visualizations"
  },
  {
    "objectID": "sports/eda/other/phf.html#timeline",
    "href": "sports/eda/other/phf.html#timeline",
    "title": "EDA project: PHF shots",
    "section": "Timeline",
    "text": "Timeline\nThere will be two submission deadlines:\nThursday, June 13 at 5pm ET - Each student will push their individual code for the project thus far to their GitHub accounts for review. We will then provide feedback on the code submitted.\nMonday, June 17 at 11:59pm ET - Slides and full code must be completed and ready for presentation. Send your slides to Quang (quang@stat.cmu.edu). All code must be written in R. Take advantage of examples from lectures, but also feel free to explore material online that may be relevant!"
  },
  {
    "objectID": "sports/eda/other/phf.html#data",
    "href": "sports/eda/other/phf.html#data",
    "title": "EDA project: PHF shots",
    "section": "Data",
    "text": "Data\nThis dataset contains shot information during the 2021-2022 PHF season. The Premier Hockey Federation (PHF) (2015-2023) was a professional women’s ice hockey league in North America. Recently, more data has become available for people to explore and join the women’s hockey analytics community. The data were collected using the fastRhockey package in R and include the following columns:\n\nplay_description: string detailed description of event\nplay_type: string denoting the outcome of the shot, either Goal, PP Goal (meaning power play goal), SH Goal, (meaning shorthanded goal), Shot (shot saved by goalie), or Shot BLK (meaning blocked by a non-goalie)\nperiod_id: integer value of the game period\ntime_remaining String display of time remaining in period in MM:SS format where MM and SS denotes minutes and seconds remaning respectively\nsec_from_start Numeric value of the seconds since the start of the game\nhome_team: string name of the home team\naway_team: string name of the away team\nhome_goals: integer value of the home team score after the event\naway_goals: integer value of the away team score after the event\nshooting_team: string defining the team taking the shot\nplayer_name_1: string name of the player taking the shot\nplayer_name_2: string name of the secondary event player (blocker or goalie)\ngoalie_involved: string name of the goalie involved in the shot attempt\nshot_result: string denoting the outcome of the shot, either blocked (meaning blocked by a non-goalie), made (goal), or saved (shot on net that was saved by a goalie)\non_ice_situation: string indicator for game strength: Even Strength or Power Play\nhome_score_total: integer value of the home team score at the end of the game\naway_score_total: integer value of the away team score at the end of the game"
  },
  {
    "objectID": "sports/eda/other/phf.html#starter-code",
    "href": "sports/eda/other/phf.html#starter-code",
    "title": "EDA project: PHF shots",
    "section": "Starter code",
    "text": "Starter code\n\nlibrary(tidyverse)\nlibrary(fastRhockey)\nphf_pbp &lt;- load_phf_pbp(2021)\nphf_shots &lt;- phf_pbp |&gt; \n  # do not include miss, these were only recorded for a few game\n  filter(play_type %in% c(\"Goal\", \"PP Goal\", \"SH Goal\", \"Shot\", \"Shot BLK\")) %&gt;%\n  select(play_description, play_type, period_id, time_remaining, \n         sec_from_start, home_team, away_team, home_goals, away_goals, \n         shooting_team = team, player_name_1, player_name_2, goalie_involved,\n         shot_result, on_ice_situation, home_score_total, away_score_total)"
  },
  {
    "objectID": "sports/eda/wta.html#overview",
    "href": "sports/eda/wta.html#overview",
    "title": "EDA project: WTA Grand Slam statistics",
    "section": "Overview",
    "text": "Overview\nThis project will be released on Thursday, June 6 and conclude with an 8-minute presentation on Tuesday, June 18 during lecture time.\nStudents will be randomly placed into groups of three and each group will be randomly assigned a dataset.\nThe goal of this project is to practice understanding the structure of a dataset, and to practice generating and evaluating hypotheses using fundamental EDA and data visualization techniques."
  },
  {
    "objectID": "sports/eda/wta.html#deliverables",
    "href": "sports/eda/wta.html#deliverables",
    "title": "EDA project: WTA Grand Slam statistics",
    "section": "Deliverables",
    "text": "Deliverables\nEach group is expected to make slides to accompany the 8-minute presentation.\nThe presentation should feature the following:\n\nOverview of the structure of your dataset\nThree questions/hypotheses you are interested in exploring\nThree data visualizations exploring the questions, at least two of which must be multivariate. Each visualization must be in a different format from the other two, and you must have at least one categorical and one continuous visualization\nOne clustering analysis\nConclusions for the hypotheses based on your EDA and data visualizations"
  },
  {
    "objectID": "sports/eda/wta.html#timeline",
    "href": "sports/eda/wta.html#timeline",
    "title": "EDA project: WTA Grand Slam statistics",
    "section": "Timeline",
    "text": "Timeline\nThere will be two submission deadlines:\nThursday, June 13 at 5pm ET - Each student will push their individual code for the project thus far to GitHub for review. We will then provide feedback.\nMonday, June 17 at 5pm ET - Slides and full code must be completed and ready for presentation. Send your slides to Quang (quang@stat.cmu.edu). All code must be written in R; but the slides may be created in any software. Take advantage of examples from lectures, but also feel free to explore online resources that may be relevant. (But be sure to always consult the R help documentation first before attempting to google around or ask ChatGPT.)"
  },
  {
    "objectID": "sports/eda/wta.html#data",
    "href": "sports/eda/wta.html#data",
    "title": "EDA project: WTA Grand Slam statistics",
    "section": "Data",
    "text": "Data\nThis dataset contains all WTA matches between 2018 and 2023, courtesy of the famous tennis data repository maintained by Jeff Sackmann.\nEach row in the data corresponds to a single WTA Grand Slam match. The columns contain general information about the matches:\n\ntourney_name: name of the Grand Slam tournament\nsurface: type of court surface\ntourney_date: eight digits, YYYYMMDD, usually the Monday of the tournament week\n{winner, loser}_seed: seed of winning/losing player\n{winner, loser}_name: Name of the winning/losing player\n{winner, loser}_hand: R: right, L: left, U: unknown (for ambidextrous players, this is their serving hand)\n{winner, loser}_ht: height in centimeters, where available\n{winner, loser}_ioc: three-character country code\n{winner, loser}_age: age, in years, as of the tourney_date\nscore: final match score\nround: tournament round\nminutes: match length in minutes\n{w, l}_ace: winner/loser’s number of aces\n{w, l}_df: winner/loser’s number of doubles faults\n{w, l}_svpt: winner/loser’s number of serve points\n{w, l}_1stIn: winner/loser’s number of first serves made\n{w, l}_1stWon: winner/loser’s number of first-serve points won\n{w, l}_2ndWon: winner/loser’s number of second-serve points won\n{w, l}_SvGms: winner/loser’s number of serve games\n{w, l}_bpSaved: winner/loser’s number of break points saved\n{w, l}_bpFaced: winner/loser’s number of break points faced\n{winner, loser}_rank: winner/loser’s WTA rank, as of the tourney_date, or the most recent ranking date before the tourney_date\n\nNote that a full glossary of the features available for match data can be found here."
  },
  {
    "objectID": "sports/eda/wta.html#starter-code",
    "href": "sports/eda/wta.html#starter-code",
    "title": "EDA project: WTA Grand Slam statistics",
    "section": "Starter code",
    "text": "Starter code\n\nlibrary(tidyverse)\nwta_grand_slam_matches &lt;- read_csv(\"https://raw.githubusercontent.com/36-SURE/36-SURE.github.io/main/data/wta_grand_slam_matches.csv\")\n\nIn case you’re curious, the code to build this dataset can be found below.\n\nlibrary(tidyverse)\n\nget_grand_slam_matches &lt;- function(year) {\n  year_url &lt;- str_c(\n    \"https://raw.githubusercontent.com/JeffSackmann/tennis_wta/master/wta_matches_\",\n     year, \".csv\"\n  )\n  year_matches &lt;- year_url |&gt; \n    read_csv() |&gt; \n    filter(tourney_level == \"G\") |&gt; \n    mutate(winner_seed = as.character(winner_seed),\n           loser_seed = as.character(loser_seed),\n           tourney_name = str_to_upper(tourney_name)) |&gt; \n    select(-tourney_id, -tourney_level, -best_of, -draw_size, -match_num, -winner_id,\n           -winner_entry, -loser_id, -loser_entry, -winner_rank_points, -loser_rank_points)\n  \n  return(year_matches)\n}\n\nyears &lt;- 2018:2023\nwta_grand_slam_matches &lt;- years |&gt; \n  map(get_grand_slam_matches) |&gt; \n  bind_rows()"
  },
  {
    "objectID": "sports/eda/wnba.html#overview",
    "href": "sports/eda/wnba.html#overview",
    "title": "EDA project: WNBA shooting",
    "section": "Overview",
    "text": "Overview\nThis project will be released on Thursday, June 6 and conclude with an 8-minute presentation on Tuesday, June 18 during lecture time.\nStudents will be randomly placed into groups of three and each group will be randomly assigned a dataset.\nThe goal of this project is to practice understanding the structure of a dataset, and to practice generating and evaluating hypotheses using fundamental EDA and data visualization techniques."
  },
  {
    "objectID": "sports/eda/wnba.html#deliverables",
    "href": "sports/eda/wnba.html#deliverables",
    "title": "EDA project: WNBA shooting",
    "section": "Deliverables",
    "text": "Deliverables\nEach group is expected to make slides to accompany the 8-minute presentation.\nThe presentation should feature the following:\n\nOverview of the structure of your dataset\nThree questions/hypotheses you are interested in exploring\nThree data visualizations exploring the questions, at least two of which must be multivariate. Each visualization must be in a different format from the other two, and you must have at least one categorical and one continuous visualization\nOne clustering analysis\nConclusions for the hypotheses based on your EDA and data visualizations"
  },
  {
    "objectID": "sports/eda/wnba.html#timeline",
    "href": "sports/eda/wnba.html#timeline",
    "title": "EDA project: WNBA shooting",
    "section": "Timeline",
    "text": "Timeline\nThere will be two submission deadlines:\nThursday, June 13 at 5pm ET - Each student will push their individual code for the project thus far to GitHub for review. We will then provide feedback.\nMonday, June 17 at 5pm ET - Slides and full code must be completed and ready for presentation. Send your slides to Quang (quang@stat.cmu.edu). All code must be written in R; but the slides may be created in any software. Take advantage of examples from lectures, but also feel free to explore online resources that may be relevant. (But be sure to always consult the R help documentation first before attempting to google around or ask ChatGPT.)"
  },
  {
    "objectID": "sports/eda/wnba.html#data",
    "href": "sports/eda/wnba.html#data",
    "title": "EDA project: WNBA shooting",
    "section": "Data",
    "text": "Data\nThis dataset contains all shot attempts in the 2023 WNBA season accessed via the wehoop package.\nEach row in the dataset corresponds to a shot attempt and the columns are:\n\ngame_id: unique integer ID for each WNBA game\ngame_play_number: integer indicating the recorded play number for the shot attempt, where 1 indicates the first play of the game\ngame_type: type of the game (2: regular season, 3: postseason)\ndesc: string detailed description of shot attempt\nshot_type: string description of the shot type (e.g., dunk, layup, jump shot, etc.)\nmade_shot: boolean denoting if the shot was made (TRUE) or not (FALSE)\nshot_value: numeric value of the shot outcome (0 for shots that were not made, and a positive value for made shots)\ncoordinate_x: horizontal location in feet of shot attempt where the hoop would be located at 25 feet\ncoordinate_y: vertical location in feet of shot attempt with respect to the target hoop (the hoop should be a little in front of 0 but the coordinate system is not exact)\nshooting_team: string name of the team taking the shot\nhome_team_name: string name of the home team\naway_team_name: string name of the away team\nhome_score: integer value of the home team score after the shot\naway_score: integer value of the away team score after the shot\nqtr: integer denoting the quarter/period in the game\nquarter_seconds_remaining: numeric integer value for number of seconds remaining in quarter/period\ngame_seconds_remaining: numeric integer value for number of seconds remaining in game\n\nNote that a full glossary of the features available in the WNBA shot data can be found here."
  },
  {
    "objectID": "sports/eda/wnba.html#starter-code",
    "href": "sports/eda/wnba.html#starter-code",
    "title": "EDA project: WNBA shooting",
    "section": "Starter code",
    "text": "Starter code\n\nlibrary(tidyverse)\nwnba_shots &lt;- read_csv(\"https://raw.githubusercontent.com/36-SURE/36-SURE.github.io/main/data/wnba_shots.csv\")\n\nIn case you’re curious, the code to build this dataset can be found below.\n\nlibrary(tidyverse)\nlibrary(wehoop) # install.packages(\"wehoop\")\nwnba_pbp &lt;- load_wnba_pbp(2023)\nwnba_shots &lt;- wnba_pbp |&gt; \n  filter(shooting_play) |&gt; \n  # make a column to indicate the shooting team\n  mutate(shooting_team = ifelse(team_id == home_team_id, \n                                home_team_name,\n                                away_team_name)) |&gt; \n  select(game_id, game_play_number, game_type = season_type,\n         desc = text, shot_type = type_text, \n         made_shot = scoring_play, shot_value = score_value, \n         coordinate_x, coordinate_y, shooting_team, \n         home_team_name, away_team_name, home_score, away_score, qtr,\n         quarter_seconds_remaining = start_quarter_seconds_remaining,\n         game_seconds_remaining = start_game_seconds_remaining)"
  },
  {
    "objectID": "sports/eda/nwsl.html#overview",
    "href": "sports/eda/nwsl.html#overview",
    "title": "EDA project: NWSL team statistics",
    "section": "Overview",
    "text": "Overview\nThis project will be released on Thursday, June 6 and conclude with an 8-minute presentation on Tuesday, June 18 during lecture time.\nStudents will be randomly placed into groups of three and each group will be randomly assigned a dataset.\nThe goal of this project is to practice understanding the structure of a dataset, and to practice generating and evaluating hypotheses using fundamental EDA and data visualization techniques."
  },
  {
    "objectID": "sports/eda/nwsl.html#deliverables",
    "href": "sports/eda/nwsl.html#deliverables",
    "title": "EDA project: NWSL team statistics",
    "section": "Deliverables",
    "text": "Deliverables\nEach group is expected to make slides to accompany the 8-minute presentation.\nThe presentation should feature the following:\n\nOverview of the structure of your dataset\nThree questions/hypotheses you are interested in exploring\nThree data visualizations exploring the questions, at least two of which must be multivariate. Each visualization must be in a different format from the other two, and you must have at least one categorical and one continuous visualization\nOne clustering analysis\nConclusions for the hypotheses based on your EDA and data visualizations"
  },
  {
    "objectID": "sports/eda/nwsl.html#timeline",
    "href": "sports/eda/nwsl.html#timeline",
    "title": "EDA project: NWSL team statistics",
    "section": "Timeline",
    "text": "Timeline\nThere will be two submission deadlines:\nThursday, June 13 at 5pm ET - Each student will push their individual code for the project thus far to GitHub for review. We will then provide feedback.\nMonday, June 17 at 5pm ET - Slides and full code must be completed and ready for presentation. Send your slides to Quang (quang@stat.cmu.edu). All code must be written in R; but the slides may be created in any software. Take advantage of examples from lectures, but also feel free to explore online resources that may be relevant. (But be sure to always consult the R help documentation first before attempting to google around or ask ChatGPT.)"
  },
  {
    "objectID": "sports/eda/nwsl.html#data",
    "href": "sports/eda/nwsl.html#data",
    "title": "EDA project: NWSL team statistics",
    "section": "Data",
    "text": "Data\nThis dataset contains regular season statistics for each NWSL team from 2016 to 2022 (excluding 2020 which was cancelled due to COVID). The National Women’s Soccer League (NWSL) is the top professional women’s soccer league in the United States. The data was collected using the nwslR package in R.\nEach row in the dataset corresponds to a team-season and the columns are:\n\nteam_name: name of NWSL team\nseason: regular season year of team’s statistics\ngames_played: number of games team played in season\ngoal_differential: goals scored - goals conceded\ngoals: number of goals scores\ngoals_conceded: number of goals conceded\ncross_accuracy: percent of crosses that were successful\ngoal_conversion_pct: percent of shots scored\npass_pct: pass accuracy\npass_pct_opposition_half: pass accuracy in opposition half\npossession_pct: percentage of overall ball possession the team had during the season\nshot_accuracy: percentage of shots on target\ntackle_success_pct: percent of successful tackles"
  },
  {
    "objectID": "sports/eda/nwsl.html#starter-code",
    "href": "sports/eda/nwsl.html#starter-code",
    "title": "EDA project: NWSL team statistics",
    "section": "Starter code",
    "text": "Starter code\n\nlibrary(tidyverse)\nnwsl_team_stats &lt;- read_csv(\"https://raw.githubusercontent.com/36-SURE/36-SURE.github.io/main/data/nwsl_team_stats.csv\")\n\nIn case you’re curious, the code to build the data can be found here."
  },
  {
    "objectID": "sports/eda/nfl.html#overview",
    "href": "sports/eda/nfl.html#overview",
    "title": "EDA project: NFL passing",
    "section": "Overview",
    "text": "Overview\nThis project will be released on Thursday, June 6 and conclude with an 8-minute presentation on Tuesday, June 18 during lecture time.\nStudents will be randomly placed into groups of three and each group will be randomly assigned a dataset.\nThe goal of this project is to practice understanding the structure of a dataset, and to practice generating and evaluating hypotheses using fundamental EDA and data visualization techniques."
  },
  {
    "objectID": "sports/eda/nfl.html#deliverables",
    "href": "sports/eda/nfl.html#deliverables",
    "title": "EDA project: NFL passing",
    "section": "Deliverables",
    "text": "Deliverables\nEach group is expected to make slides to accompany the 8-minute presentation.\nThe presentation should feature the following:\n\nOverview of the structure of your dataset\nThree questions/hypotheses you are interested in exploring\nThree data visualizations exploring the questions, at least two of which must be multivariate. Each visualization must be in a different format from the other two, and you must have at least one categorical and one continuous visualization\nOne clustering analysis\nConclusions for the hypotheses based on your EDA and data visualizations"
  },
  {
    "objectID": "sports/eda/nfl.html#timeline",
    "href": "sports/eda/nfl.html#timeline",
    "title": "EDA project: NFL passing",
    "section": "Timeline",
    "text": "Timeline\nThere will be two submission deadlines:\nThursday, June 13 at 5pm ET - Each student will push their individual code for the project thus far to GitHub for review. We will then provide feedback.\nMonday, June 17 at 5pm ET - Slides and full code must be completed and ready for presentation. Send your slides to Quang (quang@stat.cmu.edu). All code must be written in R; but the slides may be created in any software. Take advantage of examples from lectures, but also feel free to explore online resources that may be relevant. (But be sure to always consult the R help documentation first before attempting to google around or ask ChatGPT.)"
  },
  {
    "objectID": "sports/eda/nfl.html#data",
    "href": "sports/eda/nfl.html#data",
    "title": "EDA project: NFL passing",
    "section": "Data",
    "text": "Data\nThis dataset contains all passing plays from the 2023 NFL regular season accessed via the nflfastR package (part of the nflverse).\nEach row in the dataset corresponds to a single passing play (including sacks) and the columns are:\n\npasser_player_name: name for the player that attempted the pass\npasser_player_id: unique identifier for the player that attempted the pass\nposteam: abbreviation for the team with possession\ncomplete_pass: indicator denoting whether or not the pass was completed\ninterception: indicator denoting whether or not the pass was intercepted by the defense\nyards_gained: yards gained (or lost) by the possessing team, excluding yards gained via fumble recoveries and laterals\ntouchdown: indicator denoting if the play resulted in a touchdown\npass_location: categorical location of pass\npass_length: categorical length of pass\nair_yards: distance in yards perpendicular to the line of scrimmage at where the targeted receiver either caught or didn’t catch the ball\nyards_after_catch: distance in yards perpendicular to the yard line where the receiver made the reception to where the play ended\nepa: expected points added (EPA) by the posteam for the given play\nwpa: win probability added (WPA) for the posteam\nshotgun: indicator for whether or not the play was in shotgun formation\nno_huddle: indicator for whether or not the play was in no_huddle formation\nqb_dropback: indicator for whether or not the QB dropped back on the play (pass attempt, sack, or scrambled)\nqb_hit: indicator if the QB was hit on the play\nsack: indicator for if the play ended in a sack\nreceiver_player_name: name for the targeted receiver\nreceiver_player_id: unique identifier for the receiver that was targeted on the pass\ndefteam: abbreviation for the team on defense\nposteam_type: indicating whether the posteam team is home or away\nplay_id: unique identifier for a single play\nyardline_100: distance in the number of yards from the opponent’s endzone for the posteam\nside_of_field: abbreviation for which team’s side of the field the team with possession is currently on\ndown: down for the given play\nqtr: quarter of the game (5 is overtime)\nplay_clock: time on the playclock when the ball was snapped\nhalf_seconds_remaining: seconds remaining in the half\ngame_half: indicating which half the play is in\ngame_id: ten digit identifier for NFL game\nhome_team: abbreviation for the home team\naway_team: abbreviation for the away team\nhome_score: total points scored by the home team\naway_score: total points scored by the away team\ndesc: detailed description for the given play\n\nNote that a full glossary of the features available for NFL play-by-play data can be found here."
  },
  {
    "objectID": "sports/eda/nfl.html#starter-code",
    "href": "sports/eda/nfl.html#starter-code",
    "title": "EDA project: NFL passing",
    "section": "Starter code",
    "text": "Starter code\n\nlibrary(tidyverse)\nnfl_passing &lt;- read_csv(\"https://raw.githubusercontent.com/36-SURE/36-SURE.github.io/main/data/nfl_passing.csv\")\n\nIn case you’re curious, the code to build this dataset can be found below.\n\nlibrary(tidyverse)\nlibrary(nflfastR) # install.packages(\"nflverse\")\nnfl_pbp &lt;- load_pbp(2023)\nnfl_passing &lt;- nfl_pbp |&gt; \n  filter(play_type == \"pass\", season_type == \"REG\", \n         !is.na(epa), !is.na(posteam), posteam != \"\") |&gt; \n  select(# player info attempting the pass\n         passer_player_name, passer_player_id, posteam,\n         # info about the pass:\n         complete_pass, interception, yards_gained, touchdown,\n         pass_location, pass_length, air_yards, yards_after_catch, epa, wpa,\n         shotgun, no_huddle, qb_dropback, qb_hit, sack,\n         # context about the receiver:\n         receiver_player_name, receiver_player_id,\n         # team context\n         posteam, defteam, posteam_type,\n         # play and game context\n         play_id, yardline_100, side_of_field, down, qtr, play_clock,\n         half_seconds_remaining, game_half, game_id,\n         home_team, away_team, home_score, away_score,\n         # description of play\n         desc)"
  },
  {
    "objectID": "sports/project/report-template.html#introduction",
    "href": "sports/project/report-template.html#introduction",
    "title": "Title",
    "section": "Introduction",
    "text": "Introduction\nDescribe the problem and why it is important."
  },
  {
    "objectID": "sports/project/report-template.html#data",
    "href": "sports/project/report-template.html#data",
    "title": "Title",
    "section": "Data",
    "text": "Data\nDescribe the data you’re using in detail, where you accessed it, along with relevant exploratory data analysis (EDA). You should also include descriptions of any major data pre-processing/cleaning steps."
  },
  {
    "objectID": "sports/project/report-template.html#methods",
    "href": "sports/project/report-template.html#methods",
    "title": "Title",
    "section": "Methods",
    "text": "Methods\nDescribe the modeling techniques you chose, their assumptions, justifications for why they are appropriate for the problem, and your plan for comparison/evaluation approaches."
  },
  {
    "objectID": "sports/project/report-template.html#results",
    "href": "sports/project/report-template.html#results",
    "title": "Title",
    "section": "Results",
    "text": "Results\nDescribe your results. This can include tables and plots showing your results, as well as text describing how your models worked and the appropriate interpretations of the relevant output. (Note: Don’t just write out the textbook interpretations of all model coefficients! Instead, interpret the output that is relevant for your question of interest that is framed in the introduction)"
  },
  {
    "objectID": "sports/project/report-template.html#discussion",
    "href": "sports/project/report-template.html#discussion",
    "title": "Title",
    "section": "Discussion",
    "text": "Discussion\nGive your conclusions and summarize what you have learned with regards to your question of interest. Are there any limitations with the approaches you used? What do you think are the next steps to follow-up your project?"
  },
  {
    "objectID": "sports/project/report-template.html#appendix-a-quick-tutorial",
    "href": "sports/project/report-template.html#appendix-a-quick-tutorial",
    "title": "Title",
    "section": "Appendix: A quick tutorial",
    "text": "Appendix: A quick tutorial\n(Feel free to remove this section when you submit)\nThis a Quarto document. To learn more about Quarto see https://quarto.org. You can use the Render button to see what it looks like in HTML.\n\nText formatting\nText can be bolded with double asterisks and italicized with single asterisks. Monospace text, such as for short code snippets, uses backticks. (Note these are different from quotation marks or apostrophes.) Links are written like this.\nBulleted lists can be written with asterisks:\n\nEach item starts on a new line with an asterisk.\nItems should start on the beginning of the line.\nLeave blank lines after the end of the list so the list does not continue.\n\nMathematics can be written with LaTeX syntax using dollar signs. For instance, using single dollar signs we can write inline math: (-b \\pm \\sqrt{b^2 - 4ac})/2a.\nTo write math in “display style”, i.e. displayed on its own line centered on the page, we use double dollar signs: \nx^2 + y^2 = 1\n\n\n\nCode blocks\nCode blocks are evaluated sequentially when you hit Render. As the code runs, R prints out which block is running, so naming blocks is useful if you want to know which one takes a long time. After the block name, you can specify chunk options. For example, echo controls whether the code is printed in the document. By default, output is printed in the document in monospace:\n\n\n                   mpg cyl disp  hp drat    wt  qsec vs am gear carb\nMazda RX4         21.0   6  160 110 3.90 2.620 16.46  0  1    4    4\nMazda RX4 Wag     21.0   6  160 110 3.90 2.875 17.02  0  1    4    4\nDatsun 710        22.8   4  108  93 3.85 2.320 18.61  1  1    4    1\nHornet 4 Drive    21.4   6  258 110 3.08 3.215 19.44  1  0    3    1\nHornet Sportabout 18.7   8  360 175 3.15 3.440 17.02  0  0    3    2\nValiant           18.1   6  225 105 2.76 3.460 20.22  1  0    3    1\n\n\nChunk options can also be written inside the code block, which is helpful for really long options, as we’ll see soon.\n\n\n                   mpg cyl disp  hp drat    wt  qsec vs am gear carb\nMazda RX4         21.0   6  160 110 3.90 2.620 16.46  0  1    4    4\nMazda RX4 Wag     21.0   6  160 110 3.90 2.875 17.02  0  1    4    4\nDatsun 710        22.8   4  108  93 3.85 2.320 18.61  1  1    4    1\nHornet 4 Drive    21.4   6  258 110 3.08 3.215 19.44  1  0    3    1\nHornet Sportabout 18.7   8  360 175 3.15 3.440 17.02  0  0    3    2\nValiant           18.1   6  225 105 2.76 3.460 20.22  1  0    3    1\n\n\n\n\nFigures\nIf a code block produces a plot or figure, this figure will automatically be inserted inline in the report. That is, it will be inserted exactly where the code block is.\n\n\n\n\n\nThis is a caption. It should explain what’s in the figure and what’s interesting about it. For instance: There is a negative, strong linear correlation between miles per gallon and horsepower for US cars in the 1970s.\n\n\n\n\nNotice the use of fig-width and fig-height to control the figure’s size (in inches). These control the sizes given to R when it generates the plot, so R proportionally adjusts the font sizes to be large enough.\n\n\nTables\nUse the knitr::kable() function to print tables as HTML:\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nmpg\ncyl\ndisp\nhp\ndrat\nwt\nqsec\nvs\nam\ngear\ncarb\n\n\n\n\nMazda RX4\n21.0\n6\n160\n110\n3.90\n2.620\n16.46\n0\n1\n4\n4\n\n\nMazda RX4 Wag\n21.0\n6\n160\n110\n3.90\n2.875\n17.02\n0\n1\n4\n4\n\n\nDatsun 710\n22.8\n4\n108\n93\n3.85\n2.320\n18.61\n1\n1\n4\n1\n\n\nHornet 4 Drive\n21.4\n6\n258\n110\n3.08\n3.215\n19.44\n1\n0\n3\n1\n\n\nHornet Sportabout\n18.7\n8\n360\n175\n3.15\n3.440\n17.02\n0\n0\n3\n2\n\n\n\n\n\nWe can summarize model results with a table. For instance, suppose we fit a linear regression model:\n\nmodel1 &lt;- lm(mpg ~ disp + hp + drat, data = mtcars)\n\nIt is not appropriate to simply print summary(model1) into the report. If we want the reader to understand what models we have fit and what their results are, we should provide a nicely formatted table. A simple option is to use the tidy() function from the broom package to get a data frame of the model fit, and simply report that as a table.\n\n\n\nPredicting fuel economy using vehicle features. \n\n\nTerm\nEstimate\nSE\nt\np\n\n\n\n\n(Intercept)\n19.34\n6.37\n3.04\n0.01\n\n\ndisp\n-0.02\n0.01\n-2.05\n0.05\n\n\nhp\n-0.03\n0.01\n-2.34\n0.03\n\n\ndrat\n2.71\n1.49\n1.83\n0.08"
  },
  {
    "objectID": "sports/project/description.html",
    "href": "sports/project/description.html",
    "title": "Project Guidelines",
    "section": "",
    "text": "Each student has been allocated into a project group of three. Each group has been assigned a specific project research topic. Your goal is to complete the required project deliverables and checkpoints, in accordance with the guidelines detailed in the remainder of this document."
  },
  {
    "objectID": "sports/project/description.html#your-task",
    "href": "sports/project/description.html#your-task",
    "title": "Project Guidelines",
    "section": "",
    "text": "Each student has been allocated into a project group of three. Each group has been assigned a specific project research topic. Your goal is to complete the required project deliverables and checkpoints, in accordance with the guidelines detailed in the remainder of this document."
  },
  {
    "objectID": "sports/project/description.html#deliverables",
    "href": "sports/project/description.html#deliverables",
    "title": "Project Guidelines",
    "section": "Deliverables",
    "text": "Deliverables\nThis project has the following three key deliverables:\n\n1. Report\n[template] (right click and choose “Save Link As…” to download)\nDUE THURSDAY JULY 25, 6PM\nYour report should be written using Quarto and submitted as a rendered .html file. We recommend using an IDMRaD (Introduction, Data, Methods, Results and Discussion) report format, with details provided in the report template.\n\n\n2. Poster\n[template] (Google Slides link)\nDUE TUESDAY JULY 23, 6PM (so that we have time to print the posters)\nYour poster should be submitted as a .pdf file. We will then make a printed copy for the poster session on the final day (July 26).\n\n\n3. Presentation\nDUE THURSDAY JULY 25, 6PM\nEach group will give an 8-minute presentation on the final day (July 26). The presentation should effectively have the same structure as your report with an introduction, followed by data description, an overview of methods, followed by results and discussion. Your slides may be created in any software, but we ony accept submissions in the form of a .pdf file, a Google Slides link, or a Quarto presentation (self-contained .html file or hosted online)."
  },
  {
    "objectID": "sports/project/description.html#checkpoints",
    "href": "sports/project/description.html#checkpoints",
    "title": "Project Guidelines",
    "section": "Checkpoints",
    "text": "Checkpoints\nCheckpoint 1: 5-minute presentation during lab on July 1\nNote: It is perfectly fine if you don’t have any results at this point\nYour first checkpoint presentation should be structured as follows.\n\nIntroduction (1 slide): Describe your project topic/question(s) and why it is important\nData (1 slide): Data description, any preliminary data pre-processing/cleaning steps\nEDA (2 slides max): 1–2 EDA plots related to your question(s) of interest\n\nDesign the slides using the assertion-evidence model\n\nMethods (1 slide): Early thoughts on methods/modeling strategy. Justify why it might be appropriate to answer your question(s) of interest\n\n\n\nPlan of action (1 slide): List all the steps needed to complete your project (be specific). Highlight the completed steps. What are the next steps?\n\nCheckpoint 2: 8-minute presentation during lab on July 16\nYour second checkpoint presentation should be structured as follows.\n\nIntroduction (1 slide): Describe your project topic/question(s) and why it is important\nData: (1 slide) Data description and any major data pre-processing/cleaning steps\nPlan of action (1 slide): List all the steps needed to complete your project (be specific). Highlight the completed steps.\nPresent the completed steps (5 slides max): methods, plots, findings, etc.\n\nDesign the slides using the assertion-evidence model\n\nPlan of action (1 slide, use the same one as before): what are the steps still to be completed?"
  },
  {
    "objectID": "sports/project/description.html#analysis",
    "href": "sports/project/description.html#analysis",
    "title": "Project Guidelines",
    "section": "Analysis",
    "text": "Analysis\nYour analysis should focus on both:\nExploratory data analysis: Create visualizations to explore the underlying structure of the data and gain insights about distributions and relationships between variables. These should be ideally based on reasoned hypotheses.\nStatistical modeling: Demonstrate the use of statistical and machine learning modeling techniques. This may involve justifications for your choice of model (e.g., comparison with model specifications such as using different predictors, or with other methods), and then any relevant interpretation of the model with regards to your project’s topic. Depending on your project, the model(s) you rely on may be used for either an inference (i.e., interpreting coefficients) or prediction task. The model you choose just needs to be motivated by your question of interest."
  },
  {
    "objectID": "git-setup.html#step-1-create-a-github-account",
    "href": "git-setup.html#step-1-create-a-github-account",
    "title": "Git and GitHub Setup",
    "section": "Step 1: Create a GitHub account",
    "text": "Step 1: Create a GitHub account\nRegister for a GitHub account at https://github.com if you do not have one. It is completely free to use. You may use any username you prefer; we will later ask for your username so we can keep track."
  },
  {
    "objectID": "git-setup.html#step-2-install-git",
    "href": "git-setup.html#step-2-install-git",
    "title": "Git and GitHub Setup",
    "section": "Step 2: Install Git",
    "text": "Step 2: Install Git\n(Windows)\n\nGo to https://git-scm.com/download/win\nNavigate to “Click here to download” on the first line and click on it\nFollow the installation instructions\n\n(macOS)\n\nOpen the Terminal app on your computer (Finder \\(\\rightarrow\\) Applications \\(\\rightarrow\\) Terminal)\nGo to https://brew.sh and copy/paste the chunk under “Install Homebrew” to the Terminal\nOnce Homebrew is installed, type this into the Terminal: brew install git"
  },
  {
    "objectID": "git-setup.html#step-3-configure-git",
    "href": "git-setup.html#step-3-configure-git",
    "title": "Git and GitHub Setup",
    "section": "Step 3: Configure Git",
    "text": "Step 3: Configure Git\n\nAfter installation, you need to configure Git. This can be done directly in R:\n\n\n# uncomment and run the following line to install the usethis package\n# install.packages(\"usethis\")\nusethis::use_git_config(user.name = \"Your Name\", user.email = \"your-github@email.address\")\n\n\nUse your full name for the user.name field and the same email as your GitHub account for user.email\nYou then need to create a personal access token for authentication as follows:\n\n\nusethis::create_github_token()\n\n\nThis will direct you to the GitHub site on your browser (you may have to log in). On this site:\n\nUnder “Note”, type in some description for this token (e.g., “SURE 2024 GitHub token”)\nFor “Expiration”, set an expiration date for this token (e.g., 90 days) or make it permanent (i.e. choose “No expiration” if you don’t want to deal with this again in the future)\nUnder “Select scopes”, recommended scopes will be pre-selected. Stick with these for now.\n\nNext, click on “Generate token”\nCopy the token to your clipboard (or leave the browser window open, so you can come back to copy the token later)\nIn RStudio, run the following to get a prompt where you can paste your token:\n\n\n# uncomment and run the following line to install the gitcreds package\n# install.packages(\"gitcreds\")\ngitcreds::gitcreds_set()\n\nYou should then be ready to use GitHub!"
  },
  {
    "objectID": "git-setup.html#step-4-create-a-github-repository",
    "href": "git-setup.html#step-4-create-a-github-repository",
    "title": "Git and GitHub Setup",
    "section": "Step 4: Create a GitHub repository",
    "text": "Step 4: Create a GitHub repository\nWe will follow the paradigm of “GitHub first”. What this means is that when we create a repository, we will create it on GitHub first, then link a local repository to it from inside RStudio.\nAfter you’ve logged in, to create a GitHub repository\n\nGo to https://github.com/new\nName the repository (give it a meaningful name)\nGive the repository a description (don’t leave this blank although this is optional)\nDecide whether to keep the repository public or private (for now, let’s just keep it public)\nClick on “Initialize this repository with a README”. For now, there’s no need to “Add .gitignore” or “Choose a license”\nClick on “Create Repository”"
  },
  {
    "objectID": "git-setup.html#step-5-connect-rstudio-to-the-github-repository",
    "href": "git-setup.html#step-5-connect-rstudio-to-the-github-repository",
    "title": "Git and GitHub Setup",
    "section": "Step 5: Connect RStudio to the GitHub repository",
    "text": "Step 5: Connect RStudio to the GitHub repository\n\nGo to the browser page for your GitHub repository\nClick on Code (in the same line as “Go to file”). Under HTTPS, copy the URL\nIn RStudio, click on File &gt; New Project.... Next, click on “Version Control” and then on “Git”. Paste the URL you just copied into “Repository URL”\nType the name for the folder on your computer associated with this repository into Project directory name\n\nYou can choose whatever name you want, but it is recommended to give a name similar to the repository name on GitHub\n\nMake sure “Create project as subdirectory of:” points to where you want to locate this new folder\nClick on “Create Project”\nAt this point, you should find that the “Files” pane (in the bottom right of RStudio) is listing the files in your local repository."
  },
  {
    "objectID": "git-setup.html#step-6-modify-the-repository",
    "href": "git-setup.html#step-6-modify-the-repository",
    "title": "Git and GitHub Setup",
    "section": "Step 6: Modify the repository",
    "text": "Step 6: Modify the repository\nTo add a new file from your local repository to GitHub:\n\nIn RStudio, open a new file (could be anything - e.g. R Script, Quarto document, etc.).\nFill the file with some code/comments/etc. (This is just for illustration purpose, to show how you can add a file to GitHub from your computer)\nSave the file. At this point, this file should show up in the “Git” pane (in the top right of RStudio)\nCheck the box under “Staged” in the Git pane to stage the file for a commit\nClick on “Commit” in the Git pane\nIn the new window that opens, add a “Commit message”, then click on the “Commit” button\nClick on “Push” to push your changes from your local repository to the remote repository on GitHub\n\nIf you encountered no errors then you’re done! While working on a single project you will repeatedly perform the tasks in Step 6: make changes to files, commit changes, then push changes\nEvery time you want to create a new repository, you can just start with Step 4, use GitHub, copy the repository into RStudio, then repeatedly update, commit, and push.\nAsk us for help if you have any questions."
  },
  {
    "objectID": "lectures.html",
    "href": "lectures.html",
    "title": "Lectures",
    "section": "",
    "text": "Date\nTitle\nMaterials\n\n\n\n\nLecture 0\nJune 3\nWelcome to SURE 2024\nslides\n\n\nLecture 1\nJune 4\nExploring data: into the tidyverse\nslides\n\n\nLecture 2\nJune 5\nData visualization: the grammar of graphics and ggplot2\nslides\n\n\nLecture 3\nJune 6\nData visualization: categorical data\nslides\n\n\nLecture 4\nJune 7\nData visualization: quantitative data\nslides\n\n\nLecture 5\nJune 10\nUnsupervised learning: \\(k\\)-means clustering\nslides\n\n\nLecture 6\nJune 11\nUnsupervised learning: hierarchical clustering\nslides\n\n\nLecture 7\nJune 12\nData visualization: density estimation\nslides\n\n\nLecture 8\nJune 13\nSimulation\nslides\n\n\nLecture 9\nJune 14\nPresentations\nslides\n\n\nLecture 10\nJune 17\nSupervised learning: the tradeoffs\nslides"
  },
  {
    "objectID": "lectures.html#contents",
    "href": "lectures.html#contents",
    "title": "Lectures",
    "section": "",
    "text": "Date\nTitle\nMaterials\n\n\n\n\nLecture 0\nJune 3\nWelcome to SURE 2024\nslides\n\n\nLecture 1\nJune 4\nExploring data: into the tidyverse\nslides\n\n\nLecture 2\nJune 5\nData visualization: the grammar of graphics and ggplot2\nslides\n\n\nLecture 3\nJune 6\nData visualization: categorical data\nslides\n\n\nLecture 4\nJune 7\nData visualization: quantitative data\nslides\n\n\nLecture 5\nJune 10\nUnsupervised learning: \\(k\\)-means clustering\nslides\n\n\nLecture 6\nJune 11\nUnsupervised learning: hierarchical clustering\nslides\n\n\nLecture 7\nJune 12\nData visualization: density estimation\nslides\n\n\nLecture 8\nJune 13\nSimulation\nslides\n\n\nLecture 9\nJune 14\nPresentations\nslides\n\n\nLecture 10\nJune 17\nSupervised learning: the tradeoffs\nslides"
  },
  {
    "objectID": "lectures.html#references",
    "href": "lectures.html#references",
    "title": "Lectures",
    "section": "References",
    "text": "References\n\nR for Data Science\nModern Data Science with R\nPosit Cheatsheets\nQuarto\nggplot2\nTidyTuesday\nAdvanced Data Analysis from an Elementary Point of View\nBeyond Multiple Linear Regression\nAn Introduction to Statistical Learning\nThe Elements of Statistical Learning\nMore recommended R books"
  },
  {
    "objectID": "lectures/12-selection.html#setting",
    "href": "lectures/12-selection.html#setting",
    "title": "Supervised learning: variable selection",
    "section": "Setting",
    "text": "Setting\nWe wish to learn a linear model. Our estimate (denoted by hats) is \\[\n\\hat{Y} = \\hat{\\beta}_0 + \\hat{\\beta}_1 X_1 + \\cdots + \\hat{\\beta}_p X_p\n\\]\n\nWhy would we attempt to select a subset of the \\(p\\) variables?\n\n\n\nTo improve prediction accuracy\n\nEliminating uninformative predictors can lead to lower variance in the test set MSE, at the expense of a slight increase in bias\n\n\n\n\n\nTo improve model interpretability\n\nEliminating uninformative predictors is obviously a good thing when your goal is to tell the story of how your predictors are associated with your response."
  },
  {
    "objectID": "lectures/12-selection.html#best-subset-selection",
    "href": "lectures/12-selection.html#best-subset-selection",
    "title": "Supervised learning: variable selection",
    "section": "Best subset selection",
    "text": "Best subset selection\n\nStart with the null model \\(\\mathcal{M}_0\\) (intercept-only) that has no predictors\n\nPredict the sample mean for each observation\n\n\n\n\nFor \\(k = 1, 2, \\dots, p\\) (each possible number of predictors)\n\nFit all \\(\\binom{p}{k} = \\frac{p!}{k!(p-k)!}\\) with exactly \\(k\\) predictors\nPick the best among these \\(\\binom{p}{k}\\) models, call it \\(\\mathcal{M}_k\\)\n\n“Best” can be based on cross-validation error, highest adjusted \\(R^2\\), etc.\n“It depends on your loss function”\n\n\n\n\n\n\nSelect a single best model from among \\(\\mathcal{M}_0, \\dots, \\mathcal{M}_p\\)\n\n\n\nThis is not typically used in research!\n\nonly practical for a smaller number of variables\narbitrary way of defining best and ignores prior knowledge about potential predictors"
  },
  {
    "objectID": "lectures/12-selection.html#use-the-shoe-leather-approach",
    "href": "lectures/12-selection.html#use-the-shoe-leather-approach",
    "title": "Supervised learning: variable selection",
    "section": "Use the shoe leather approach",
    "text": "Use the shoe leather approach\nDo not turn off your brain!\n\nalgorithms can be tempting but they are NOT substitutes!\nyou should NOT avoid the hard work of EDA in your modeling efforts\n\n\nVariable selection is a difficult problem!\n\nLike much of a statistics research, there is not one unique, correct answer\n\n\n\nJustify which predictors / variables used in modeling based on:\n\ncontext\nextensive EDA\nmodel assessment based on holdout predictions\n\n\n\n\nRecommended reading: David A. Freedman (1991). Statistical Models and Shoe Leather"
  },
  {
    "objectID": "lectures/12-selection.html#covariance-and-correlation",
    "href": "lectures/12-selection.html#covariance-and-correlation",
    "title": "Supervised learning: variable selection",
    "section": "Covariance and correlation",
    "text": "Covariance and correlation\n\nCovariance is a measure of the linear dependence between two variables\n\nTo be “uncorrelated” is not the same as to be “independent”…\nIndependence means there is no dependence, linear or otherwise\n\n\n\n\nCorrelation is a normalized form of covariance, ranges from -1 to 1\n\n-1 means one variable linearly decreases absolutely in value while the other increases\n0 means no linear dependence\n1 means one variable linear increases absolutely while the other increases\n\n\n\n\n\nWe can use the cov() / cor() functions in R to generate the covariance / correlation matrices"
  },
  {
    "objectID": "lectures/12-selection.html#data-hollywood-movies-2012-2018",
    "href": "lectures/12-selection.html#data-hollywood-movies-2012-2018",
    "title": "Supervised learning: variable selection",
    "section": "Data: Hollywood Movies (2012-2018)",
    "text": "Data: Hollywood Movies (2012-2018)\n\nlibrary(tidyverse)\ntheme_set(theme_light())\nmovies &lt;- read_csv(\"https://www.lock5stat.com/datasets3e/HollywoodMovies.csv\")\nglimpse(movies)\n\nRows: 1,295\nColumns: 15\n$ Movie            &lt;chr&gt; \"2016: Obama's America\", \"21 Jump Street\", \"A Late Qu…\n$ LeadStudio       &lt;chr&gt; \"Rocky Mountain Pictures\", \"Sony Pictures Releasing\",…\n$ RottenTomatoes   &lt;dbl&gt; 26, 85, 76, 90, 35, 27, 91, 56, 11, 44, 93, 63, 87, 9…\n$ AudienceScore    &lt;dbl&gt; 73, 82, 71, 82, 51, 72, 62, 47, 47, 63, 82, 51, 63, 9…\n$ Genre            &lt;chr&gt; \"Documentary\", \"Comedy\", \"Drama\", \"Drama\", \"Horror\", …\n$ TheatersOpenWeek &lt;dbl&gt; 1, 3121, 9, 7, 3108, 3039, 132, 245, 2539, 3192, 3, 1…\n$ OpeningWeekend   &lt;dbl&gt; 0.03, 36.30, 0.08, 0.04, 16.31, 24.48, 1.14, 0.70, 11…\n$ BOAvgOpenWeekend &lt;dbl&gt; 30000, 11631, 8889, 5714, 5248, 8055, 8636, 2857, 449…\n$ Budget           &lt;dbl&gt; 3.0, 42.0, NA, NA, 68.0, 12.0, NA, 7.5, 35.0, 50.0, 1…\n$ DomesticGross    &lt;dbl&gt; 33.35, 138.45, 1.56, 1.55, 37.52, 70.01, 1.99, 3.01, …\n$ WorldGross       &lt;dbl&gt; 33.35, 202.81, 6.30, 7.60, 137.49, 82.50, 3.59, 8.54,…\n$ ForeignGross     &lt;dbl&gt; 0.00, 64.36, 4.74, 6.05, 99.97, 12.49, 1.60, 5.53, 9.…\n$ Profitability    &lt;dbl&gt; 1334.00, 482.88, NA, NA, 202.19, 687.50, NA, 113.87, …\n$ OpenProfit       &lt;dbl&gt; 1.20, 86.43, NA, NA, 23.99, 204.00, NA, 9.33, 32.57, …\n$ Year             &lt;dbl&gt; 2012, 2012, 2012, 2012, 2012, 2012, 2012, 2012, 2012,…\n\nmovies &lt;- movies |&gt; \n  select(AudienceScore, RottenTomatoes, TheatersOpenWeek, OpeningWeekend, Budget, DomesticGross, ForeignGross) |&gt; \n  drop_na()"
  },
  {
    "objectID": "lectures/12-selection.html#modeling-audience-rating",
    "href": "lectures/12-selection.html#modeling-audience-rating",
    "title": "Supervised learning: variable selection",
    "section": "Modeling audience rating",
    "text": "Modeling audience rating\nInterested in modeling the audience rating of a movie\n\nmovies |&gt; \n  ggplot(aes(x = AudienceScore)) +\n  geom_histogram(fill = \"gray\", color = \"white\")"
  },
  {
    "objectID": "lectures/12-selection.html#correlation-matrix-of-score-differential-and-candidate-predictors",
    "href": "lectures/12-selection.html#correlation-matrix-of-score-differential-and-candidate-predictors",
    "title": "Supervised learning: variable selection",
    "section": "Correlation matrix of score differential and candidate predictors",
    "text": "Correlation matrix of score differential and candidate predictors\n\n\n\nInterested in AudienceScore relationships with opening critics rating, weekend statistics, budget, gross income of viewers\nPlot correlation matrix with ggcorrplot\n\n\n# can also use corrr package\n# library(corrr)\n# movies |&gt; \n#   correlate(diagonal = 1) |&gt; # get correlation matrix\n#   stretch() |&gt;  # similar to pivot_longer\n#   ggplot(aes(x, y, fill = r)) +\n#   geom_tile()\nlibrary(ggcorrplot)\nmovies_cor &lt;- cor(movies)\nggcorrplot(movies_cor)"
  },
  {
    "objectID": "lectures/12-selection.html#customize-the-appearance-of-the-correlation-matrix",
    "href": "lectures/12-selection.html#customize-the-appearance-of-the-correlation-matrix",
    "title": "Supervised learning: variable selection",
    "section": "Customize the appearance of the correlation matrix",
    "text": "Customize the appearance of the correlation matrix\n\n\n\nAvoid redundancy by only using one half of matrix with type\nAdd correlation value labels using lab (but round first!)\nCan arrange variables based on clustering…\n\n\nmovies_cor |&gt; \n  round(2) |&gt; \n  ggcorrplot(hc.order = TRUE, type = \"lower\", lab = TRUE)"
  },
  {
    "objectID": "lectures/12-selection.html#clustering-variables-using-the-correlation-matrix",
    "href": "lectures/12-selection.html#clustering-variables-using-the-correlation-matrix",
    "title": "Supervised learning: variable selection",
    "section": "Clustering variables using the correlation matrix",
    "text": "Clustering variables using the correlation matrix\nApply hierarchical clustering to variables instead of observations\n\n\nSelect the explanatory variables of interest from our data\n\n\nmovies_feat &lt;- movies |&gt; \n  select(-AudienceScore)\n\n\n\n\nCompute correlation matrix of these variables\n\n\nfeat_cor &lt;- cor(movies_feat)\n\n\n\n\nCorrelations measure similarity and can be negative BUT distances measure dissimilarity and CANNOT\nConvert your correlations to all be \\(\\geq 0\\): e.g., \\(1 - |\\rho|\\) (which drops the sign) or \\(1 - \\rho\\)\n\n\ncor_dist_matrix &lt;- 1 - abs(feat_cor)\n\n\n\n\nConvert to distance matrix before using hclust\n\n\ncor_dist_matrix &lt;- as.dist(cor_dist_matrix)"
  },
  {
    "objectID": "lectures/12-selection.html#clustering-variables-using-the-correlation-matrix-1",
    "href": "lectures/12-selection.html#clustering-variables-using-the-correlation-matrix-1",
    "title": "Supervised learning: variable selection",
    "section": "Clustering variables using the correlation matrix",
    "text": "Clustering variables using the correlation matrix\n\n\n\nCluster variables using hclust() as before\nUse ggdendro to quickly visualize dendrogram\n\n\nlibrary(ggdendro)\nmovies_feat_hc &lt;- hclust(cor_dist_matrix, \"complete\")\nggdendrogram(movies_feat_hc,\n             rotate = TRUE,\n             size = 2)"
  },
  {
    "objectID": "lectures/12-selection.html#clustering-variables-using-the-correlation-matrix-2",
    "href": "lectures/12-selection.html#clustering-variables-using-the-correlation-matrix-2",
    "title": "Supervised learning: variable selection",
    "section": "Clustering variables using the correlation matrix",
    "text": "Clustering variables using the correlation matrix\n\n\n\nAnother flexible option is dendextend\n\n\nlibrary(dendextend)\ncor_dist_matrix |&gt;\n  hclust() |&gt;\n  as.dendrogram() |&gt;\n  set(\"branches_k_col\", k = 2) |&gt;\n  set(\"labels_cex\", 1) |&gt;\n  ggplot(horiz = TRUE)\n\n\nExplore the package documentation for more formatting"
  },
  {
    "objectID": "lectures/12-selection.html#back-to-the-response-variable",
    "href": "lectures/12-selection.html#back-to-the-response-variable",
    "title": "Supervised learning: variable selection",
    "section": "Back to the response variable…",
    "text": "Back to the response variable…\nPairs plot using GGally\n\nalways look at your data\ncorrelation values alone are not enough!\nwhat if a variable displayed a nonlinear (e.g. quadratic) relationship?\n\n\nlibrary(GGally)\nggpairs(movies)"
  },
  {
    "objectID": "lectures/12-selection.html#back-to-the-response-variable-1",
    "href": "lectures/12-selection.html#back-to-the-response-variable-1",
    "title": "Supervised learning: variable selection",
    "section": "Back to the response variable…",
    "text": "Back to the response variable…"
  },
  {
    "objectID": "lectures/12-selection.html#which-variables-matter-for-modeling-audience-rating",
    "href": "lectures/12-selection.html#which-variables-matter-for-modeling-audience-rating",
    "title": "Supervised learning: variable selection",
    "section": "Which variables matter for modeling audience rating?",
    "text": "Which variables matter for modeling audience rating?\n\nUse 10-fold cross-validation to assess how well different sets of variables perform in predicting AudienceScore?\n\n\nCreate a column of test fold assignments to our dataset with the sample() function:\n\nset.seed(100)\nk &lt;- 10\nmovies &lt;- movies |&gt;\n  mutate(test_fold = sample(rep(1:k, length.out = n())))\n\n# table(movies$test_fold)  \n\n\n\nAlways remember to set.seed() prior to performing \\(k\\)-fold cross-validation!"
  },
  {
    "objectID": "lectures/12-selection.html#writing-a-function-for-k-fold-cross-validation",
    "href": "lectures/12-selection.html#writing-a-function-for-k-fold-cross-validation",
    "title": "Supervised learning: variable selection",
    "section": "Writing a function for \\(k\\)-fold cross-validation",
    "text": "Writing a function for \\(k\\)-fold cross-validation\n\nget_cv_pred &lt;- function(model_formula, data = movies) {\n  # generate holdout predictions for every row based season\n  get_test_pred &lt;- function(fold) {\n  \n    # separate test and training data\n  \n    test_data &lt;- data |&gt; filter(test_fold == fold)\n    train_data &lt;- data |&gt; filter(test_fold != fold)\n    train_fit &lt;- lm(as.formula(model_formula), data = train_data)\n  \n    # return test results\n    test_res &lt;- tibble(test_pred = predict(train_fit, newdata = test_data),\n                       test_actual = test_data$AudienceScore,\n                       test_fold = fold)\n    return(test_res)\n  }\n  \n  test_pred &lt;- map(1:k, get_test_pred) |&gt; \n    bind_rows()\n  \n  return(test_pred)\n}"
  },
  {
    "objectID": "lectures/12-selection.html#function-enables-easy-generation-of-holdout-analysis",
    "href": "lectures/12-selection.html#function-enables-easy-generation-of-holdout-analysis",
    "title": "Supervised learning: variable selection",
    "section": "Function enables easy generation of holdout analysis",
    "text": "Function enables easy generation of holdout analysis\n\nall_pred &lt;- get_cv_pred(\n  \"AudienceScore ~ RottenTomatoes + TheatersOpenWeek + OpeningWeekend + Budget + DomesticGross + ForeignGross\"\n)\nall_no_critics_pred &lt;- get_cv_pred(\n  \"AudienceScore ~ TheatersOpenWeek + OpeningWeekend + Budget + DomesticGross + ForeignGross\"\n)\ncritics_only_pred &lt;- get_cv_pred(\"AudienceScore ~ RottenTomatoes\")\nopening_only_pred &lt;- get_cv_pred(\"AudienceScore ~ Budget + TheatersOpenWeek + OpeningWeekend + RottenTomatoes\")\ngross_only_pred &lt;- get_cv_pred(\"AudienceScore ~ DomesticGross + ForeignGross + RottenTomatoes\")\nint_only_pred &lt;- get_cv_pred(\"AudienceScore ~ 1\")\n\n\n\n\nCan then summarize together for a single plot:\n\nbind_rows(\n  mutate(all_pred, mod = \"All\"),\n  mutate(all_no_critics_pred, mod = \"All but critics\"),\n  mutate(critics_only_pred, mod = \"Critics only\"),\n  mutate(opening_only_pred, mod = \"Opening only\"),\n  mutate(gross_only_pred, mod = \"Gross income only\"),\n  mutate(int_only_pred, mod = \"Intercept only\")\n) |&gt;\n  group_by(mod) |&gt;\n  summarize(\n    rmse = sqrt(mean((test_actual - test_pred)^2))\n  ) |&gt;\n  mutate(mod = fct_reorder(mod, rmse)) |&gt;\n  ggplot(aes(x = rmse, y = mod)) +\n  geom_point()"
  },
  {
    "objectID": "lectures/12-selection.html#fit-selected-model-on-all-data-and-view-summary",
    "href": "lectures/12-selection.html#fit-selected-model-on-all-data-and-view-summary",
    "title": "Supervised learning: variable selection",
    "section": "Fit selected model on all data and view summary",
    "text": "Fit selected model on all data and view summary\n\nall_fit &lt;- lm(\n  AudienceScore ~ RottenTomatoes + TheatersOpenWeek + OpeningWeekend + Budget + DomesticGross + ForeignGross, \n  data = movies\n)\n# summary(all_fit)\nlibrary(broom)\nall_fit |&gt; tidy()\n\n# A tibble: 7 × 5\n  term             estimate std.error statistic   p.value\n  &lt;chr&gt;               &lt;dbl&gt;     &lt;dbl&gt;     &lt;dbl&gt;     &lt;dbl&gt;\n1 (Intercept)      40.5      1.35        29.9   1.66e-142\n2 RottenTomatoes    0.395    0.0160      24.7   1.74e-106\n3 TheatersOpenWeek -0.00214  0.000393    -5.44  6.55e-  8\n4 OpeningWeekend   -0.114    0.0362      -3.16  1.62e-  3\n5 Budget            0.0168   0.0116       1.45  1.46e-  1\n6 DomesticGross     0.0690   0.0125       5.52  4.16e-  8\n7 ForeignGross      0.00415  0.00497      0.835 4.04e-  1\n\n\n\n\nBut… do NOT show a coefficients table in a presentation (well… it depends)\nA nicely formatted table of the summary output is more appropriate in a written report\nPackages that can take a model object and produce a neat table summary: texreg, modelsummary, gtsummary, huxtable, sjPlot"
  },
  {
    "objectID": "lectures/12-selection.html#coefficient-plot-or-dot-and-whisker-plot-or-forest-plot-or-blobbogram",
    "href": "lectures/12-selection.html#coefficient-plot-or-dot-and-whisker-plot-or-forest-plot-or-blobbogram",
    "title": "Supervised learning: variable selection",
    "section": "Coefficient plot (or dot-and-whisker plot or forest plot or blobbogram)",
    "text": "Coefficient plot (or dot-and-whisker plot or forest plot or blobbogram)\nDisplay a coefficient plot with confidence intervals based on the reported standard errors\n\nall_fit |&gt; \n  tidy(conf.int = TRUE) |&gt; \n  filter(term != \"(Intercept)\") |&gt; \n  ggplot(aes(x = estimate, y = term))  +\n  geom_point(size = 4) +\n  geom_errorbar(aes(xmin = conf.low, xmax = conf.high, width = 0.2)) +\n  geom_vline(xintercept = 0, linetype = \"dashed\", color = \"red\")\n\n\n\n\n\n\n\n\n\n36-SURE.github.io"
  },
  {
    "objectID": "lectures/04-quantitative.html#quantitative-data",
    "href": "lectures/04-quantitative.html#quantitative-data",
    "title": "Data visualization: quantitative data",
    "section": "Quantitative data",
    "text": "Quantitative data\nTwo different versions of quantitative data:\nDiscrete: countable and has clear space between values (i.e. whole number only)\n\nExamples: number of goals scored in a game, number of children in a family\n\nContinuous: can take any value within some interval\n\nExamples: price of houses in Pittsburgh, water temperature, wind speed"
  },
  {
    "objectID": "lectures/04-quantitative.html#data",
    "href": "lectures/04-quantitative.html#data",
    "title": "Data visualization: quantitative data",
    "section": "Data",
    "text": "Data\nTaylor Swift songs via the taylor package (data dictionary here)\n\nlibrary(tidyverse)\ntheme_set(theme_light())\nlibrary(taylor)\nnames(taylor_all_songs)\n\n [1] \"album_name\"          \"ep\"                  \"album_release\"      \n [4] \"track_number\"        \"track_name\"          \"artist\"             \n [7] \"featuring\"           \"bonus_track\"         \"promotional_release\"\n[10] \"single_release\"      \"track_release\"       \"danceability\"       \n[13] \"energy\"              \"key\"                 \"loudness\"           \n[16] \"mode\"                \"speechiness\"         \"acousticness\"       \n[19] \"instrumentalness\"    \"liveness\"            \"valence\"            \n[22] \"tempo\"               \"time_signature\"      \"duration_ms\"        \n[25] \"explicit\"            \"key_name\"            \"mode_name\"          \n[28] \"key_mode\"            \"lyrics\"             \n\ntaylor_all_songs &lt;- taylor_all_songs |&gt; \n  mutate(duration = duration_ms / 60000)"
  },
  {
    "objectID": "lectures/04-quantitative.html#summarizing-1d-quantitative-data",
    "href": "lectures/04-quantitative.html#summarizing-1d-quantitative-data",
    "title": "Data visualization: quantitative data",
    "section": "Summarizing 1D quantitative data",
    "text": "Summarizing 1D quantitative data\n\nCenter: mean, median, number and location of modes\n\n\n\nSpread: range, variance, standard deviation, IQR, etc.\n\n\n\n\nShape: skew vs symmetry, outliers, heavy vs light tails, etc.\n\n\n\nCompute various statistics in R with summary(), mean(), median(), quantile(), range(), sd(), var(), etc.\n\n\nExample: Summarizing the duration of Taylor Swift songs\n\nsummary(taylor_all_songs$duration)\n\n   Min. 1st Qu.  Median    Mean 3rd Qu.    Max.    NA's \n  2.198   3.543   3.930   3.992   4.349  10.217      11 \n\nsd(taylor_all_songs$duration, na.rm = TRUE)\n\n[1] 0.7562114"
  },
  {
    "objectID": "lectures/04-quantitative.html#boxplots-visualize-summary-statistics",
    "href": "lectures/04-quantitative.html#boxplots-visualize-summary-statistics",
    "title": "Data visualization: quantitative data",
    "section": "Boxplots visualize summary statistics",
    "text": "Boxplots visualize summary statistics\n\n\nPros:\n\nDisplays outliers, percentiles, spread, skew\nUseful for side-by-side comparison\n\nCons:\n\nDoes not display the full distribution shape\nDoes not display modes\n\nThe expert weighed in…\n\n\ntaylor_all_songs |&gt; \n  ggplot(aes(x = duration)) +\n  geom_boxplot() +\n  theme(axis.text.y = element_blank())"
  },
  {
    "objectID": "lectures/04-quantitative.html#histograms-display-1d-continuous-distributions",
    "href": "lectures/04-quantitative.html#histograms-display-1d-continuous-distributions",
    "title": "Data visualization: quantitative data",
    "section": "Histograms display 1D continuous distributions",
    "text": "Histograms display 1D continuous distributions\n\n\n\\(\\displaystyle \\text{# total obs.} = \\sum_{j=1}^k \\text{# obs. in bin }j\\)\nPros:\n\nDisplays full shape of distribution\nEasy to interpret\n\nCons:\n\nHave to choose number of bins and bin locations (will revisit later)\n\n\n\ntaylor_all_songs |&gt; \n  ggplot(aes(x = duration)) +\n  geom_histogram()"
  },
  {
    "objectID": "lectures/04-quantitative.html#display-the-data-points-directly-with-beeswarm-plots",
    "href": "lectures/04-quantitative.html#display-the-data-points-directly-with-beeswarm-plots",
    "title": "Data visualization: quantitative data",
    "section": "Display the data points directly with beeswarm plots",
    "text": "Display the data points directly with beeswarm plots\n\n\nPros:\n\nDisplays each data point\nEasy to view full shape of distribution\n\nCons:\n\nCan be overbearing with large datasets\nWhich algorithm for arranging points?\n\n\n\nlibrary(ggbeeswarm)\ntaylor_all_songs |&gt; \n  ggplot(aes(x = duration, y = \"\")) +\n  geom_beeswarm(cex = 2)"
  },
  {
    "objectID": "lectures/04-quantitative.html#smooth-summary-with-violin-plots",
    "href": "lectures/04-quantitative.html#smooth-summary-with-violin-plots",
    "title": "Data visualization: quantitative data",
    "section": "Smooth summary with violin plots",
    "text": "Smooth summary with violin plots\n\n\nPros:\n\nDisplays full shape of distribution\nCan easily layer…\n\n\n\ntaylor_all_songs |&gt; \n  ggplot(aes(x = duration, y = \"\")) +\n  geom_violin()"
  },
  {
    "objectID": "lectures/04-quantitative.html#smooth-summary-with-violin-plots-box-plots",
    "href": "lectures/04-quantitative.html#smooth-summary-with-violin-plots-box-plots",
    "title": "Data visualization: quantitative data",
    "section": "Smooth summary with violin plots + box plots",
    "text": "Smooth summary with violin plots + box plots\n\n\nPros:\n\nDisplays full shape of distribution\nCan easily layer… with box plots on top\n\nCons:\n\nSummary of data via density estimate\nMirror image is duplicate information\n\n\n\ntaylor_all_songs |&gt; \n  ggplot(aes(x = duration, y = \"\")) +\n  geom_violin() +\n  geom_boxplot(width = 0.4)"
  },
  {
    "objectID": "lectures/04-quantitative.html#what-do-visualizations-of-continuous-distributions-display",
    "href": "lectures/04-quantitative.html#what-do-visualizations-of-continuous-distributions-display",
    "title": "Data visualization: quantitative data",
    "section": "What do visualizations of continuous distributions display?",
    "text": "What do visualizations of continuous distributions display?\nProbability that continuous variable \\(X\\) takes a particular value is 0\ne.g. \\(P(\\) duration \\(= 3) = 0\\) (why?)\nFor continuous variables, the cumulative distribution function (CDF) is \\[F(x) = P(X \\leq x)\\]\nFor \\(n\\) observations, the empirical CDF (ECDF) can be computed based on the observed data \\[\\hat{F}_n(x)  = \\frac{\\text{# obs. with variable} \\leq x}{n} = \\frac{1}{n} \\sum_{i=1}^{n} I (x_i \\leq x)\\]\nwhere \\(I()\\) is the indicator function, i.e. ifelse(x_i &lt;= x, 1, 0)"
  },
  {
    "objectID": "lectures/04-quantitative.html#display-full-distribution-with-ecdf-plot",
    "href": "lectures/04-quantitative.html#display-full-distribution-with-ecdf-plot",
    "title": "Data visualization: quantitative data",
    "section": "Display full distribution with ECDF plot",
    "text": "Display full distribution with ECDF plot\n\n\nPros:\n\nDisplays all of your data at once\nAs \\(n \\rightarrow \\infty\\), the ECDF \\(\\hat F_n(x)\\) converges to the true CDF \\(F(x)\\)\n\nCons:\n\nWhat are the cons?\n\n\n\ntaylor_all_songs |&gt; \n  ggplot(aes(x = duration)) +\n  stat_ecdf()"
  },
  {
    "objectID": "lectures/04-quantitative.html#rug-plots-display-raw-data",
    "href": "lectures/04-quantitative.html#rug-plots-display-raw-data",
    "title": "Data visualization: quantitative data",
    "section": "Rug plots display raw data",
    "text": "Rug plots display raw data\n\n\nPros:\n\nDisplays raw data points\nUseful supplement for summaries and 2D plots\n\nCons:\n\nCan be overbearing for large datasets\n\n\n\ntaylor_all_songs |&gt; \n  ggplot(aes(x = duration)) +\n  geom_rug(alpha = 0.5)"
  },
  {
    "objectID": "lectures/04-quantitative.html#rug-plots-supplement-other-displays",
    "href": "lectures/04-quantitative.html#rug-plots-supplement-other-displays",
    "title": "Data visualization: quantitative data",
    "section": "Rug plots supplement other displays",
    "text": "Rug plots supplement other displays\n\n\n\ntaylor_all_songs |&gt; \n  ggplot(aes(x = duration)) +\n  geom_histogram() +\n  geom_rug(alpha = 0.5)\n\n\n\n\n\n\n\n\n\n\ntaylor_all_songs |&gt; \n  ggplot(aes(x = duration)) +\n  stat_ecdf() +\n  geom_rug(alpha = 0.5)"
  },
  {
    "objectID": "lectures/04-quantitative.html#summarizing-2d-quantitative-data",
    "href": "lectures/04-quantitative.html#summarizing-2d-quantitative-data",
    "title": "Data visualization: quantitative data",
    "section": "Summarizing 2D quantitative data",
    "text": "Summarizing 2D quantitative data\n\nDirection/trend (positive, negative)\nStrength of the relationship (strong, moderate, weak)\nLinearity (linear, non-linear)\n\n\nBig picture\n\nScatterplots are by far the most common visual\nRegression analysis is by far the most popular analysis (we will have a class on this)\nRelationships may vary across other variables, e.g., categorical variables"
  },
  {
    "objectID": "lectures/04-quantitative.html#making-scatterplots",
    "href": "lectures/04-quantitative.html#making-scatterplots",
    "title": "Data visualization: quantitative data",
    "section": "Making scatterplots",
    "text": "Making scatterplots\n\n\n\nUse geom_point()\nDisplaying the joint (bivariate) distribution\nWhat is the obvious flaw with this plot?\n\n\n\ntaylor_all_songs |&gt; \n  ggplot(aes(x = loudness, y = energy)) +\n  geom_point(color = \"darkred\", size = 4)"
  },
  {
    "objectID": "lectures/04-quantitative.html#making-scatterplots-always-adjust-the-transparency-alpha",
    "href": "lectures/04-quantitative.html#making-scatterplots-always-adjust-the-transparency-alpha",
    "title": "Data visualization: quantitative data",
    "section": "Making scatterplots: always adjust the transparency (alpha)",
    "text": "Making scatterplots: always adjust the transparency (alpha)\n\n\n\nAdjust the transparency of points via alpha to visualize overlap\nProvides better understanding of joint frequency\nEspecially important with larger datasets\nSee also: ggblend\n\n\n\ntaylor_all_songs |&gt; \n  ggplot(aes(x = loudness, y = energy)) +\n  geom_point(color = \"darkred\", size = 4, alpha = 0.5)"
  },
  {
    "objectID": "lectures/04-quantitative.html#summarizing-2d-quantitative-data-1",
    "href": "lectures/04-quantitative.html#summarizing-2d-quantitative-data-1",
    "title": "Data visualization: quantitative data",
    "section": "Summarizing 2D quantitative data",
    "text": "Summarizing 2D quantitative data\n\n\n\nScatterplot\n\n\ntaylor_all_songs |&gt; \n  ggplot(aes(x = loudness, y = energy)) +\n  geom_point(color = \"darkred\", size = 4, alpha = 0.5)\n\n\n\n\n\n\n\n\n\n\nCorrelation coefficient\n\n\ncor(taylor_all_songs$loudness, \n    taylor_all_songs$energy, \n    use = \"complete.obs\")\n\n[1] 0.7826175\n\n\nNote: the default correlation you get from cor() is Pearson correlation coefficient\nOther correlations:\n\nSpearman’s correlation\nKendall rank correlation coefficient\nand more"
  },
  {
    "objectID": "lectures/04-quantitative.html#when-the-correlations-high",
    "href": "lectures/04-quantitative.html#when-the-correlations-high",
    "title": "Data visualization: quantitative data",
    "section": "When the correlation’s high…",
    "text": "When the correlation’s high…"
  },
  {
    "objectID": "lectures/04-quantitative.html#displaying-trend-line-linear-regression-a-preview",
    "href": "lectures/04-quantitative.html#displaying-trend-line-linear-regression-a-preview",
    "title": "Data visualization: quantitative data",
    "section": "Displaying trend line: linear regression (a preview)",
    "text": "Displaying trend line: linear regression (a preview)\n\n\n\nDisplay regression line for energy ~ loudness\n95% confidence intervals by default\nEstimating the conditional expectation of energy | loudness\n\ni.e., \\(\\mathbb{E}[\\) energy \\(\\mid\\) loudness \\(]\\)\n\n\n\n\ntaylor_all_songs |&gt; \n  ggplot(aes(x = loudness, y = energy)) +\n  geom_point(color = \"darkred\", size = 4, alpha = 0.5) +\n  geom_smooth(method = \"lm\", linewidth = 2)"
  },
  {
    "objectID": "lectures/04-quantitative.html#summarizing-2d-quantitative-data-2",
    "href": "lectures/04-quantitative.html#summarizing-2d-quantitative-data-2",
    "title": "Data visualization: quantitative data",
    "section": "Summarizing 2D quantitative data",
    "text": "Summarizing 2D quantitative data\n\n\nAdd rug plots to supplement scatterplot\n\ntaylor_all_songs |&gt; \n  ggplot(aes(x = loudness, y = energy)) +\n  geom_point(color = \"darkred\", size = 4, alpha = 0.5) +\n  geom_rug(alpha = 0.4)"
  },
  {
    "objectID": "lectures/04-quantitative.html#pairs-plot",
    "href": "lectures/04-quantitative.html#pairs-plot",
    "title": "Data visualization: quantitative data",
    "section": "Pairs plot",
    "text": "Pairs plot\n\nlibrary(GGally)\ntaylor_all_songs |&gt; \n  select(danceability, energy, loudness, tempo) |&gt; \n  ggpairs()"
  },
  {
    "objectID": "lectures/04-quantitative.html#continuous-by-categorical-side-by-side-plots",
    "href": "lectures/04-quantitative.html#continuous-by-categorical-side-by-side-plots",
    "title": "Data visualization: quantitative data",
    "section": "Continuous by categorical: side by side plots",
    "text": "Continuous by categorical: side by side plots\n\ntaylor_all_songs |&gt; \n  filter(album_name %in% c(\"Lover\", \"folklore\", \"evermore\", \"Midnights\")) |&gt;\n  ggplot(aes(x = duration, y = album_name)) +\n  geom_violin() +\n  geom_boxplot(width = 0.4)"
  },
  {
    "objectID": "lectures/04-quantitative.html#continuous-by-categorical-color",
    "href": "lectures/04-quantitative.html#continuous-by-categorical-color",
    "title": "Data visualization: quantitative data",
    "section": "Continuous by categorical: color",
    "text": "Continuous by categorical: color\n\ntaylor_all_songs |&gt; \n  filter(album_name %in% c(\"Lover\", \"folklore\", \"evermore\", \"Midnights\")) |&gt;\n  ggplot(aes(x = duration, color = album_name)) +\n  stat_ecdf(linewidth = 1) +\n  scale_color_albums() + # from the taylor package \n  theme(legend.position = \"bottom\")"
  },
  {
    "objectID": "lectures/04-quantitative.html#continuous-by-categorical-ridgeline-plot-joyplot",
    "href": "lectures/04-quantitative.html#continuous-by-categorical-ridgeline-plot-joyplot",
    "title": "Data visualization: quantitative data",
    "section": "Continuous by categorical: ridgeline plot (joyplot)",
    "text": "Continuous by categorical: ridgeline plot (joyplot)\nFor more, see this tutorial\n\nlibrary(ggridges)\ntaylor_all_songs |&gt; \n  filter(album_name %in% c(\"Lover\", \"folklore\", \"evermore\", \"Midnights\")) |&gt;\n  ggplot(aes(x = duration, y = album_name)) +\n  geom_density_ridges(scale = 1)"
  },
  {
    "objectID": "lectures/04-quantitative.html#what-about-for-histograms",
    "href": "lectures/04-quantitative.html#what-about-for-histograms",
    "title": "Data visualization: quantitative data",
    "section": "What about for histograms?",
    "text": "What about for histograms?\n\ntaylor_all_songs |&gt; \n  filter(album_name %in% c(\"Lover\", \"folklore\", \"evermore\", \"Midnights\")) |&gt;\n  ggplot(aes(x = duration, fill = album_name)) +\n  geom_histogram(alpha = 0.6, bins = 15) +\n  scale_fill_albums()"
  },
  {
    "objectID": "lectures/04-quantitative.html#what-about-facets",
    "href": "lectures/04-quantitative.html#what-about-facets",
    "title": "Data visualization: quantitative data",
    "section": "What about facets?",
    "text": "What about facets?\nDifference between facet_wrap and facet_grid\n\ntaylor_all_songs |&gt; \n  filter(album_name %in% c(\"Lover\", \"folklore\", \"evermore\", \"Midnights\")) |&gt;\n  ggplot(aes(x = duration)) +\n  geom_histogram(bins = 15) +\n  facet_wrap(~ album_name, nrow = 1)"
  },
  {
    "objectID": "lectures/04-quantitative.html#what-about-facets-1",
    "href": "lectures/04-quantitative.html#what-about-facets-1",
    "title": "Data visualization: quantitative data",
    "section": "What about facets?",
    "text": "What about facets?\n\ntaylor_all_songs |&gt; \n  filter(album_name %in% c(\"Lover\", \"folklore\", \"evermore\", \"Midnights\")) |&gt;\n  ggplot(aes(x = duration)) +\n  geom_histogram(bins = 15) +\n  facet_grid(album_name ~ ., margins = TRUE)\n\n\n\n\n\n\n\n\n\n\n\n\n36-SURE.github.io"
  },
  {
    "objectID": "lectures/01-explore.html#workflow-diagram",
    "href": "lectures/01-explore.html#workflow-diagram",
    "title": "Exploring data: into the tidyverse",
    "section": "Workflow diagram",
    "text": "Workflow diagram\n\nSource: R for Data Science (2e)\nExploring data: data wrangling and data visualization\nAspects of data wrangling\n\nimport: load in data (e.g., read_csv())\ntidy: each row is an observation, each column is a variable\ntransform: filter observations, create new variables, etc."
  },
  {
    "objectID": "lectures/01-explore.html#exploratory-data-analysis",
    "href": "lectures/01-explore.html#exploratory-data-analysis",
    "title": "Exploring data: into the tidyverse",
    "section": "Exploratory data analysis",
    "text": "Exploratory data analysis\n\nWhat is the goal of EDA?\n\nto perform initial investigations on the data in order to better understand the data, discover trends/patterns, spot anomalies, etc.\n\n\n\n\n“EDA is an iterative cycle”\n\nquestions about data ⟶ wrangling ⟶ visualization\n\n\n\n\n\n“EDA is a state of mind”\n\n\n\n\nThe term EDA was coined by statistician John Tukey in the 1970s."
  },
  {
    "objectID": "lectures/01-explore.html#exploratory-data-analysis-contd",
    "href": "lectures/01-explore.html#exploratory-data-analysis-contd",
    "title": "Exploring data: into the tidyverse",
    "section": "Exploratory data analysis (cont’d)",
    "text": "Exploratory data analysis (cont’d)\n\nData can be explored numerically (tables, descriptive statistics, etc.) or visually (graphs)\nExamples of questions\n\nWhat type of variation do the variables display?\nWhat type of relationships exist between variables?\n\n\n\n\nEDA is NOT a replacement for statistical inference and learning\n\n\n\n\nEDA is an important and necessary step to build intuition"
  },
  {
    "objectID": "lectures/01-explore.html#first-example-mlb-batting",
    "href": "lectures/01-explore.html#first-example-mlb-batting",
    "title": "Exploring data: into the tidyverse",
    "section": "First example: MLB batting",
    "text": "First example: MLB batting\n\nImport Batting table of historical batting statistics from the Lahman\n\n\nlibrary(tidyverse) # load the tidyverse\nlibrary(Lahman) # load the Lahman package to access its datasets\nBatting &lt;- as_tibble(Batting) # initialize the Batting dataset\n\n\n\nBasic info about the Batting dataset\n\n\n# number of rows and columns\n# can also do nrow(Batting) and ncol(Batting)\ndim(Batting) \n\n[1] 112184     22\n\n\n\nclass(Batting)\n\n[1] \"tbl_df\"     \"tbl\"        \"data.frame\"\n\n\n\n\n\ntbl (pronounced tibble) is the tidyverse way of storing tabular data, like a spreadsheet or data.frame"
  },
  {
    "objectID": "lectures/01-explore.html#first-example-mlb-batting-1",
    "href": "lectures/01-explore.html#first-example-mlb-batting-1",
    "title": "Exploring data: into the tidyverse",
    "section": "First example: MLB batting",
    "text": "First example: MLB batting\n\nView the first 6 (by default) rows with head()\n\n\n# try just typing Batting into your console, what happens?\n# also try glimpse(Batting)\nhead(Batting) \n\n# A tibble: 6 × 22\n  playerID  yearID stint teamID lgID      G    AB     R     H   X2B   X3B    HR\n  &lt;chr&gt;      &lt;int&gt; &lt;int&gt; &lt;fct&gt;  &lt;fct&gt; &lt;int&gt; &lt;int&gt; &lt;int&gt; &lt;int&gt; &lt;int&gt; &lt;int&gt; &lt;int&gt;\n1 abercda01   1871     1 TRO    NA        1     4     0     0     0     0     0\n2 addybo01    1871     1 RC1    NA       25   118    30    32     6     0     0\n3 allisar01   1871     1 CL1    NA       29   137    28    40     4     5     0\n4 allisdo01   1871     1 WS3    NA       27   133    28    44    10     2     2\n5 ansonca01   1871     1 RC1    NA       25   120    29    39    11     3     0\n6 armstbo01   1871     1 FW1    NA       12    49     9    11     2     1     0\n# ℹ 10 more variables: RBI &lt;int&gt;, SB &lt;int&gt;, CS &lt;int&gt;, BB &lt;int&gt;, SO &lt;int&gt;,\n#   IBB &lt;int&gt;, HBP &lt;int&gt;, SH &lt;int&gt;, SF &lt;int&gt;, GIDP &lt;int&gt;\n\n\n\nIs the Batting dataset tidy?\n\nEach row: a player’s season stint with a team (i.e. players can play for multiple teams in year)\nEach column: different measurement or recording about the player-team-season observation (get all column names with colnames(Batting) or names(Batting))"
  },
  {
    "objectID": "lectures/01-explore.html#descriptive-statistics",
    "href": "lectures/01-explore.html#descriptive-statistics",
    "title": "Exploring data: into the tidyverse",
    "section": "Descriptive statistics",
    "text": "Descriptive statistics\nSummarize quantitative (e.g. yearID, AB) and categorical (e.g. teamID, lgID) variables in different ways…\n\n\nSummary statistics for quantitative variables with the summary() function\n\n\nsummary(Batting$yearID)\n\n   Min. 1st Qu.  Median    Mean 3rd Qu.    Max. \n   1871    1938    1978    1969    2003    2022 \n\n\n\nCounts of categorical variables with the table() function\n\n\n# be careful it ignores NA values\n# can do table(Batting$lgID, useNA = \"always\")\ntable(Batting$lgID)\n\n\n   AA    AL    FL    NA    NL    PL    UA \n 1893 51799   472   737 56800   149   334"
  },
  {
    "objectID": "lectures/01-explore.html#the-dplyr-package",
    "href": "lectures/01-explore.html#the-dplyr-package",
    "title": "Exploring data: into the tidyverse",
    "section": "The dplyr package",
    "text": "The dplyr package\n\ndplyr is a package within the tidyverse with functions for data wrangling\nThe dplyr data verbs for manipulating data\n\nfilter()\nselect()\narrange()\nmutate()\ngroup_by()\nsummarize()"
  },
  {
    "objectID": "lectures/01-explore.html#filter",
    "href": "lectures/01-explore.html#filter",
    "title": "Exploring data: into the tidyverse",
    "section": "filter()",
    "text": "filter()\n\nUse filter() to extract ROWS (observations) that meet certain conditions\nNeed to specify a logical condition (aka boolean expression)"
  },
  {
    "objectID": "lectures/01-explore.html#filter-1",
    "href": "lectures/01-explore.html#filter-1",
    "title": "Exploring data: into the tidyverse",
    "section": "filter()",
    "text": "filter()\nExample: Extract batting stats for 2 leagues AL and NL only\n\nfilter(Batting, lgID %in% c(\"AL\", \"NL\")) # or filter(Batting, lgID == \"AL\" | lgID == \"NL\")\n\n# A tibble: 108,599 × 22\n   playerID  yearID stint teamID lgID      G    AB     R     H   X2B   X3B    HR\n   &lt;chr&gt;      &lt;int&gt; &lt;int&gt; &lt;fct&gt;  &lt;fct&gt; &lt;int&gt; &lt;int&gt; &lt;int&gt; &lt;int&gt; &lt;int&gt; &lt;int&gt; &lt;int&gt;\n 1 addybo01    1876     1 CHN    NL       32   142    36    40     4     1     0\n 2 allisar01   1876     1 LS1    NL       31   130     9    27     2     1     0\n 3 allisdo01   1876     1 HAR    NL       44   163    19    43     4     0     0\n 4 andrufr01   1876     1 CHN    NL        8    36     6    11     3     0     0\n 5 ansonca01   1876     1 CHN    NL       66   309    63   110     9     7     2\n 6 barnero01   1876     1 CHN    NL       66   322   126   138    21    14     1\n 7 battijo01   1876     1 SL3    NL       64   283    34    85    11     4     0\n 8 bechtge01   1876     1 LS1    NL       14    55     2    10     1     0     0\n 9 bechtge01   1876     2 NY3    NL        2    10     2     3     0     0     0\n10 berghjo01   1876     1 PHN    NL        1     4     0     0     0     0     0\n# ℹ 108,589 more rows\n# ℹ 10 more variables: RBI &lt;int&gt;, SB &lt;int&gt;, CS &lt;int&gt;, BB &lt;int&gt;, SO &lt;int&gt;,\n#   IBB &lt;int&gt;, HBP &lt;int&gt;, SH &lt;int&gt;, SF &lt;int&gt;, GIDP &lt;int&gt;"
  },
  {
    "objectID": "lectures/01-explore.html#filter-2",
    "href": "lectures/01-explore.html#filter-2",
    "title": "Exploring data: into the tidyverse",
    "section": "filter()",
    "text": "filter()\nExample: Extract batting stats for Pirates players in 2022\n\n# multiple conditions\nfilter(Batting, yearID == 2022 & teamID == \"PIT\")\n\n# A tibble: 68 × 22\n   playerID  yearID stint teamID lgID      G    AB     R     H   X2B   X3B    HR\n   &lt;chr&gt;      &lt;int&gt; &lt;int&gt; &lt;fct&gt;  &lt;fct&gt; &lt;int&gt; &lt;int&gt; &lt;int&gt; &lt;int&gt; &lt;int&gt; &lt;int&gt; &lt;int&gt;\n 1 alforan01   2022     1 PIT    NL        2     4     0     1     0     0     0\n 2 alldrca01   2022     1 PIT    NL        1     0     0     0     0     0     0\n 3 allengr01   2022     1 PIT    NL       46   118    17    22     4     0     2\n 4 andujmi01   2022     2 PIT    NL        9    36     4     9     3     1     0\n 5 baeji01     2022     1 PIT    NL       10    33     5    11     3     0     0\n 6 bandaan01   2022     1 PIT    NL       23     0     0     0     0     0     0\n 7 banuema01   2022     2 PIT    NL       31     0     0     0     0     0     0\n 8 bednada01   2022     1 PIT    NL       45     0     0     0     0     0     0\n 9 beedety01   2022     2 PIT    NL       25     0     0     0     0     0     0\n10 briceau01   2022     1 PIT    NL        4     0     0     0     0     0     0\n# ℹ 58 more rows\n# ℹ 10 more variables: RBI &lt;int&gt;, SB &lt;int&gt;, CS &lt;int&gt;, BB &lt;int&gt;, SO &lt;int&gt;,\n#   IBB &lt;int&gt;, HBP &lt;int&gt;, SH &lt;int&gt;, SF &lt;int&gt;, GIDP &lt;int&gt;"
  },
  {
    "objectID": "lectures/01-explore.html#logical-conditions",
    "href": "lectures/01-explore.html#logical-conditions",
    "title": "Exploring data: into the tidyverse",
    "section": "Logical conditions",
    "text": "Logical conditions\n\n\n\nx &lt; y: less than\nx &lt;= y: less than or equal to\nx == y: equal to\nx != y: not equal to\nx &gt; y: greater than\nx &gt;= y: greater than or equal to\n\n\n\nx %in% y: whether the value is present in a given vector\nis.na(x): is missing\n!is.na(x): is not missing\nx & y: and\nx | y: or\n!x: not\n\n\n\n… and basically anything that returns a TRUE/FALSE value"
  },
  {
    "objectID": "lectures/01-explore.html#common-mistakes",
    "href": "lectures/01-explore.html#common-mistakes",
    "title": "Exploring data: into the tidyverse",
    "section": "Common mistakes",
    "text": "Common mistakes\n\n\n\n= instead of ==\n\nnay\n\nfilter(Batting, team = \"PIT\")\n\nyay\n\nfilter(Batting, team == \"PIT\")\n\n\n\nForgetting quotes (for string/character)\n\nnay\n\nfilter(Batting, team == PIT)\n\nyay\n\nfilter(Batting, team == \"PIT\")"
  },
  {
    "objectID": "lectures/01-explore.html#select",
    "href": "lectures/01-explore.html#select",
    "title": "Exploring data: into the tidyverse",
    "section": "select()",
    "text": "select()\n\nUse select() to extract COLUMNS (variables) of interest\nJust simply specify the column names…\n\n\nselect(Batting, playerID, yearID, G, AB, R, H, HR, BB)\n\n# A tibble: 112,184 × 8\n   playerID  yearID     G    AB     R     H    HR    BB\n   &lt;chr&gt;      &lt;int&gt; &lt;int&gt; &lt;int&gt; &lt;int&gt; &lt;int&gt; &lt;int&gt; &lt;int&gt;\n 1 abercda01   1871     1     4     0     0     0     0\n 2 addybo01    1871    25   118    30    32     0     4\n 3 allisar01   1871    29   137    28    40     0     2\n 4 allisdo01   1871    27   133    28    44     2     0\n 5 ansonca01   1871    25   120    29    39     0     2\n 6 armstbo01   1871    12    49     9    11     0     0\n 7 barkeal01   1871     1     4     0     1     0     1\n 8 barnero01   1871    31   157    66    63     0    13\n 9 barrebi01   1871     1     5     1     1     0     0\n10 barrofr01   1871    18    86    13    13     0     0\n# ℹ 112,174 more rows"
  },
  {
    "objectID": "lectures/01-explore.html#mutate",
    "href": "lectures/01-explore.html#mutate",
    "title": "Exploring data: into the tidyverse",
    "section": "mutate()",
    "text": "mutate()\n\nUse mutate() to create new variables\nNew variables created via mutate() are usually based on existing variables\n\nMake sure to give your new variable a name\nNote that naming the new variable the same as the existing variable will overwrite the original column"
  },
  {
    "objectID": "lectures/01-explore.html#mutate-1",
    "href": "lectures/01-explore.html#mutate-1",
    "title": "Exploring data: into the tidyverse",
    "section": "mutate()",
    "text": "mutate()\nExample: Get the batting average and strikeout-to-walk ratio for every player\n\nmutate(Batting, batting_avg = H / AB, so_bb_ratio = SO / BB)\n\n# A tibble: 112,184 × 24\n   playerID  yearID stint teamID lgID      G    AB     R     H   X2B   X3B    HR\n   &lt;chr&gt;      &lt;int&gt; &lt;int&gt; &lt;fct&gt;  &lt;fct&gt; &lt;int&gt; &lt;int&gt; &lt;int&gt; &lt;int&gt; &lt;int&gt; &lt;int&gt; &lt;int&gt;\n 1 abercda01   1871     1 TRO    NA        1     4     0     0     0     0     0\n 2 addybo01    1871     1 RC1    NA       25   118    30    32     6     0     0\n 3 allisar01   1871     1 CL1    NA       29   137    28    40     4     5     0\n 4 allisdo01   1871     1 WS3    NA       27   133    28    44    10     2     2\n 5 ansonca01   1871     1 RC1    NA       25   120    29    39    11     3     0\n 6 armstbo01   1871     1 FW1    NA       12    49     9    11     2     1     0\n 7 barkeal01   1871     1 RC1    NA        1     4     0     1     0     0     0\n 8 barnero01   1871     1 BS1    NA       31   157    66    63    10     9     0\n 9 barrebi01   1871     1 FW1    NA        1     5     1     1     1     0     0\n10 barrofr01   1871     1 BS1    NA       18    86    13    13     2     1     0\n# ℹ 112,174 more rows\n# ℹ 12 more variables: RBI &lt;int&gt;, SB &lt;int&gt;, CS &lt;int&gt;, BB &lt;int&gt;, SO &lt;int&gt;,\n#   IBB &lt;int&gt;, HBP &lt;int&gt;, SH &lt;int&gt;, SF &lt;int&gt;, GIDP &lt;int&gt;, batting_avg &lt;dbl&gt;,\n#   so_bb_ratio &lt;dbl&gt;"
  },
  {
    "objectID": "lectures/01-explore.html#arrange",
    "href": "lectures/01-explore.html#arrange",
    "title": "Exploring data: into the tidyverse",
    "section": "arrange()",
    "text": "arrange()\n\nSort observations (rows) by variables (columns)\n\nascending order is the default (low to high for numeric columns, alphabetical order for character columns)\n\n\n\nExample: Who holds the single-season home run record?\n\narrange(Batting, desc(HR)) # desc() for descending order\n\n# A tibble: 112,184 × 22\n   playerID  yearID stint teamID lgID      G    AB     R     H   X2B   X3B    HR\n   &lt;chr&gt;      &lt;int&gt; &lt;int&gt; &lt;fct&gt;  &lt;fct&gt; &lt;int&gt; &lt;int&gt; &lt;int&gt; &lt;int&gt; &lt;int&gt; &lt;int&gt; &lt;int&gt;\n 1 bondsba01   2001     1 SFN    NL      153   476   129   156    32     2    73\n 2 mcgwima01   1998     1 SLN    NL      155   509   130   152    21     0    70\n 3 sosasa01    1998     1 CHN    NL      159   643   134   198    20     0    66\n 4 mcgwima01   1999     1 SLN    NL      153   521   118   145    21     1    65\n 5 sosasa01    2001     1 CHN    NL      160   577   146   189    34     5    64\n 6 sosasa01    1999     1 CHN    NL      162   625   114   180    24     2    63\n 7 judgeaa01   2022     1 NYA    AL      157   570   133   177    28     0    62\n 8 marisro01   1961     1 NYA    AL      161   590   132   159    16     4    61\n 9 ruthba01    1927     1 NYA    AL      151   540   158   192    29     8    60\n10 ruthba01    1921     1 NYA    AL      152   540   177   204    44    16    59\n# ℹ 112,174 more rows\n# ℹ 10 more variables: RBI &lt;int&gt;, SB &lt;int&gt;, CS &lt;int&gt;, BB &lt;int&gt;, SO &lt;int&gt;,\n#   IBB &lt;int&gt;, HBP &lt;int&gt;, SH &lt;int&gt;, SF &lt;int&gt;, GIDP &lt;int&gt;"
  },
  {
    "objectID": "lectures/01-explore.html#arrange-1",
    "href": "lectures/01-explore.html#arrange-1",
    "title": "Exploring data: into the tidyverse",
    "section": "arrange()",
    "text": "arrange()\nExample: arrange by multiple columns — at bats from high to low (first sort), then home runs from low to high (second sort) — variable order matters\n\narrange(Batting, desc(AB), HR)\n\n# A tibble: 112,184 × 22\n   playerID  yearID stint teamID lgID      G    AB     R     H   X2B   X3B    HR\n   &lt;chr&gt;      &lt;int&gt; &lt;int&gt; &lt;fct&gt;  &lt;fct&gt; &lt;int&gt; &lt;int&gt; &lt;int&gt; &lt;int&gt; &lt;int&gt; &lt;int&gt; &lt;int&gt;\n 1 rolliji01   2007     1 PHI    NL      162   716   139   212    38    20    30\n 2 wilsowi02   1980     1 KCA    AL      161   705   133   230    28    15     3\n 3 suzukic01   2004     1 SEA    AL      161   704   101   262    24     5     8\n 4 samueju01   1984     1 PHI    NL      160   701   105   191    36    19    15\n 5 pierrju01   2006     1 CHN    NL      162   699    87   204    32    13     3\n 6 cashda01    1975     1 PHI    NL      162   699   111   213    40     3     4\n 7 alouma01    1969     1 PIT    NL      162   698   105   231    41     6     1\n 8 reyesjo01   2005     1 NYN    NL      161   696    99   190    24    17     7\n 9 jensewo01   1936     1 PIT    NL      153   696    98   197    34    10    10\n10 soriaal01   2002     1 NYA    AL      156   696   128   209    51     2    39\n# ℹ 112,174 more rows\n# ℹ 10 more variables: RBI &lt;int&gt;, SB &lt;int&gt;, CS &lt;int&gt;, BB &lt;int&gt;, SO &lt;int&gt;,\n#   IBB &lt;int&gt;, HBP &lt;int&gt;, SH &lt;int&gt;, SF &lt;int&gt;, GIDP &lt;int&gt;"
  },
  {
    "objectID": "lectures/01-explore.html#performing-multiple-operations",
    "href": "lectures/01-explore.html#performing-multiple-operations",
    "title": "Exploring data: into the tidyverse",
    "section": "Performing multiple operations",
    "text": "Performing multiple operations\n\nWhat if we want to perform several different tasks using multiple dplyr verbs?\n\n\n\nIntroducing the pipe operator |&gt;\n\n\n\n\nYou might have seen the magrittr pipe %&gt;%…\n\n…from the maggritr package, automatically loaded when loading tidyverse\n\nRecently, many people (including Hadley Wickham) have switched to |&gt;, the built-in “native” pipe to base R\n\nWhat are the differences?\n\n\n\n\n\nShortcut for the pipe operator in RStudio: Command (or Ctrl) + Shift + M"
  },
  {
    "objectID": "lectures/01-explore.html#the-pipe-operator",
    "href": "lectures/01-explore.html#the-pipe-operator",
    "title": "Exploring data: into the tidyverse",
    "section": "The pipe operator",
    "text": "The pipe operator\n\nUse |&gt; to perform a sequence of operations\nThe pipe takes an object (e.g., tibble, data frame, matrix, vector, etc.) on the left and passes it as the first argument of the function on the right\n\n\n# the workflow\nobject |&gt;\n  first_operation(...) |&gt;\n  second_operation(...) |&gt; \n  .\n  .\n  .\n  last_operation(...)"
  },
  {
    "objectID": "lectures/01-explore.html#performing-multiple-operations-1",
    "href": "lectures/01-explore.html#performing-multiple-operations-1",
    "title": "Exploring data: into the tidyverse",
    "section": "Performing multiple operations",
    "text": "Performing multiple operations\nExample: Which Pirates players had the highest batting average in 2022, among those with at least 50 at bats?\n\nWhat are the tasks to be done here?\n\n\n\nfilter(): only Pirates players in 2022 with at least 50 at bats\nmutate(): create a new column for batting average\narrange(): sort by batting average in descending order\nselect(): report player name, at bats, and batting average"
  },
  {
    "objectID": "lectures/01-explore.html#performing-multiple-operations-2",
    "href": "lectures/01-explore.html#performing-multiple-operations-2",
    "title": "Exploring data: into the tidyverse",
    "section": "Performing multiple operations",
    "text": "Performing multiple operations\n\nBatting |&gt; \n  filter(yearID == 2022, teamID == \"PIT\", AB &gt;= 50) |&gt; \n  mutate(batting_avg = H / AB) |&gt; \n  arrange(desc(batting_avg)) |&gt; \n  select(playerID, AB, batting_avg)\n\n# A tibble: 23 × 3\n   playerID     AB batting_avg\n   &lt;chr&gt;     &lt;int&gt;       &lt;dbl&gt;\n 1 newmake01   288       0.274\n 2 reynobr01   542       0.262\n 3 hayeske01   505       0.244\n 4 marisja01    77       0.234\n 5 perezro02    60       0.233\n 6 castrro01   253       0.233\n 7 cruzon01    331       0.233\n 8 gamelbe01   371       0.232\n 9 chavimi01   401       0.229\n10 vogelda01   237       0.228\n# ℹ 13 more rows\n\n\n\nWithout the pipe, the code looks every ugly with functions nested within functions…\n\nselect(arrange(mutate(filter(Batting, yearID == 2022, teamID == \"PIT\", AB &gt;= 50), batting_avg = H / AB), \ndesc(batting_avg)), playerID, AB, batting_avg)"
  },
  {
    "objectID": "lectures/01-explore.html#summarize-by-itself",
    "href": "lectures/01-explore.html#summarize-by-itself",
    "title": "Exploring data: into the tidyverse",
    "section": "summarize() (by itself)",
    "text": "summarize() (by itself)\n\nUse summarize() to collapse the data down to a single row (per group) by aggregating variables into single values\nUseful for computing summaries (e.g., mean, median, max, min, correlation, etc.)\n\n\nBatting |&gt; \n  summarize(median_at_bats = median(AB))\n\n# A tibble: 1 × 1\n  median_at_bats\n           &lt;dbl&gt;\n1             45\n\nBatting |&gt; \n  summarize(cor_ab_hr = cor(AB, HR))\n\n# A tibble: 1 × 1\n  cor_ab_hr\n      &lt;dbl&gt;\n1     0.704"
  },
  {
    "objectID": "lectures/01-explore.html#group_by-and-summarize",
    "href": "lectures/01-explore.html#group_by-and-summarize",
    "title": "Exploring data: into the tidyverse",
    "section": "group_by() and summarize()",
    "text": "group_by() and summarize()\n\ngroup_by() converts the data into a “grouped tbl” where operations are performed by group\n\ni.e., it splits the data into groups based on values in a column\n\ngroup_by() becomes powerful when combining with summarize()\nAfter the operation at the group-level is done, use ungroup() to remove grouping"
  },
  {
    "objectID": "lectures/01-explore.html#group_by-and-summarize-1",
    "href": "lectures/01-explore.html#group_by-and-summarize-1",
    "title": "Exploring data: into the tidyverse",
    "section": "group_by() and summarize()",
    "text": "group_by() and summarize()\nExample: How many home runs, strike outs, and walks did each team accumulate in each season from 2015 to 2019?\n\nBatting |&gt; \n  filter(yearID %in% 2015:2019) |&gt; \n  group_by(teamID) |&gt; \n  summarize(total_hr = sum(HR), total_so = sum(SO), total_bb = sum(BB)) |&gt; \n  arrange(desc(total_hr))\n\n# A tibble: 30 × 4\n   teamID total_hr total_so total_bb\n   &lt;fct&gt;     &lt;int&gt;    &lt;int&gt;    &lt;int&gt;\n 1 NYA        1209     6659     2839\n 2 HOU        1159     6294     2759\n 3 TOR        1139     6741     2752\n 4 LAN        1111     6751     2991\n 5 BAL        1103     6914     2162\n 6 TEX        1041     7008     2572\n 7 SEA        1036     6693     2489\n 8 MIN        1035     6694     2604\n 9 OAK        1033     6474     2610\n10 MIL        1031     7434     2724\n# ℹ 20 more rows"
  },
  {
    "objectID": "lectures/01-explore.html#count",
    "href": "lectures/01-explore.html#count",
    "title": "Exploring data: into the tidyverse",
    "section": "count()",
    "text": "count()\n\n\ncount() returns the number of observations in each group\n\nBatting |&gt; \n  count(lgID, name = \"freq\")\n\n# A tibble: 7 × 2\n  lgID   freq\n  &lt;fct&gt; &lt;int&gt;\n1 AA     1893\n2 AL    51799\n3 FL      472\n4 NA      737\n5 NL    56800\n6 PL      149\n7 UA      334\n\n\n\n# recall that in base R...\ntable(Batting$lgID)\n\n\n   AA    AL    FL    NA    NL    PL    UA \n 1893 51799   472   737 56800   149   334 \n\n\n\nThis can also be done with group_by() and summarize()\n\n# note: count is a \"shortcut\" of this\nBatting |&gt; \n  group_by(lgID) |&gt; \n  summarize(freq = n()) |&gt; \n  ungroup()\n\n# A tibble: 7 × 2\n  lgID   freq\n  &lt;fct&gt; &lt;int&gt;\n1 AA     1893\n2 AL    51799\n3 FL      472\n4 NA      737\n5 NL    56800\n6 PL      149\n7 UA      334"
  },
  {
    "objectID": "lectures/01-explore.html#slice_-family-for-subsetting-rows",
    "href": "lectures/01-explore.html#slice_-family-for-subsetting-rows",
    "title": "Exploring data: into the tidyverse",
    "section": "slice_*() family for subsetting rows",
    "text": "slice_*() family for subsetting rows\n\nslice(): extract rows (observations) based on the row index\n\n\nBatting |&gt; \n  slice(c(1, 99, 101, 500))\n\n# A tibble: 4 × 22\n  playerID  yearID stint teamID lgID      G    AB     R     H   X2B   X3B    HR\n  &lt;chr&gt;      &lt;int&gt; &lt;int&gt; &lt;fct&gt;  &lt;fct&gt; &lt;int&gt; &lt;int&gt; &lt;int&gt; &lt;int&gt; &lt;int&gt; &lt;int&gt; &lt;int&gt;\n1 abercda01   1871     1 TRO    NA        1     4     0     0     0     0     0\n2 spaldal01   1871     1 BS1    NA       31   144    43    39    10     1     1\n3 stearbi01   1871     1 WS3    NA        2     9     1     0     0     0     0\n4 smilebi01   1874     1 BL1    NA        2     7     0     0     0     0     0\n# ℹ 10 more variables: RBI &lt;int&gt;, SB &lt;int&gt;, CS &lt;int&gt;, BB &lt;int&gt;, SO &lt;int&gt;,\n#   IBB &lt;int&gt;, HBP &lt;int&gt;, SH &lt;int&gt;, SF &lt;int&gt;, GIDP &lt;int&gt;\n\n\n\n\nslice_head() / slice_tail(): extract the first / last n rows\n\n\n# Batting |&gt; slice_tail(n = 5)\nBatting |&gt; \n  slice_head(n = 5)\n\n# A tibble: 5 × 22\n  playerID  yearID stint teamID lgID      G    AB     R     H   X2B   X3B    HR\n  &lt;chr&gt;      &lt;int&gt; &lt;int&gt; &lt;fct&gt;  &lt;fct&gt; &lt;int&gt; &lt;int&gt; &lt;int&gt; &lt;int&gt; &lt;int&gt; &lt;int&gt; &lt;int&gt;\n1 abercda01   1871     1 TRO    NA        1     4     0     0     0     0     0\n2 addybo01    1871     1 RC1    NA       25   118    30    32     6     0     0\n3 allisar01   1871     1 CL1    NA       29   137    28    40     4     5     0\n4 allisdo01   1871     1 WS3    NA       27   133    28    44    10     2     2\n5 ansonca01   1871     1 RC1    NA       25   120    29    39    11     3     0\n# ℹ 10 more variables: RBI &lt;int&gt;, SB &lt;int&gt;, CS &lt;int&gt;, BB &lt;int&gt;, SO &lt;int&gt;,\n#   IBB &lt;int&gt;, HBP &lt;int&gt;, SH &lt;int&gt;, SF &lt;int&gt;, GIDP &lt;int&gt;"
  },
  {
    "objectID": "lectures/01-explore.html#slice_-family-for-subsetting-rows-1",
    "href": "lectures/01-explore.html#slice_-family-for-subsetting-rows-1",
    "title": "Exploring data: into the tidyverse",
    "section": "slice_*() family for subsetting rows",
    "text": "slice_*() family for subsetting rows\n\nslice_min() / slice_max(): extract rows with the smallest or largest values of a variable\n\n\n# single-season home run record (top 5)\nBatting |&gt; \n  slice_max(HR, n = 5)\n\n# A tibble: 5 × 22\n  playerID  yearID stint teamID lgID      G    AB     R     H   X2B   X3B    HR\n  &lt;chr&gt;      &lt;int&gt; &lt;int&gt; &lt;fct&gt;  &lt;fct&gt; &lt;int&gt; &lt;int&gt; &lt;int&gt; &lt;int&gt; &lt;int&gt; &lt;int&gt; &lt;int&gt;\n1 bondsba01   2001     1 SFN    NL      153   476   129   156    32     2    73\n2 mcgwima01   1998     1 SLN    NL      155   509   130   152    21     0    70\n3 sosasa01    1998     1 CHN    NL      159   643   134   198    20     0    66\n4 mcgwima01   1999     1 SLN    NL      153   521   118   145    21     1    65\n5 sosasa01    2001     1 CHN    NL      160   577   146   189    34     5    64\n# ℹ 10 more variables: RBI &lt;int&gt;, SB &lt;int&gt;, CS &lt;int&gt;, BB &lt;int&gt;, SO &lt;int&gt;,\n#   IBB &lt;int&gt;, HBP &lt;int&gt;, SH &lt;int&gt;, SF &lt;int&gt;, GIDP &lt;int&gt;\n\n\n\n\nslice_sample(): randomly sample a specified number / fraction of observation in the data\n\nUseful for performing resampling (e.g., bootstrap, cross-validation, etc.)\n\n# randomly sample 1000 rows (without replacement, by default)\nBatting |&gt; \n  slice_sample(n = 1000)\n\n# randomly sample 70% of the rows, with replacement\nBatting |&gt; \n  slice_sample(prop = 0.7, replace = TRUE)"
  },
  {
    "objectID": "lectures/01-explore.html#putting-it-all-together",
    "href": "lectures/01-explore.html#putting-it-all-together",
    "title": "Exploring data: into the tidyverse",
    "section": "Putting it all together",
    "text": "Putting it all together\nExample: Get batting stats for each year: each row is a year with the following variables\n\ntotal hits, home runs, strikeouts, walks, atbats\ntotal batting average for each year = total H / total AB\nonly keeps AL and NL leagues\n\n\n\nyearly_batting &lt;- Batting |&gt;\n  filter(lgID %in% c(\"AL\", \"NL\")) |&gt;\n  group_by(yearID) |&gt;\n  summarize(total_h = sum(H, na.rm = TRUE),\n            total_hr = sum(HR, na.rm = TRUE),\n            total_so = sum(SO, na.rm = TRUE),\n            total_bb = sum(BB, na.rm = TRUE),\n            total_ab = sum(AB, na.rm = TRUE)) |&gt;\n  mutate(batting_avg = total_h / total_ab)"
  },
  {
    "objectID": "lectures/01-explore.html#putting-it-all-together-1",
    "href": "lectures/01-explore.html#putting-it-all-together-1",
    "title": "Exploring data: into the tidyverse",
    "section": "Putting it all together",
    "text": "Putting it all together\nWhat are the top three years with the most HRs?\n\n\nyearly_batting |&gt; \n  slice_max(total_hr, n = 3)\n\n# A tibble: 3 × 7\n  yearID total_h total_hr total_so total_bb total_ab batting_avg\n   &lt;int&gt;   &lt;int&gt;    &lt;int&gt;    &lt;int&gt;    &lt;int&gt;    &lt;int&gt;       &lt;dbl&gt;\n1   2019   42039     6776    42823    15895   166651       0.252\n2   2017   42215     6105    40104    15829   165567       0.255\n3   2021   39484     5944    42145    15794   161941       0.244\n\n\n\n# or this \nyearly_batting |&gt;\n  arrange(desc(total_hr)) |&gt;\n  slice(1:3)\n\n# A tibble: 3 × 7\n  yearID total_h total_hr total_so total_bb total_ab batting_avg\n   &lt;int&gt;   &lt;int&gt;    &lt;int&gt;    &lt;int&gt;    &lt;int&gt;    &lt;int&gt;       &lt;dbl&gt;\n1   2019   42039     6776    42823    15895   166651       0.252\n2   2017   42215     6105    40104    15829   165567       0.255\n3   2021   39484     5944    42145    15794   161941       0.244"
  },
  {
    "objectID": "lectures/01-explore.html#putting-it-all-together-2",
    "href": "lectures/01-explore.html#putting-it-all-together-2",
    "title": "Exploring data: into the tidyverse",
    "section": "Putting it all together",
    "text": "Putting it all together\nWhich years have the best and worst strikeout to walk ratios?\n\n\nyearly_batting |&gt;\n  mutate(so_bb_ratio = total_so / total_bb) |&gt;\n  arrange(so_bb_ratio) |&gt;\n  slice(c(1, n()))\n\n# A tibble: 2 × 8\n  yearID total_h total_hr total_so total_bb total_ab batting_avg so_bb_ratio\n   &lt;int&gt;   &lt;int&gt;    &lt;int&gt;    &lt;int&gt;    &lt;int&gt;    &lt;int&gt;       &lt;dbl&gt;       &lt;dbl&gt;\n1   1893   15913      460     3341     6143    56898       0.280       0.544\n2   1879    6171       58     1843      508    24155       0.255       3.63"
  },
  {
    "objectID": "lectures/01-explore.html#whats-next",
    "href": "lectures/01-explore.html#whats-next",
    "title": "Exploring data: into the tidyverse",
    "section": "What’s next?",
    "text": "What’s next?\n\n\nDATA VISUALIZATION\n\nThe simple graph has brought more information to the data analyst’s mind than any other device. — John Tukey\n\n\nUse ggplot2 (and the grammar of graphics) to visually explore data\nMore intuitive than base R plotting\nDifferent types of visualizations for categorical and quantitative data, faceting, etc.\ndplyr verbs and |&gt; leads to natural pipeline for EDA"
  },
  {
    "objectID": "lectures/01-explore.html#check-out-this-song",
    "href": "lectures/01-explore.html#check-out-this-song",
    "title": "Exploring data: into the tidyverse",
    "section": "Check out this song",
    "text": "Check out this song\n\n\n\n36-SURE.github.io"
  },
  {
    "objectID": "lectures/17-mixture.html#previously-clustering",
    "href": "lectures/17-mixture.html#previously-clustering",
    "title": "Unsupervised learning: Gaussian mixture models",
    "section": "Previously: clustering",
    "text": "Previously: clustering\n\nPrevious methods: \\(k\\)-means clustering  and hierarchical clustering\nOutput hard assignments, strictly assigning observations to only one cluster\n\n\n\nWhat about soft assignments and uncertainty in the clustering results?\n\n\n\n\nEntering mixture models"
  },
  {
    "objectID": "lectures/17-mixture.html#previously-kernel-density-estimation",
    "href": "lectures/17-mixture.html#previously-kernel-density-estimation",
    "title": "Unsupervised learning: Gaussian mixture models",
    "section": "Previously: kernel density estimation",
    "text": "Previously: kernel density estimation\nKernel density estimator: \\(\\quad \\displaystyle \\hat{f}(x) = \\frac{1}{n} \\sum_{i=1}^n \\frac{1}{h} K_h(x - x_i)\\)\n\nWe have to use every observation when estimating the density for new points\n\n\n\n\n\n\n\n\n\n\n\nInstead we can make some simplifying assumptions"
  },
  {
    "objectID": "lectures/17-mixture.html#mixture-models",
    "href": "lectures/17-mixture.html#mixture-models",
    "title": "Unsupervised learning: Gaussian mixture models",
    "section": "Mixture models",
    "text": "Mixture models\nAssume \\(f(x)\\) is a mixture of \\(K\\) component distributions \\(\\displaystyle f(x) = \\sum_{k=1}^K \\pi_k f_k(x)\\)\n\nwhere \\(\\pi_k =\\) mixing proportions (or weights), with \\(\\pi_k &gt; 0\\), and \\(\\sum_k \\pi_k = 1\\)\n\n\nThis is a data generating process, meaning to generate a new point:\n\npick a distribution/component among our \\(K\\) options by introducing a new variable \\(z \\sim \\text{Multinomial} (\\pi_1, \\pi_2, \\dots, \\pi_k)\\) (categorical variable for which group the new point is from)\n\n\n\n\ngenerate an observation with that distribution / component, i.e. \\(x \\mid z \\sim f_{z}\\)\n\n\n\nSo what do we use for each \\(f_k\\)?"
  },
  {
    "objectID": "lectures/17-mixture.html#gaussian-mixture-models-gmms",
    "href": "lectures/17-mixture.html#gaussian-mixture-models-gmms",
    "title": "Unsupervised learning: Gaussian mixture models",
    "section": "Gaussian mixture models (GMMs)",
    "text": "Gaussian mixture models (GMMs)\nAssume a parametric mixture model, with parameters \\(\\theta_k\\) for the \\(k\\)th component \\[f(x) = \\sum_{k=1}^K \\pi_k f_k(x; \\theta_k)\\]\n\nAssume each component is Gaussian/normal, where the 1D case is\n\\[f_k(x; \\theta_k) = N(x; \\mu_k, \\sigma_k^2) = \\frac{1}{\\sqrt{2\\pi \\sigma_k^2}}\\text{exp} \\Big( -\\frac{(x - \\mu_k)^2}{2 \\sigma_k^2} \\Big)\\]\n\n\nWe need to estimate each \\(\\pi_1, \\dots, \\pi_k\\), \\(\\mu_1, \\dots, \\mu_k\\), \\(\\sigma_1, \\dots, \\sigma_k\\)"
  },
  {
    "objectID": "lectures/17-mixture.html#lets-pretend-we-only-have-one-component",
    "href": "lectures/17-mixture.html#lets-pretend-we-only-have-one-component",
    "title": "Unsupervised learning: Gaussian mixture models",
    "section": "Let’s pretend we only have one component…",
    "text": "Let’s pretend we only have one component…\nGiven \\(n\\) observations from a single Normal distribution, we estimate the distribution parameters using the likelihood function, the probability/density of observing the data given the parameters\n\\[\\mathcal{L}(\\mu, \\sigma \\mid x_1, \\dots, x_n) = f( x_1, \\dots, x_n \\mid \\mu, \\sigma) =  \\prod_i^n \\frac{1}{\\sqrt{2\\pi \\sigma^2}}\\text{exp } \\left\\{-\\frac{(x_i - \\mu)^2}{2 \\sigma^2}\\right\\}\\]\n\nWe can compute the maximum likelihood estimates (MLEs) for \\(\\mu\\) and \\(\\sigma\\)\nYou already know these values!\n\n\\(\\displaystyle \\hat{\\mu}_{\\rm MLE} = \\frac{1}{n} \\sum_i^n x_i\\), sample mean\n\\(\\displaystyle \\hat{\\sigma}_{\\rm MLE} = \\sqrt{\\frac{1}{n}\\sum_i^n (x_i - \\mu)^2}\\), sample standard deviation (plug in \\(\\hat{\\mu}_{\\rm MLE}\\))"
  },
  {
    "objectID": "lectures/17-mixture.html#the-problem-with-more-than-one-component",
    "href": "lectures/17-mixture.html#the-problem-with-more-than-one-component",
    "title": "Unsupervised learning: Gaussian mixture models",
    "section": "The problem with more than one component",
    "text": "The problem with more than one component\n\n\n\nWe don’t know which component an observation belongs to\nIF WE DID KNOW, then we could compute each component’s MLEs as before\nBut we don’t know because \\(z\\) is a latent variable! So what about its distribution given the data?\n\n\\(\\displaystyle P(z_i = k \\mid x_i) = \\frac{P(x_i \\mid z_i = k) P(z_i = k)}{P(x_i)}\\)\n\\(\\displaystyle =\\frac{\\pi_{k} N\\left(\\mu_{k}, \\sigma_{k}^{2}\\right)}{\\sum_{k=1}^{K} \\pi_{k} N\\left(\\mu_{k}, \\sigma_{k}\\right)}\\)\n\nBut we do NOT know these parameters!"
  },
  {
    "objectID": "lectures/17-mixture.html#expectation-maximization-em-algorithm",
    "href": "lectures/17-mixture.html#expectation-maximization-em-algorithm",
    "title": "Unsupervised learning: Gaussian mixture models",
    "section": "Expectation-maximization (EM) algorithm",
    "text": "Expectation-maximization (EM) algorithm\nAlternate between the following:\n\npretend to know the probability each observation belongs to each group, to estimate the parameters of the components\n\n\n\npretend to know the parameters of the components, to estimate the probability each observation belong to each group\n\n\n\nWhere have we seen this before?\n\n\n\\(k\\)-means algorithm!\n\n\n\nStart with initial guesses about \\(\\pi_1, \\dots, \\pi_k\\), \\(\\mu_1, \\dots, \\mu_k\\), \\(\\sigma_1, \\dots, \\sigma_k\\)\nRepeat until nothing changes:\n\n\n\n\nExpectation step: calculate \\(\\hat{z}_{ik}\\) = expected membership of observation \\(i\\) in cluster \\(k\\)\nMaximization step: update parameter estimates with weighted MLE using \\(\\hat{z}_{ik}\\)"
  },
  {
    "objectID": "lectures/17-mixture.html#how-does-this-relate-back-to-clustering",
    "href": "lectures/17-mixture.html#how-does-this-relate-back-to-clustering",
    "title": "Unsupervised learning: Gaussian mixture models",
    "section": "How does this relate back to clustering?",
    "text": "How does this relate back to clustering?\n\nFrom the EM algorithm: \\(\\hat{z}_{ik}\\) is a soft membership of observation \\(i\\) in cluster \\(k\\)\n\n\n\nassign observation \\(i\\) to a cluster with the largest \\(\\hat{z}_{ik}\\)\nmeasure cluster assignment uncertainty of \\(\\displaystyle 1 - \\max_k \\hat{z}_{ik}\\)\n\n\n\nOur parameters determine the type of clusters\n\n\nIn the 1D case, there are two options:\n\n\n\neach cluster is assumed to have equal variance (spread): \\(\\sigma_1^2 = \\sigma_2^2 = \\dots = \\sigma_k^2\\)\n\n\n\n\neach cluster is allowed to have a different variance\n\n\n\nBut that is only 1D… what happens in multiple dimensions?"
  },
  {
    "objectID": "lectures/17-mixture.html#multivariate-gmms",
    "href": "lectures/17-mixture.html#multivariate-gmms",
    "title": "Unsupervised learning: Gaussian mixture models",
    "section": "Multivariate GMMs",
    "text": "Multivariate GMMs\n\\[f(x) = \\sum_{k=1}^K \\pi_k f_k(x; \\theta_k) \\qquad \\text{where }f_k(x; \\theta_k) \\sim N(\\boldsymbol{\\mu}_k, \\boldsymbol{\\Sigma}_k)\\]\nEach component is a multivariate normal distribution:\n\n\n\\(\\boldsymbol{\\mu}_k\\) is a vector of means in \\(p\\) dimensions\n\n\n\n\n\\(\\boldsymbol{\\Sigma}_k\\) is the \\(p \\times p\\) covariance matrix - describes the joint variability between pairs of variables\n\n\\[\\sum=\\left[\\begin{array}{cccc}\n\\sigma_{1}^{2} & \\sigma_{1,2} & \\cdots & \\sigma_{1, p} \\\\\n\\sigma_{2,1} & \\sigma_{2}^{2} & \\cdots & \\sigma_{2, p} \\\\\n\\vdots & \\vdots & \\ddots & \\vdots \\\\\n\\sigma_{p, 1} & \\sigma_{p, 2}^{2} & \\cdots & \\sigma_{p}^{2}\n\\end{array}\\right]\\]"
  },
  {
    "objectID": "lectures/17-mixture.html#covariance-constraints",
    "href": "lectures/17-mixture.html#covariance-constraints",
    "title": "Unsupervised learning: Gaussian mixture models",
    "section": "Covariance constraints",
    "text": "Covariance constraints\n\\[\\sum=\\left[\\begin{array}{cccc}\n\\sigma_{1}^{2} & \\sigma_{1,2} & \\cdots & \\sigma_{1, p} \\\\\n\\sigma_{2,1} & \\sigma_{2}^{2} & \\cdots & \\sigma_{2, p} \\\\\n\\vdots & \\vdots & \\ddots & \\vdots \\\\\n\\sigma_{p, 1} & \\sigma_{p, 2}^{2} & \\cdots & \\sigma_{p}^{2}\n\\end{array}\\right]\\]\n\nAs the number of dimensions increases, model fitting and estimation become increasingly difficult\n\n\nWe can use constraints on multiple aspects of the \\(k\\) covariance matrices:\n\n\n\nvolume: size of the clusters, i.e., number of observations,\nshape: direction of variance, i.e. which variables display more variance\norientation: aligned with axes (low covariance) versus tilted (due to relationships between variables)"
  },
  {
    "objectID": "lectures/17-mixture.html#covariance-constraints-1",
    "href": "lectures/17-mixture.html#covariance-constraints-1",
    "title": "Unsupervised learning: Gaussian mixture models",
    "section": "Covariance constraints",
    "text": "Covariance constraints\n\n\nControl volume, shape, orientation\nE: equal and V: variable (VVV is the most flexible, but has the most parameters)\nTwo II: spherical, one I: diagonal, the remaining are general\n\nHow do we know which one to choose?"
  },
  {
    "objectID": "lectures/17-mixture.html#bayesian-information-criterion-bic",
    "href": "lectures/17-mixture.html#bayesian-information-criterion-bic",
    "title": "Unsupervised learning: Gaussian mixture models",
    "section": "Bayesian information criterion (BIC)",
    "text": "Bayesian information criterion (BIC)\nThis is a statistical model\n\\[f(x) = \\sum_{k=1}^K \\pi_k f_k(x; \\theta_k) \\qquad \\text{where }f_k(x; \\theta_k) \\sim N(\\boldsymbol{\\mu}_k, \\boldsymbol{\\Sigma}_k)\\]\n\nUse a model selection procedure for determining which best characterizes the data\n\n\nSpecifically, use a penalized likelihood measure \\[\\text{BIC} = 2\\log \\mathcal{L} - m\\log n\\]\n\n\\(\\log \\mathcal{L}\\): log-likelihood of the considered model\nwith \\(m\\) parameters (VVV has the most parameters) and \\(n\\) observations\n\n\n\n\npenalizes large models with many clusters without constraints\nuse BIC to choose the covariance constraints AND number of clusters\n\n\n\n\nThe above \\(\\text{BIC}\\) is really the \\(- \\text{BIC}\\) of what you typically see, this sign flip is just for ease"
  },
  {
    "objectID": "lectures/17-mixture.html#data-nba-player-statistics-per-100-possessions",
    "href": "lectures/17-mixture.html#data-nba-player-statistics-per-100-possessions",
    "title": "Unsupervised learning: Gaussian mixture models",
    "section": "Data: NBA player statistics per 100 possessions",
    "text": "Data: NBA player statistics per 100 possessions\n\nlibrary(tidyverse)\ntheme_set(theme_light())\nlibrary(ballr)\nnba_stats &lt;- NBAPerGameStatisticsPer100Poss(season = 2024)\nnba_stats &lt;- nba_stats |&gt; \n  group_by(player) |&gt; \n  slice_max(g) |&gt; \n  ungroup() |&gt; \n  filter(mp &gt;= 150) # keep players with at least 150 minutes played\nhead(nba_stats)\n\n# A tibble: 6 × 33\n     rk player   pos     age tm        g    gs    mp    fg   fga fgpercent   x3p\n  &lt;dbl&gt; &lt;chr&gt;    &lt;chr&gt; &lt;dbl&gt; &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt;     &lt;dbl&gt; &lt;dbl&gt;\n1   183 A.J. Gr… SG       24 MIL      56     0   614   6.5  15.3     0.423   5.4\n2   290 A.J. La… SG       23 DAL      42     0   311   8.3  18.7     0.446   2  \n3   190 AJ Grif… SF       20 ATL      20     0   171   5    17.4     0.29    2.8\n4   178 Aaron G… PF       28 DEN      73    73  2297   8.6  15.5     0.556   0.9\n5   221 Aaron H… PG       27 HOU      78     1  1269   7.1  15.9     0.446   3.2\n6   372 Aaron N… SF       24 IND      72    47  1995   7.4  15       0.496   3.3\n# ℹ 21 more variables: x3pa &lt;dbl&gt;, x3ppercent &lt;dbl&gt;, x2p &lt;dbl&gt;, x2pa &lt;dbl&gt;,\n#   x2ppercent &lt;dbl&gt;, ft &lt;dbl&gt;, fta &lt;dbl&gt;, ftpercent &lt;dbl&gt;, orb &lt;dbl&gt;,\n#   drb &lt;dbl&gt;, trb &lt;dbl&gt;, ast &lt;dbl&gt;, stl &lt;dbl&gt;, blk &lt;dbl&gt;, tov &lt;dbl&gt;, pf &lt;dbl&gt;,\n#   pts &lt;dbl&gt;, x &lt;dbl&gt;, ortg &lt;dbl&gt;, drtg &lt;dbl&gt;, link &lt;chr&gt;"
  },
  {
    "objectID": "lectures/17-mixture.html#implementation-with-mclust-package",
    "href": "lectures/17-mixture.html#implementation-with-mclust-package",
    "title": "Unsupervised learning: Gaussian mixture models",
    "section": "Implementation with mclust package",
    "text": "Implementation with mclust package\nSelect the model and number of clusters\nUse Mclust() function to search over 1 to 9 clusters (default) and the different covariance constraints (i.e. models)\n\nlibrary(mclust)\n# x3pa: 3pt attempts per 100 possessions\n# trb: total rebounds per 100 possessions\nnba_mclust &lt;- nba_stats |&gt; \n  select(x3pa, trb) |&gt; \n  Mclust()\nsummary(nba_mclust)\n\n---------------------------------------------------- \nGaussian finite mixture model fitted by EM algorithm \n---------------------------------------------------- \n\nMclust VVV (ellipsoidal, varying volume, shape, and orientation) model with 3\ncomponents: \n\n log-likelihood   n df     BIC       ICL\n      -2246.434 452 17 -4596.8 -4749.605\n\nClustering table:\n  1   2   3 \n138  40 274"
  },
  {
    "objectID": "lectures/17-mixture.html#display-the-bic-for-each-model-and-number-of-clusters",
    "href": "lectures/17-mixture.html#display-the-bic-for-each-model-and-number-of-clusters",
    "title": "Unsupervised learning: Gaussian mixture models",
    "section": "Display the BIC for each model and number of clusters",
    "text": "Display the BIC for each model and number of clusters\n\n\n\nnba_mclust |&gt; \n  plot(what = \"BIC\", \n       legendArgs = list(x = \"bottomright\", ncol = 4))\n\n\n\n\n\n\n\n\n\n\nnba_mclust |&gt; \n  plot(what = \"classification\")"
  },
  {
    "objectID": "lectures/17-mixture.html#how-do-the-cluster-assignments-compare-to-the-positions",
    "href": "lectures/17-mixture.html#how-do-the-cluster-assignments-compare-to-the-positions",
    "title": "Unsupervised learning: Gaussian mixture models",
    "section": "How do the cluster assignments compare to the positions?",
    "text": "How do the cluster assignments compare to the positions?\nTwo-way table to compare the clustering assignments with player positions\n(What’s the way to visually compare the two labels?)\n\ntable(\"Clusters\" = nba_mclust$classification, \"Positions\" = nba_stats$pos)\n\n        Positions\nClusters  C C-PF PF PF-C PF-SF PG PG-SG SF SF-PF SF-SG SG SG-PG\n       1 43    1 56    1     0  4     0 24     1     0  8     0\n       2 34    2  3    0     0  1     0  0     0     0  0     0\n       3  3    0 30    0     1 77     4 68     1     1 88     1\n\n\nTakeaway: positions tend to fall within particular clusters"
  },
  {
    "objectID": "lectures/17-mixture.html#what-about-the-cluster-probabilities",
    "href": "lectures/17-mixture.html#what-about-the-cluster-probabilities",
    "title": "Unsupervised learning: Gaussian mixture models",
    "section": "What about the cluster probabilities?",
    "text": "What about the cluster probabilities?\n\nnba_player_probs &lt;- nba_mclust$z\ncolnames(nba_player_probs) &lt;- c(\"cluster1\", \"cluster2\", \"cluster3\")\n\nnba_player_probs &lt;- nba_player_probs |&gt;\n  as_tibble() |&gt;\n  mutate(player = nba_stats$player) |&gt;\n  pivot_longer(!player, names_to = \"cluster\", values_to = \"prob\")\n\nnba_player_probs |&gt;\n  ggplot(aes(prob)) +\n  geom_histogram() +\n  facet_wrap(~ cluster)"
  },
  {
    "objectID": "lectures/17-mixture.html#which-players-have-the-highest-uncertainty",
    "href": "lectures/17-mixture.html#which-players-have-the-highest-uncertainty",
    "title": "Unsupervised learning: Gaussian mixture models",
    "section": "Which players have the highest uncertainty?",
    "text": "Which players have the highest uncertainty?\n\n\n\n\nnba_stats |&gt;\n  mutate(cluster = nba_mclust$classification,\n         uncertainty = nba_mclust$uncertainty) |&gt; \n  group_by(cluster) |&gt;\n  slice_max(uncertainty, n = 5) |&gt; \n  mutate(player = fct_reorder(player, uncertainty)) |&gt; \n  ggplot(aes(x = uncertainty, y = player)) +\n  geom_point(size = 3) +\n  facet_wrap(~ cluster, scales = \"free_y\", nrow = 3)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n36-SURE.github.io"
  },
  {
    "objectID": "lectures/13-regularization.html#previously",
    "href": "lectures/13-regularization.html#previously",
    "title": "Supervised learning: variable selection",
    "section": "Previously…",
    "text": "Previously…\nVariable selection for a linear model  \\(\\qquad \\hat{Y} = \\hat{\\beta}_0 + \\hat{\\beta}_1 x_1 + \\cdots + \\hat{\\beta}_p x_p\\)\nWhy would we attempt to select a subset of the \\(p\\) variables?\n\n\nTo improve prediction accuracy\n\nEliminating uninformative predictors can lead to lower variance in the test-set MSE, at the expense of a slight increase in bias\n\n\n\n\n\nTo improve model interpretability\n\nEliminating uninformative predictors is obviously a good thing when your goal is to tell the story of how your predictors are associated with your response.\n\n\n\n\nRemember the bias-variance tradeoff \\(\\qquad {\\rm MSE} = {\\rm (Bias)}^2 + {\\rm Variance}\\)\n\nIntroduce bias but decrease variance to improve predictions"
  },
  {
    "objectID": "lectures/13-regularization.html#shrinkage-methods-ridge-regression",
    "href": "lectures/13-regularization.html#shrinkage-methods-ridge-regression",
    "title": "Supervised learning: variable selection",
    "section": "Shrinkage methods: Ridge regression",
    "text": "Shrinkage methods: Ridge regression\nRecall: linear regression estimates coefficients by minimizing:\n\\[\\text{RSS} = \\sum_i^n \\big(Y_i - \\beta_0 -  \\sum_j^p \\hat{\\beta}_j x_{ij}\\big)^2\\]\n\nRidge regression introduces a shrinkage penalty \\(\\lambda \\geq 0\\) by minimizing:\n\\[\\sum_i^n \\big(Y_i - \\beta_0 -  \\sum_j^p \\beta_j x_{ij}\\big)^2 + \\lambda \\sum_j^p \\beta_j^2 = \\text{RSS} + \\lambda \\sum_j^p \\beta_j^2\\]\n\n\n\nas \\(\\lambda\\) increases, flexibility of models decreases\n\nincreases bias, but decreases variance\n\n\n\n\n\nfor fixed value of \\(\\lambda\\), ridge regression fits only a single model\n\nneed to use cross-validation to tune \\(\\lambda\\)"
  },
  {
    "objectID": "lectures/13-regularization.html#shrinkage-methods-ridge-regression-1",
    "href": "lectures/13-regularization.html#shrinkage-methods-ridge-regression-1",
    "title": "Supervised learning: variable selection",
    "section": "Shrinkage methods: Ridge regression",
    "text": "Shrinkage methods: Ridge regression\nFor example: note how the magnitude of the coefficient for Income trends as \\(\\lambda \\rightarrow \\infty\\)\n\nThe coefficient shrinks towards zero, but never actually reaches it\n\nIncome is always a variable in the learned model, regardless of the value of \\(\\lambda\\)"
  },
  {
    "objectID": "lectures/13-regularization.html#shrinkage-methods-lasso-regression",
    "href": "lectures/13-regularization.html#shrinkage-methods-lasso-regression",
    "title": "Supervised learning: variable selection",
    "section": "Shrinkage methods: Lasso regression",
    "text": "Shrinkage methods: Lasso regression\nRidge regression keeps all variables… but we may believe there is a sparse solution\n\nLasso enables variable selection with \\(\\lambda\\) by minimizing:\n\\[\\sum_i^n \\big(Y_i - \\beta_0 -  \\sum_j^p \\beta_j X_{ij}\\big)^2 + \\lambda \\sum_j^p\\vert  \\beta_j \\vert = \\text{RSS} + \\lambda \\sum_j^p \\vert \\beta_j \\vert\\]\n\nLasso uses an \\(\\ell_1\\) (“ell 1”) penalty\n\n\n\n\nas \\(\\lambda\\) increases, flexibility of models decreases\n\nincreases bias, but decreases variance\n\n\n\n\n\nCan handle the \\(p &gt; n\\) case, i.e. more variables than observations!"
  },
  {
    "objectID": "lectures/13-regularization.html#shrinkage-methods-lasso-regression-1",
    "href": "lectures/13-regularization.html#shrinkage-methods-lasso-regression-1",
    "title": "Supervised learning: variable selection",
    "section": "Shrinkage methods: Lasso regression",
    "text": "Shrinkage methods: Lasso regression\nLasso regression performs variable selection yielding sparse models\n\nThe coefficient shrinks towards and eventually equals zero at \\(\\lambda \\approx 1000\\)\n\nif the optimum value of \\(\\lambda\\) is larger, then Income would NOT be included in the learned model"
  },
  {
    "objectID": "lectures/13-regularization.html#best-of-both-worlds-elastic-net",
    "href": "lectures/13-regularization.html#best-of-both-worlds-elastic-net",
    "title": "Supervised learning: variable selection",
    "section": "Best of both worlds? Elastic net",
    "text": "Best of both worlds? Elastic net\n\\[\\sum_{i}^{n}\\left(Y_{i}-\\beta_{0}-\\sum_{j}^{p} \\beta_{j} X_{i j}\\right)^{2}+\\lambda\\left[(1-\\alpha)\\|\\beta\\|_{2}^{2} / 2+\\alpha\\|\\beta\\|_{1} \\right]\\]\n\n\\(\\vert \\vert \\beta \\vert \\vert_1\\): \\(\\ell_1\\) norm: \\(\\vert \\vert \\beta \\vert \\vert_1 = \\sum_j^p \\vert \\beta_j \\vert\\)\n\\(\\vert \\vert \\beta \\vert \\vert_2\\): \\(\\ell_2\\) (Euclidean) norm: \\(\\vert \\vert \\beta \\vert \\vert_2 = \\sqrt{\\sum_j^p \\beta_j^2}\\)\n\n\n\nRidge penalty: \\(\\lambda \\cdot (1 - \\alpha) / 2\\)\nLasso penalty: \\(\\lambda \\cdot \\alpha\\)\n\n\n\n\n\\(\\alpha\\) controls the mixing between the two types, ranges from 0 to 1\n\n\\(\\alpha = 1\\) returns lasso\n\\(\\alpha = 0\\) return ridge"
  },
  {
    "objectID": "lectures/13-regularization.html#caveats-to-consider",
    "href": "lectures/13-regularization.html#caveats-to-consider",
    "title": "Supervised learning: variable selection",
    "section": "Caveats to consider…",
    "text": "Caveats to consider…\n\nFor either ridge, lasso, or elastic net: you should standardize your data\nCommon convention: within each column, compute then subtract off the sample mean, and compute the divide off the sample standard deviation \\(\\qquad \\displaystyle \\tilde{x}_{ij} = \\frac{x_{ij} - \\bar{x}_j}{s_{x,j}}\\)\nglmnet package does this by default and reports coefficients on the original scale\n\n\n\n\\(\\lambda\\) and \\(\\alpha\\) are tuning parameters\nChoose appropriate values based on test data / cross-validation\nUse cv.glmnet() function in glmnet to perform cross-validation"
  },
  {
    "objectID": "lectures/13-regularization.html#data-prostate-cancer-data",
    "href": "lectures/13-regularization.html#data-prostate-cancer-data",
    "title": "Supervised learning: variable selection",
    "section": "Data: Prostate Cancer Data",
    "text": "Data: Prostate Cancer Data\nExamine the level of a prostate specific antigen and a number of clinical measures in men who were about to receive a radical prostatectomy\n\nlibrary(tidyverse)\ntheme_set(theme_light())\n# more info: https://rdrr.io/cran/ElemStatLearn/man/prostate.html\nprostate &lt;- read_tsv(\"https://hastie.su.domains/ElemStatLearn/datasets/prostate.data\")\nglimpse(prostate)\n\nRows: 97\nColumns: 11\n$ ...1    &lt;dbl&gt; 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18,…\n$ lcavol  &lt;dbl&gt; -0.5798185, -0.9942523, -0.5108256, -1.2039728, 0.7514161, -1.…\n$ lweight &lt;dbl&gt; 2.769459, 3.319626, 2.691243, 3.282789, 3.432373, 3.228826, 3.…\n$ age     &lt;dbl&gt; 50, 58, 74, 58, 62, 50, 64, 58, 47, 63, 65, 63, 63, 67, 57, 66…\n$ lbph    &lt;dbl&gt; -1.3862944, -1.3862944, -1.3862944, -1.3862944, -1.3862944, -1…\n$ svi     &lt;dbl&gt; 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,…\n$ lcp     &lt;dbl&gt; -1.3862944, -1.3862944, -1.3862944, -1.3862944, -1.3862944, -1…\n$ gleason &lt;dbl&gt; 6, 6, 7, 6, 6, 6, 6, 6, 6, 6, 6, 6, 7, 7, 7, 6, 7, 6, 6, 6, 6,…\n$ pgg45   &lt;dbl&gt; 0, 0, 20, 0, 0, 0, 0, 0, 0, 0, 0, 0, 30, 5, 5, 0, 30, 0, 0, 0,…\n$ lpsa    &lt;dbl&gt; -0.4307829, -0.1625189, -0.1625189, -0.1625189, 0.3715636, 0.7…\n$ train   &lt;lgl&gt; TRUE, TRUE, TRUE, TRUE, TRUE, TRUE, FALSE, TRUE, FALSE, FALSE,…"
  },
  {
    "objectID": "lectures/13-regularization.html#introduction-to-glmnet",
    "href": "lectures/13-regularization.html#introduction-to-glmnet",
    "title": "Supervised learning: variable selection",
    "section": "Introduction to glmnet",
    "text": "Introduction to glmnet\nFit ridge, lasso, and elastic net models with glmnet\n\nlibrary(glmnet)\nprostate &lt;- prostate |&gt; \n  select(lcavol:lpsa)\n\nCreate predictor matrix and response vector\n\n# predictors\n# model_x &lt;- model.matrix(lpsa ~ ., prostate)\nmodel_x &lt;- prostate |&gt; \n  select(lcavol:pgg45) |&gt; \n  as.matrix()\n\n# response\n# model_y &lt;- prostate$lpsa\nmodel_y &lt;- prostate |&gt; \n  pull(lpsa)"
  },
  {
    "objectID": "lectures/13-regularization.html#vanilla-linear-regression-model",
    "href": "lectures/13-regularization.html#vanilla-linear-regression-model",
    "title": "Supervised learning: variable selection",
    "section": "Vanilla linear regression model",
    "text": "Vanilla linear regression model\n\n\n\nWhat do the initial regression coefficients look like?\nUse broom to tidy model output for plotting\n\n\nprostate_lm &lt;- lm(lpsa ~ ., data = prostate)\nlibrary(broom)\nprostate_lm |&gt; \n  tidy() |&gt; \n  mutate(term = fct_reorder(term, estimate)) |&gt; \n  ggplot(aes(x = estimate, y = term, \n             fill = estimate &gt; 0)) +\n  geom_col(color = \"white\", show.legend = FALSE) +\n  scale_fill_manual(values = c(\"darkred\", \"darkblue\"))"
  },
  {
    "objectID": "lectures/13-regularization.html#ridge-regression",
    "href": "lectures/13-regularization.html#ridge-regression",
    "title": "Supervised learning: variable selection",
    "section": "Ridge regression",
    "text": "Ridge regression\nPerform ridge regression using glmnet with alpha = 0\nBy default, predictors are standardized and models are fitted across a range of \\(\\lambda\\) values (can plot these!)\n\nprostate_ridge &lt;- glmnet(model_x, model_y, alpha = 0)\nplot(prostate_ridge, xvar = \"lambda\")"
  },
  {
    "objectID": "lectures/13-regularization.html#ridge-regression-1",
    "href": "lectures/13-regularization.html#ridge-regression-1",
    "title": "Supervised learning: variable selection",
    "section": "Ridge regression",
    "text": "Ridge regression\nUse cross-validation to select \\(\\lambda\\) with cv.glmnet() which uses 10-folds by default\nSpecify ridge regression with alpha = 0\n\nprostate_ridge_cv &lt;- cv.glmnet(model_x, model_y, alpha = 0)\nplot(prostate_ridge_cv)"
  },
  {
    "objectID": "lectures/13-regularization.html#tidy-ridge-regression",
    "href": "lectures/13-regularization.html#tidy-ridge-regression",
    "title": "Supervised learning: variable selection",
    "section": "Tidy ridge regression",
    "text": "Tidy ridge regression\n\n\n\n# str(prostate_ridge_cv)\ntidy_ridge_coef &lt;- tidy(prostate_ridge_cv$glmnet.fit)\ntidy_ridge_coef |&gt; \n  ggplot(aes(x = lambda, y = estimate, group = term)) +\n  scale_x_log10() +\n  geom_line(alpha = 0.75) +\n  geom_vline(xintercept = prostate_ridge_cv$lambda.min) +\n  geom_vline(xintercept = prostate_ridge_cv$lambda.1se, \n             linetype = \"dashed\", color = \"red\")\n\n\nCould easily add color with legend for variables…"
  },
  {
    "objectID": "lectures/13-regularization.html#tidy-ridge-regression-1",
    "href": "lectures/13-regularization.html#tidy-ridge-regression-1",
    "title": "Supervised learning: variable selection",
    "section": "Tidy ridge regression",
    "text": "Tidy ridge regression\n\n\n\ntidy_ridge_cv &lt;- tidy(prostate_ridge_cv)\ntidy_ridge_cv |&gt; \n  ggplot(aes(x = lambda, y = estimate)) +\n  geom_line() + \n  scale_x_log10() +\n  geom_ribbon(aes(ymin = conf.low, ymax = conf.high), \n              alpha = 0.2) +\n  geom_vline(xintercept = prostate_ridge_cv$lambda.min) +\n  geom_vline(xintercept = prostate_ridge_cv$lambda.1se,\n             linetype = \"dashed\", color = \"red\")"
  },
  {
    "objectID": "lectures/13-regularization.html#ridge-regression-coefficients",
    "href": "lectures/13-regularization.html#ridge-regression-coefficients",
    "title": "Supervised learning: variable selection",
    "section": "Ridge regression coefficients",
    "text": "Ridge regression coefficients\n\n\nCoefficients using the 1 standard error rule \\(\\lambda\\)\n\ntidy_ridge_coef |&gt;\n  filter(lambda == prostate_ridge_cv$lambda.1se) |&gt;\n  mutate(term = fct_reorder(term, estimate)) |&gt;\n  ggplot(aes(x = estimate, y = term, \n             fill = estimate &gt; 0)) +\n  geom_col(color = \"white\", show.legend = FALSE) +\n  scale_fill_manual(values = c(\"darkred\", \"darkblue\"))"
  },
  {
    "objectID": "lectures/13-regularization.html#lasso-regression-example",
    "href": "lectures/13-regularization.html#lasso-regression-example",
    "title": "Supervised learning: variable selection",
    "section": "Lasso regression example",
    "text": "Lasso regression example\n\n\nSimilar syntax to ridge but specify alpha = 1:\n\nprostate_lasso_cv &lt;- cv.glmnet(model_x, model_y, \n                               alpha = 1)\ntidy_lasso_coef &lt;- tidy(prostate_lasso_cv$glmnet.fit)\ntidy_lasso_coef |&gt; \n  ggplot(aes(x = lambda, y = estimate, group = term)) +\n  scale_x_log10() +\n  geom_line(alpha = 0.75) +\n  geom_vline(xintercept = prostate_lasso_cv$lambda.min) +\n  geom_vline(xintercept = prostate_lasso_cv$lambda.1se, \n             linetype = \"dashed\", color = \"red\")"
  },
  {
    "objectID": "lectures/13-regularization.html#lasso-regression-example-1",
    "href": "lectures/13-regularization.html#lasso-regression-example-1",
    "title": "Supervised learning: variable selection",
    "section": "Lasso regression example",
    "text": "Lasso regression example\n\n\nNumber of non-zero predictors by \\(\\lambda\\)\n\ntidy_lasso_cv &lt;- tidy(prostate_lasso_cv)\ntidy_lasso_cv |&gt;\n  ggplot(aes(x = lambda, y = nzero)) +\n  geom_line() +\n  geom_vline(xintercept = prostate_lasso_cv$lambda.min) +\n  geom_vline(xintercept = prostate_lasso_cv$lambda.1se, \n             linetype = \"dashed\", color = \"red\") +\n  scale_x_log10()\n\nReduction in variables using 1 standard error rule \\(\\lambda\\)"
  },
  {
    "objectID": "lectures/13-regularization.html#lasso-regression-example-2",
    "href": "lectures/13-regularization.html#lasso-regression-example-2",
    "title": "Supervised learning: variable selection",
    "section": "Lasso regression example",
    "text": "Lasso regression example\n\n\nCoefficients using the 1 standard error rule \\(\\lambda\\)\n\ntidy_lasso_coef |&gt;\n  filter(lambda == prostate_lasso_cv$lambda.1se) |&gt;\n  mutate(term = fct_reorder(term, estimate)) |&gt;\n  ggplot(aes(x = estimate, y = term, \n             fill = estimate &gt; 0)) +\n  geom_col(color = \"white\", show.legend = FALSE) +\n  scale_fill_manual(values = c(\"darkred\", \"darkblue\"))"
  },
  {
    "objectID": "lectures/13-regularization.html#elastic-net-example",
    "href": "lectures/13-regularization.html#elastic-net-example",
    "title": "Supervised learning: variable selection",
    "section": "Elastic net example",
    "text": "Elastic net example\nNeed to tune both \\(\\lambda\\) and \\(\\alpha\\) - can do so manually with our own folds\n\nset.seed(100)\nfold_id &lt;- sample(rep(1:10, length.out = nrow(model_x)))\n\nThen use cross-validation with these folds for different candidate alpha values:\n\ncv_enet_25 &lt;- cv.glmnet(model_x, model_y, foldid = fold_id, alpha = 0.25)\ncv_enet_50 &lt;- cv.glmnet(model_x, model_y, foldid = fold_id, alpha = 0.5)\ncv_ridge &lt;- cv.glmnet(model_x, model_y, foldid = fold_id, alpha = 0)\ncv_lasso &lt;- cv.glmnet(model_x, model_y, foldid = fold_id, alpha = 1)\n\nCan see which one had the lowest CV error among its candidate \\(\\lambda\\) values:\n\nwhich.min(c(min(cv_enet_25$cvm), min(cv_enet_50$cvm), min(cv_ridge$cvm), min(cv_lasso$cvm)))\n\n[1] 3"
  },
  {
    "objectID": "lectures/13-regularization.html#elastic-net-example-1",
    "href": "lectures/13-regularization.html#elastic-net-example-1",
    "title": "Supervised learning: variable selection",
    "section": "Elastic net example",
    "text": "Elastic net example\n\n\nCan view same type of summary\n\ncv_enet_50 |&gt; \n  tidy() |&gt; \n  ggplot(aes(x = lambda, y = nzero)) +\n  geom_line() +\n  geom_vline(xintercept = cv_enet_50$lambda.min) +\n  geom_vline(xintercept = cv_enet_50$lambda.1se, \n             linetype = \"dashed\", \n             color = \"red\") +\n  scale_x_log10()\n\n\nMore relaxed than lasso for variable entry"
  },
  {
    "objectID": "lectures/13-regularization.html#comparison-of-models-based-on-holdout-performance",
    "href": "lectures/13-regularization.html#comparison-of-models-based-on-holdout-performance",
    "title": "Supervised learning: variable selection",
    "section": "Comparison of models based on holdout performance",
    "text": "Comparison of models based on holdout performance\n\nset.seed(101)\nk &lt;- 5\nprostate &lt;- prostate |&gt;\n  mutate(test_fold = sample(rep(1:k, length.out = n())))\n\n\nget_test_pred &lt;- function(k) {\n  test_data &lt;- prostate |&gt; filter(test_fold == k)                     # get test and training data\n  train_data &lt;- prostate |&gt; filter(test_fold != k)\n  test_x &lt;- as.matrix(select(test_data, -lpsa))                       # get test and training matrices\n  train_x &lt;- as.matrix(select(train_data, -lpsa))\n  \n  lm_fit &lt;- lm(lpsa ~ ., data = train_data)                           # fit models to training data\n  ridge_fit &lt;- cv.glmnet(train_x, train_data$lpsa, alpha = 0)\n  lasso_fit &lt;- cv.glmnet(train_x, train_data$lpsa, alpha = 1)\n  enet_fit &lt;- cv.glmnet(train_x, train_data$lpsa, alpha = 0.5)\n  \n  tibble(lm_pred = predict(lm_fit, newdata = test_data),              # return test results\n         ridge_pred = as.numeric(predict(ridge_fit, newx = test_x)),\n         lasso_pred = as.numeric(predict(lasso_fit, newx = test_x)),\n         enet_pred = as.numeric(predict(enet_fit, newx = test_x)),\n         test_actual = test_data$lpsa,\n         test_fold = k)\n}\n\n\ntest_pred_all &lt;- map(1:k, get_test_pred) |&gt; \n  bind_rows()"
  },
  {
    "objectID": "lectures/13-regularization.html#comparison-of-models-based-on-holdout-performance-1",
    "href": "lectures/13-regularization.html#comparison-of-models-based-on-holdout-performance-1",
    "title": "Supervised learning: variable selection",
    "section": "Comparison of models based on holdout performance",
    "text": "Comparison of models based on holdout performance\n\n\nCompute RMSE across folds with standard error intervals\n\ntest_pred_all |&gt;\n  pivot_longer(lm_pred:enet_pred, \n               names_to = \"type\", \n               values_to = \"test_pred\") |&gt;\n  group_by(type, test_fold) |&gt;\n  summarize(\n    rmse = sqrt(mean((test_actual - test_pred)^2))\n  ) |&gt; \n  ggplot(aes(x = type, y = rmse)) + \n  geom_point(size = 4) +\n  stat_summary(fun = mean, geom = \"point\", \n               color = \"red\", size = 4) + \n  stat_summary(fun.data = mean_se, geom = \"errorbar\", \n               color = \"red\", width = 0.2)\n\nRegularization methods perform better than a linear model\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n36-SURE.github.io"
  },
  {
    "objectID": "lectures/06-hierarchical.html#the-big-picture",
    "href": "lectures/06-hierarchical.html#the-big-picture",
    "title": "Unsupervised learning: hierarchical clustering",
    "section": "The big picture",
    "text": "The big picture\n\n\\(k\\)-means clustering: partition the observations into a pre-specified number of clusters\n\n\n\nHierarchical clustering: does not require commitment to a particular choice of clusters\n\nIn fact, we end up with a tree-like visual representation of the observations, called a dendrogram\nThis allows us to view at once the clusterings obtained for each possible number of clusters\nCommon approach: agglomerative (bottom-up) hierarchical clustering: build a dendrogram starting from the leaves and combining clusters up to the trunk\nThere’s also divisive (top-down) hierarchical clustering: start with one large cluster and then break the cluster recursively into smaller and smaller pieces"
  },
  {
    "objectID": "lectures/06-hierarchical.html#data-county-level-health-indicators-for-utah-in-2014",
    "href": "lectures/06-hierarchical.html#data-county-level-health-indicators-for-utah-in-2014",
    "title": "Unsupervised learning: hierarchical clustering",
    "section": "Data: County-level health indicators for Utah in 2014",
    "text": "Data: County-level health indicators for Utah in 2014\n\nlibrary(tidyverse)\ntheme_set(theme_light())\nutah_health &lt;- read_csv(\"https://raw.githubusercontent.com/36-SURE/36-SURE.github.io/main/data/utah_health.csv\")\nglimpse(utah_health)\n\nRows: 30\nColumns: 20\n$ County                 &lt;chr&gt; \"Tooele\", \"Uintah\", \"Wasatch\", \"Statewide\", \"Sa…\n$ Population             &lt;dbl&gt; 59870, 34524, 25273, 2855287, 27906, 1524, 7221…\n$ PercentUnder18         &lt;dbl&gt; 35.3, 33.6, 33.1, 31.1, 29.4, 27.8, 23.4, 33.6,…\n$ PercentOver65          &lt;dbl&gt; 7.9, 9.1, 9.1, 9.5, 12.3, 23.7, 20.5, 11.0, 10.…\n$ DiabeticRate           &lt;dbl&gt; 9, 8, 6, 7, 8, 10, 10, 8, 8, 7, 7, 9, 6, 4, 9, …\n$ HIVRate                &lt;dbl&gt; 37, 21, 28, 111, 22, NA, NA, NA, 72, 36, NA, NA…\n$ PrematureMortalityRate &lt;dbl&gt; 347.6, 396.2, 227.2, 286.7, 314.3, 445.8, 293.6…\n$ InfantMortalityRate    &lt;dbl&gt; 5.0, 5.2, NA, 5.0, NA, NA, NA, NA, 5.8, 7.1, NA…\n$ ChildMortalityRate     &lt;dbl&gt; 47.7, 57.4, 69.0, 52.9, 61.2, NA, NA, 70.4, 58.…\n$ LimitedAccessToFood    &lt;dbl&gt; 14, 14, 14, 17, 17, 15, 14, 14, 16, 19, 11, 18,…\n$ FoodInsecure           &lt;dbl&gt; 7, 5, 1, 5, 4, 41, 16, 8, 6, 12, 0, 14, 4, 4, 8…\n$ MotorDeathRate         &lt;dbl&gt; 18, 23, 16, 11, 18, NA, 21, 36, 10, 13, NA, NA,…\n$ DrugDeathRate          &lt;dbl&gt; 18, 13, 17, 17, 16, NA, 26, 18, 20, 18, NA, NA,…\n$ Uninsured              &lt;dbl&gt; 17, 24, 24, 20, 24, 26, 19, 23, 20, 26, 14, 26,…\n$ UninsuredChildren      &lt;dbl&gt; 10, 16, 16, 11, 15, 17, 12, 14, 12, 15, 10, 20,…\n$ HealthCareCosts        &lt;dbl&gt; 9095, 7086, 8327, 8925, 8942, 7824, 8121, 8527,…\n$ CouldNotSeeDr          &lt;dbl&gt; 13, 12, 13, 13, 13, NA, NA, 13, 11, 13, 13, NA,…\n$ MedianIncome           &lt;dbl&gt; 61927, 60419, 62014, 57067, 43921, 36403, 43128…\n$ ChildrenFreeLunch      &lt;dbl&gt; 34, 36, 30, 31, 38, 58, 31, 34, 37, 40, 12, 33,…\n$ HomicideRate           &lt;dbl&gt; NA, NA, NA, 2, NA, NA, NA, NA, 2, NA, NA, NA, 1…"
  },
  {
    "objectID": "lectures/06-hierarchical.html#general-setup",
    "href": "lectures/06-hierarchical.html#general-setup",
    "title": "Unsupervised learning: hierarchical clustering",
    "section": "General setup",
    "text": "General setup\n\n\n\nGiven a dataset with \\(p\\) variables (columns) and \\(n\\) observations (rows) \\(x_1,\\dots,x_n\\)\nCompute the distance/dissimilarity between observations\ne.g. Euclidean distance between observations \\(i\\) and \\(j\\)\n\n\\[d(x_i, x_j) = \\sqrt{(x_{i1}-x_{j1})^2 + \\cdots + (x_{ip}-x_{jp})^2}\\]\nWhat are the distances between these counties using PercentOver65 (percent of county population that is 65 and over) and DiabeticRate (prevalence of diabetes)?\n\n\n\nutah_health |&gt; \n  ggplot(aes(x = PercentOver65, y = DiabeticRate)) +\n  geom_point(size = 4)"
  },
  {
    "objectID": "lectures/06-hierarchical.html#remember-to-standardize",
    "href": "lectures/06-hierarchical.html#remember-to-standardize",
    "title": "Unsupervised learning: hierarchical clustering",
    "section": "Remember to standardize!",
    "text": "Remember to standardize!\n\n\n\nutah_health &lt;- utah_health |&gt; \n  mutate(\n    std_pct_over65 = as.numeric(scale(PercentOver65)),\n    std_diabetic_rate = as.numeric(scale(DiabeticRate))\n  )\n\nutah_health |&gt; \n  ggplot(aes(x = std_pct_over65, y = std_diabetic_rate)) +\n  geom_point(size = 4) +\n  coord_fixed()"
  },
  {
    "objectID": "lectures/06-hierarchical.html#compute-the-distance-matrix-using-dist",
    "href": "lectures/06-hierarchical.html#compute-the-distance-matrix-using-dist",
    "title": "Unsupervised learning: hierarchical clustering",
    "section": "Compute the distance matrix using dist()",
    "text": "Compute the distance matrix using dist()\n\nCompute pairwise Euclidean distance\n\n\ncounty_dist &lt;- utah_health |&gt; \n  select(std_pct_over65, std_diabetic_rate) |&gt; \n  dist()\n\n\nReturns an object of dist class… but not a matrix\nConvert to a matrix, then set the row and column names:\n\n\ncounty_dist_matrix &lt;- as.matrix(county_dist)\nrownames(county_dist_matrix) &lt;- utah_health$County\ncolnames(county_dist_matrix) &lt;- utah_health$County\ncounty_dist_matrix[1:4, 1:4]\n\n             Tooele    Uintah   Wasatch Statewide\nTooele    0.0000000 0.7474104 2.1010897 1.4367342\nUintah    0.7474104 0.0000000 1.3885164 0.7003632\nWasatch   2.1010897 1.3885164 0.0000000 0.7003632\nStatewide 1.4367342 0.7003632 0.7003632 0.0000000\n\n\n\nConvert to a long table with pivot_longer for plotting purpose\n\n\nlong_dist_matrix &lt;- county_dist_matrix |&gt; \n  as_tibble() |&gt; \n  mutate(county1 = rownames(county_dist_matrix)) |&gt; \n  pivot_longer(cols = !county1, names_to = \"county2\", values_to = \"distance\")"
  },
  {
    "objectID": "lectures/06-hierarchical.html#this-heatmap-is-useless",
    "href": "lectures/06-hierarchical.html#this-heatmap-is-useless",
    "title": "Unsupervised learning: hierarchical clustering",
    "section": "This heatmap is useless…",
    "text": "This heatmap is useless…\n\n\n\n\nlong_dist_matrix |&gt; \n  ggplot(aes(x = county1, y = county2, fill = distance)) +\n  geom_tile() +\n  scale_fill_gradient(low = \"darkorange\", \n                      high = \"darkblue\") +\n  coord_fixed() +\n  theme(axis.text = element_blank(), \n        axis.ticks = element_blank())"
  },
  {
    "objectID": "lectures/06-hierarchical.html#arrange-heatmap-with-seriation",
    "href": "lectures/06-hierarchical.html#arrange-heatmap-with-seriation",
    "title": "Unsupervised learning: hierarchical clustering",
    "section": "Arrange heatmap with seriation",
    "text": "Arrange heatmap with seriation\n\n\n\n\nlibrary(seriation)\ncounty_dist_seriate &lt;- seriate(county_dist)\ncounty_order &lt;- get_order(county_dist_seriate)\ncounty_names_order &lt;- utah_health$County[county_order]\nlong_dist_matrix |&gt; \n  mutate(\n    county1 = fct_relevel(county1, county_names_order),\n    county2 = fct_relevel(county2, county_names_order)\n  ) |&gt; \n  ggplot(aes(x = county1, y = county2, fill = distance)) +\n  scale_fill_gradient(low = \"darkorange\", \n                      high = \"darkblue\") +\n  geom_tile() +\n  coord_fixed() +\n  theme(axis.text = element_blank(), \n        axis.ticks = element_blank())"
  },
  {
    "objectID": "lectures/06-hierarchical.html#agglomerative-hierarchical-clustering",
    "href": "lectures/06-hierarchical.html#agglomerative-hierarchical-clustering",
    "title": "Unsupervised learning: hierarchical clustering",
    "section": "(Agglomerative) Hierarchical clustering",
    "text": "(Agglomerative) Hierarchical clustering\nLet’s pretend all \\(n\\) observations are in their own cluster\n\n\nStep 1: Compute the pairwise dissimilarities between each cluster\n\ne.g., distance matrix on previous slides\n\n\n\n\n\nStep 2: Identify the pair of clusters that are least dissimilar\n\n\n\n\nStep 3: Fuse these two clusters into a new cluster!\n\n\n\n\nRepeat Steps 1 to 3 until all observations are in the same cluster\n\n\n\n“Bottom-up”, agglomerative clustering that forms a tree/hierarchy of merging\nNo mention of any randomness. And no mention of the number of clusters \\(k\\)."
  },
  {
    "objectID": "lectures/06-hierarchical.html#agglomerative-hierarchical-clustering-1",
    "href": "lectures/06-hierarchical.html#agglomerative-hierarchical-clustering-1",
    "title": "Unsupervised learning: hierarchical clustering",
    "section": "(Agglomerative) Hierarchical clustering",
    "text": "(Agglomerative) Hierarchical clustering\n\n\nStart with all observations in their own cluster\n\nStep 1: Compute the pairwise dissimilarities between each cluster\nStep 2: Identify the pair of clusters that are least dissimilar\nStep 3: Fuse these two clusters into a new cluster!\nRepeat Steps 1 to 3 until all observations are in the same cluster"
  },
  {
    "objectID": "lectures/06-hierarchical.html#agglomerative-hierarchical-clustering-2",
    "href": "lectures/06-hierarchical.html#agglomerative-hierarchical-clustering-2",
    "title": "Unsupervised learning: hierarchical clustering",
    "section": "(Agglomerative) Hierarchical clustering",
    "text": "(Agglomerative) Hierarchical clustering\n\n\nStart with all observations in their own cluster\n\nStep 1: Compute the pairwise dissimilarities between each cluster\nStep 2: Identify the pair of clusters that are least dissimilar\nStep 3: Fuse these two clusters into a new cluster!\nRepeat Steps 1 to 3 until all observations are in the same cluster\n\n\n\n\n\n\n\n\n\n\n\nForms a dendrogram (typically displayed from bottom-up)"
  },
  {
    "objectID": "lectures/06-hierarchical.html#dissimilarity-between-clusters",
    "href": "lectures/06-hierarchical.html#dissimilarity-between-clusters",
    "title": "Unsupervised learning: hierarchical clustering",
    "section": "Dissimilarity between clusters",
    "text": "Dissimilarity between clusters\n\nWe know how to compute distance/dissimilarity between two observations\nBut how do we handle clusters?\n\nDissimilarity between a cluster and an observation, or between two clusters\n\n\n\nWe need to choose a linkage function. Clusters are built up by linking them together"
  },
  {
    "objectID": "lectures/06-hierarchical.html#types-of-linkage",
    "href": "lectures/06-hierarchical.html#types-of-linkage",
    "title": "Unsupervised learning: hierarchical clustering",
    "section": "Types of linkage",
    "text": "Types of linkage\nFirst, compute all pairwise dissimilarities between the observations in the two clusters\ni.e., compute the distance matrix between observations, \\(d(x_i, x_j)\\) for \\(i \\in C_1\\) and \\(j \\in C_2\\)\n\n\nComplete linkage: use the maximum (largest) value of these dissimilarities \\(\\underset{i \\in C_1, j \\in C_2}{\\text{max}} d(x_i, x_j)\\) (maximal inter-cluster dissimilarity)\n\n\n\n\nSingle linkage: use the minimum (smallest) value of these dissimilarities \\(\\underset{i \\in C_1, j \\in C_2}{\\text{min}} d(x_i, x_j)\\) (minimal inter-cluster dissimilarity)\n\n\n\n\nAverage linkage: use the average value of these dissimilarities \\(\\displaystyle \\frac{1}{|C_1||C_2|} \\sum_{i \\in C_1} \\sum_{j \\in C_2} d(x_i, x_j)\\) (mean inter-cluster dissimilarity)"
  },
  {
    "objectID": "lectures/06-hierarchical.html#complete-linkage-example",
    "href": "lectures/06-hierarchical.html#complete-linkage-example",
    "title": "Unsupervised learning: hierarchical clustering",
    "section": "Complete linkage example",
    "text": "Complete linkage example\n\n\n\nUse hclust() with a dist() objsect\nUse complete linkage by default\n\n\nutah_complete &lt;- county_dist |&gt; \n  hclust(method = \"complete\")\n\n\nUse cutree() to return cluster labels\nReturns compact clusters (similar to \\(k\\)-means)\n\n\nutah_health |&gt; \n  mutate(\n    cluster = as.factor(cutree(utah_complete, k = 3))\n  ) |&gt;\n  ggplot(aes(x = std_pct_over65, y = std_diabetic_rate,\n             color = cluster)) +\n  geom_point(size = 4) + \n  ggthemes::scale_color_colorblind() +\n  theme(legend.position = \"bottom\")"
  },
  {
    "objectID": "lectures/06-hierarchical.html#what-are-we-cutting-dendrograms",
    "href": "lectures/06-hierarchical.html#what-are-we-cutting-dendrograms",
    "title": "Unsupervised learning: hierarchical clustering",
    "section": "What are we cutting? Dendrograms",
    "text": "What are we cutting? Dendrograms\n\n\nUse the ggdendro package (instead of plot())\n\nlibrary(ggdendro)\nutah_complete |&gt; \n  ggdendrogram(labels = FALSE, \n               leaf_labels = FALSE,\n               theme_dendro = FALSE) +  \n  labs(y = \"Dissimilarity between clusters\") +\n  theme(axis.text.x = element_blank(), \n        axis.title.x = element_blank(),\n        panel.grid = element_blank())\n\n\nEach leaf is one observation\nHeight of branch indicates dissimilarity between clusters\n\n(After first step) Horizontal position along x-axis means nothing"
  },
  {
    "objectID": "lectures/06-hierarchical.html#textbook-example",
    "href": "lectures/06-hierarchical.html#textbook-example",
    "title": "Unsupervised learning: hierarchical clustering",
    "section": "Textbook example",
    "text": "Textbook example"
  },
  {
    "objectID": "lectures/06-hierarchical.html#cut-dendrograms-to-obtain-cluster-labels",
    "href": "lectures/06-hierarchical.html#cut-dendrograms-to-obtain-cluster-labels",
    "title": "Unsupervised learning: hierarchical clustering",
    "section": "Cut dendrograms to obtain cluster labels",
    "text": "Cut dendrograms to obtain cluster labels\n\n\nSpecify the height to cut with h (instead of k)\n\n\n\n\n\n\n\n\n\n\nFor example, cutree(utah_complete, h = 4)"
  },
  {
    "objectID": "lectures/06-hierarchical.html#single-linkage-example",
    "href": "lectures/06-hierarchical.html#single-linkage-example",
    "title": "Unsupervised learning: hierarchical clustering",
    "section": "Single linkage example",
    "text": "Single linkage example\n\n\nChange the method argument to single\n\n\n\n\n\n\n\n\n\n\nResults in a chaining effect"
  },
  {
    "objectID": "lectures/06-hierarchical.html#average-linkage-example",
    "href": "lectures/06-hierarchical.html#average-linkage-example",
    "title": "Unsupervised learning: hierarchical clustering",
    "section": "Average linkage example",
    "text": "Average linkage example\n\n\nChange the method argument to average\n\n\n\n\n\n\n\n\n\n\nCloser to complete but varies in compactness"
  },
  {
    "objectID": "lectures/06-hierarchical.html#more-linkage-functions",
    "href": "lectures/06-hierarchical.html#more-linkage-functions",
    "title": "Unsupervised learning: hierarchical clustering",
    "section": "More linkage functions",
    "text": "More linkage functions\n\nCentroid linkage: Computes the dissimilarity between the centroid for cluster 1 and the centroid for cluster 2\n\ni.e. distance between the averages of the two clusters\nuse method = centroid\n\n\n\n\nWard’s linkage: Merges a pair of clusters to minimize the within-cluster variance\n\ni.e. aim is to minimize the objection function from \\(K\\)-means\ncan use ward.D or ward.D2 (different algorithms)\n\n\n\n\n\nThere’s another one…"
  },
  {
    "objectID": "lectures/06-hierarchical.html#minimax-linkage",
    "href": "lectures/06-hierarchical.html#minimax-linkage",
    "title": "Unsupervised learning: hierarchical clustering",
    "section": "Minimax linkage",
    "text": "Minimax linkage\n\n\nEach cluster is defined by a prototype observation (most representative)\nThe prototype is “minimally dissimilar” from every point in the cluster (hence the “minimax”)\nDendrogram interpretation: each point is \\(\\leq h\\) in dissimilarity to the prototype of cluster\nCluster centers are chosen among the observations themselves - hence prototype"
  },
  {
    "objectID": "lectures/06-hierarchical.html#minimax-linkage-1",
    "href": "lectures/06-hierarchical.html#minimax-linkage-1",
    "title": "Unsupervised learning: hierarchical clustering",
    "section": "Minimax linkage",
    "text": "Minimax linkage\n\nFor each point belonging to either cluster, find the maximum distance between it and all the other points in the two clusters.\nThe smallest of these maximum distances (“minimal-maximum” distance) is defined as the distance between the two clusters\nThe distance to prototype is measured by the maximum minimax radius"
  },
  {
    "objectID": "lectures/06-hierarchical.html#minimax-linkage-example",
    "href": "lectures/06-hierarchical.html#minimax-linkage-example",
    "title": "Unsupervised learning: hierarchical clustering",
    "section": "Minimax linkage example",
    "text": "Minimax linkage example\n\n\n\nEasily done in R via the protoclust package\nUse the protoclust() function to apply the clustering to the dist() object\n\n\nlibrary(protoclust)\nutah_minimax &lt;- protoclust(county_dist_matrix)\n\nutah_minimax |&gt; \n  as.hclust() |&gt;\n  as.dendrogram() |&gt; \n  ggdendrogram(labels = FALSE, \n               leaf_labels = FALSE,\n               theme_dendro = FALSE) +  \n  labs(y = \"Dissimilarity between clusters\") +\n  theme(axis.text.x = element_blank(), \n        axis.title.x = element_blank(),\n        panel.grid = element_blank())\n\n\n\n\nconverting to dist (note: ignores above diagonal)"
  },
  {
    "objectID": "lectures/06-hierarchical.html#minimax-linkage-example-1",
    "href": "lectures/06-hierarchical.html#minimax-linkage-example-1",
    "title": "Unsupervised learning: hierarchical clustering",
    "section": "Minimax linkage example",
    "text": "Minimax linkage example\n\n\n\nUse the protocut() function to make the cut\nBut then access the cluster labels cl\n\n\nminimax_county_clusters &lt;- utah_minimax |&gt; \n  protocut(k = 3)\nutah_health |&gt; \n  mutate(cluster = as.factor(minimax_county_clusters$cl)) |&gt;\n  ggplot(aes(x = std_pct_over65, y = std_diabetic_rate,\n             color = cluster)) +\n  geom_point(size = 4) + \n  ggthemes::scale_color_colorblind() +\n  theme(legend.position = \"bottom\")"
  },
  {
    "objectID": "lectures/06-hierarchical.html#minimax-linkage-example-2",
    "href": "lectures/06-hierarchical.html#minimax-linkage-example-2",
    "title": "Unsupervised learning: hierarchical clustering",
    "section": "Minimax linkage example",
    "text": "Minimax linkage example\n\nWant to check out the prototypes for the three clusters\nprotocut returns the indices of the prototypes (in order of the cluster labels)\n\n\nminimax_county_clusters$protos\n\n[1] 18 13  7\n\n\n\nSubset the rows for these counties using slice()\n\n\nutah_health |&gt;\n  select(County, std_pct_over65, std_diabetic_rate) |&gt;\n  slice(minimax_county_clusters$protos)\n\n# A tibble: 3 × 3\n  County std_pct_over65 std_diabetic_rate\n  &lt;chr&gt;           &lt;dbl&gt;             &lt;dbl&gt;\n1 Emery          0.0600             0.116\n2 Davis         -0.978             -1.27 \n3 Kane           1.74               1.50"
  },
  {
    "objectID": "lectures/06-hierarchical.html#post-clustering-analysis",
    "href": "lectures/06-hierarchical.html#post-clustering-analysis",
    "title": "Unsupervised learning: hierarchical clustering",
    "section": "Post-clustering analysis",
    "text": "Post-clustering analysis\n\nFor context, how does population relate to our clustering results?\n\n\nutah_health |&gt; \n  mutate(cluster = as.factor(minimax_county_clusters$cl)) |&gt; \n  ggplot(aes(x = log(Population), fill = cluster)) +\n  geom_density(alpha = 0.1) +\n  scale_fill_manual(values = c(\"darkblue\", \"purple\", \"gold\", \"orange\"))"
  },
  {
    "objectID": "lectures/06-hierarchical.html#post-clustering-analysis-1",
    "href": "lectures/06-hierarchical.html#post-clustering-analysis-1",
    "title": "Unsupervised learning: hierarchical clustering",
    "section": "Post-clustering analysis",
    "text": "Post-clustering analysis\n\n\n\n# utah_health |&gt; \n#   arrange(Population)\nlibrary(dendextend)\nutah_minimax |&gt; \n  as.hclust() |&gt;\n  as.dendrogram() |&gt; \n  set(\"branches_k_color\", k = 3) |&gt; \n  set(\"labels_col\", k = 3) |&gt; \n  ggplot(horiz = TRUE)\n\n\nDifferent population levels tend to fall within particular clusters…\nIt’s easy to include more variables - just change the distance matrix"
  },
  {
    "objectID": "lectures/06-hierarchical.html#practical-issues",
    "href": "lectures/06-hierarchical.html#practical-issues",
    "title": "Unsupervised learning: hierarchical clustering",
    "section": "Practical issues",
    "text": "Practical issues\n\nWhat dissimilarity measure should be used?\nWhat type of linkage should be used?\nHow many clusters to choose?\nWhich features should we use to drive the clustering?\n\nCategorical variables?\n\nHard clustering vs. soft clustering\n\nHard clustering (\\(k\\)-means, hierachical): assigns each observation to exactly one cluster\nSoft (fuzzy) clustering: assigns each observation a probability of belonging to a cluster\n\n\n\n\n\nIT DEPENDS…\n\n\n\n\n36-SURE.github.io"
  },
  {
    "objectID": "lectures/09-presentation.html#eda-project-presentation",
    "href": "lectures/09-presentation.html#eda-project-presentation",
    "title": "Presentations",
    "section": "EDA project presentation",
    "text": "EDA project presentation\n\n8-min presentation on Tuesday, June 18 during lecture time\n\naim for 7 min + 1 min for Q&A\nmake sure you do not exceed 8 min (practice and time yourself)\nI’ll cut you off if you exceed 8 min\n\nSlides are due Monday, June 17 at 5pm ET"
  },
  {
    "objectID": "lectures/09-presentation.html#capstone-project-presentation-a-preview",
    "href": "lectures/09-presentation.html#capstone-project-presentation-a-preview",
    "title": "Presentations",
    "section": "Capstone project presentation (a preview)",
    "text": "Capstone project presentation (a preview)\n\n2 presentation checkpoints (both during lab time)\n1 final presentation on the final day (July 26) (+ other deliverables)\nDetails will be provided next week"
  },
  {
    "objectID": "lectures/09-presentation.html#begin-with-background-and-motivation",
    "href": "lectures/09-presentation.html#begin-with-background-and-motivation",
    "title": "Presentations",
    "section": "Begin with background and motivation",
    "text": "Begin with background and motivation\nEvery presentation has a story:\n\nWhat is the motivation? Why should people care about your work?\nYou want to build up what your work is trying to address\n\n\nExample: Ron’s nflWAR talk at NESSIS 2017:\n\nDo NOT begin with: “We’re introducing [project topic]” (WAR for NFL)\nInstead begin with current state of NFL analytics and need for better, reproducible player level-metrics\n\n\n\nDo NOT include an outline slide!\n\nYour presentation should flow naturally\n\n\n\nOr you can begin with a “creative” intro…"
  },
  {
    "objectID": "lectures/09-presentation.html#describing-the-data",
    "href": "lectures/09-presentation.html#describing-the-data",
    "title": "Presentations",
    "section": "Describing the data",
    "text": "Describing the data\nYou want to provide a general overview of your dataset:\n\nWhat are your observations? i.e., what does each row of your dataset represent?\n\n\n\nWhat are the relevant variables/features? i.e., what are the columns of interest?\n\nBe careful though with many variables - avoid just listing everything!\nSimplify by describing groups of variables together\n\n\n\n\nUse examples - makes your data explicit and concrete for the audience\n\nBut do NOT print out raw R console output or screenshots of output!\nUse text or a clean formatted table (via e.g., knitr, DT, gt, etc.)"
  },
  {
    "objectID": "lectures/09-presentation.html#introducing-and-describing-methods",
    "href": "lectures/09-presentation.html#introducing-and-describing-methods",
    "title": "Presentations",
    "section": "Introducing and describing methods",
    "text": "Introducing and describing methods\n\nPrior to presenting results, you want to clearly state any transformations and methods used in the analysis\nYour presentation should provide the general steps for someone to replicate your work\n\ne.g., Used complete-linkage hierarchical clustering with [INSERT VARIABLES], determined \\(k\\) number of clusters by [INSERT REASON]\ne.g., Modeled [INSERT RESPONSE VARIABLE] as a function of [INSERT EXPLANATORY VARIABLES]\n\n\n\n\nFor more complicated methods, you’ll want to provide a brief review of the methodology\nIf introducing new methodology: walk through the steps clearly\n\n\n\n\nAlways justify your choice of methodology\n\nWhy you used a flexible tree-based model over linear regression"
  },
  {
    "objectID": "lectures/09-presentation.html#presenting-results",
    "href": "lectures/09-presentation.html#presenting-results",
    "title": "Presentations",
    "section": "Presenting results",
    "text": "Presenting results\n\nUse the assertion-evidence model\nAssertion: title of the slide should be the key takeaway in brief sentence form\n\nIndicates the point of the visualization or whatever means used to display the results\n\nEvidence: the body of the slide containing the results\n\nDisplay of the results in some format that is simple to explain and understand\n\n\n\n\nLimit the amount of text in your Evidence portion - brief statements with important context\nTreat the Assertion as the title of your Evidence\n\nPlot titles are then redundant and not necessary with an effective assertion"
  },
  {
    "objectID": "lectures/09-presentation.html#assertion",
    "href": "lectures/09-presentation.html#assertion",
    "title": "Presentations",
    "section": "Assertion",
    "text": "Assertion\n\n\n\nEvidence\n\n(e.g., plots, animations, tables, etc.)"
  },
  {
    "objectID": "lectures/09-presentation.html#mlb-strikeout-rates-have-been-increasing-throughout-history",
    "href": "lectures/09-presentation.html#mlb-strikeout-rates-have-been-increasing-throughout-history",
    "title": "Presentations",
    "section": "MLB strikeout rates have been increasing throughout history",
    "text": "MLB strikeout rates have been increasing throughout history\n\n\n(Explain the aes of your graph - what is each axis, color, shape, etc referring to? And what is the unit scale?)"
  },
  {
    "objectID": "lectures/09-presentation.html#discussion-and-ending-a-presentation",
    "href": "lectures/09-presentation.html#discussion-and-ending-a-presentation",
    "title": "Presentations",
    "section": "Discussion (and ending a presentation)",
    "text": "Discussion (and ending a presentation)\n\nConclude with a recap of the main points of your work\n\n\n\nThen point out limitations and indicate future directions\n\n\n\n\nEither end with the Discussion slide (or Acknowledgements but this is sometimes placed at the beginning)\nNever end a presentation with lone Thank you slide!\n\nWant the audience to focus on the final points in your Discussion slide\n\n\n\n\n\nInclude back-up Appendix slides with additional info, ready for questions\n\n\n\n\nSlides for References should not be displayed during your talk\n\nTheir purpose is just for sharing with others\nAlternative option: include references directly on slides either in text or via footnotes 1\n\n\n\nLike this…"
  },
  {
    "objectID": "lectures/09-presentation.html#additional-tips-and-reminders",
    "href": "lectures/09-presentation.html#additional-tips-and-reminders",
    "title": "Presentations",
    "section": "Additional tips and reminders",
    "text": "Additional tips and reminders\nUse pauses effectively to highlight points and explain steps\n\n\nShowing all of your text at once can overwhelm your audience\n\n\n\nBut\n\n\ndon’t\n\n\nbe\n\n\nridiculous\n\n\nRemember: memory overload is real!\n\nDo NOT introduce too much notation at once\nRepetitive language and usage of words are useful and reminders for the audience\n\nUse consistent language and terminology throughout the talk\n\n\n\n\nKnow your audience!"
  },
  {
    "objectID": "lectures/09-presentation.html#design-tips",
    "href": "lectures/09-presentation.html#design-tips",
    "title": "Presentations",
    "section": "Design tips",
    "text": "Design tips\n\nLess ink, less ink, less ink\nYour plot should be big enough (font size, line width, point size, etc.)\nReformat variable names (don’t show the original names in the data)\nIf a table can be represented by a figure, turn it into a figure (e.g., comparison of model evaluation metrics)"
  },
  {
    "objectID": "lectures/09-presentation.html#more-general-tips",
    "href": "lectures/09-presentation.html#more-general-tips",
    "title": "Presentations",
    "section": "More general tips",
    "text": "More general tips\n\nYour slides should support what you say, not create interference between speech and vision brain areas – your slides serves as a summary of what you said so someone who may have been distracted can catch up quickly.\n\n\nDo not create your talk at the last minute (it will be obvious if you do). Designing good slides takes time. Practicing a talk takes time.\n\n\nAdvices from Andrew Patton:\n\n\nYou don’t need 17 decimal points for anything ever.\n\n\nMake sure your figures and charts, etc. all display the correct size. If I have to zoom in 27 times to read the legends that’s not going to help your case.\n\n\nPick better colors for your charts. Your submission should look expensive."
  },
  {
    "objectID": "lectures/09-presentation.html#resources-to-follow-along",
    "href": "lectures/09-presentation.html#resources-to-follow-along",
    "title": "Presentations",
    "section": "Resources to follow along",
    "text": "Resources to follow along\n\nQuarto presentation: quarto.org/docs/presentations\nrevealjs: quarto.org/docs/presentations/revealjs\n\n\n\n36-SURE.github.io"
  },
  {
    "objectID": "lectures/05-kmeans.html#statistical-learning",
    "href": "lectures/05-kmeans.html#statistical-learning",
    "title": "Unsupervised learning: \\(k\\)-means clustering",
    "section": "Statistical learning",
    "text": "Statistical learning\n\nStatistical learning refers to a set of tools for making sense of complex datasets. — Preface of ISLR\n\n\nGeneral setup: Given a dataset of \\(p\\) variables (columns) and \\(n\\) observations (rows) \\(x_1,\\dots,x_n\\).\nFor observation \\(i\\), \\[x_{i1},x_{i2},\\ldots,x_{ip} \\sim P \\,,\\] where \\(P\\) is a \\(p\\)-dimensional distribution that we might not know much about a priori"
  },
  {
    "objectID": "lectures/05-kmeans.html#statistical-learning-vs.-machine-learning",
    "href": "lectures/05-kmeans.html#statistical-learning-vs.-machine-learning",
    "title": "Unsupervised learning: \\(k\\)-means clustering",
    "section": "Statistical learning vs. machine learning???",
    "text": "Statistical learning vs. machine learning???\n\nPlenty of overlap - both fields focus on supervised and unsupervised problems\n\nMachine learning has a greater emphasis on large scale applications and prediction accuracy\nStatistical learning emphasizes models and their interpretability, and the assessment of uncertainty\n\nThe distinction has become more and more blurred, and there is a great deal of “cross-fertilization”\n“Statistical machine learning”???"
  },
  {
    "objectID": "lectures/05-kmeans.html#supervised-learning",
    "href": "lectures/05-kmeans.html#supervised-learning",
    "title": "Unsupervised learning: \\(k\\)-means clustering",
    "section": "Supervised learning",
    "text": "Supervised learning\n\nResponse variable \\(Y\\) in one of the \\(p\\) variables (columns)\nThe remaining \\(p-1\\) variables are predictor measurements \\(X\\)\n\nRegression: \\(Y\\) is quantitative\nClassification: \\(Y\\) is categorical\n\n\n\nObjective: Given training data \\((x_1, y_1), ..., (x_n, y_n)\\)\n\nAccurately predict unseen test cases\nUnderstand which features affect the response (and how)\nAssess the quality of our predictions and inferences"
  },
  {
    "objectID": "lectures/05-kmeans.html#unsupervised-learning",
    "href": "lectures/05-kmeans.html#unsupervised-learning",
    "title": "Unsupervised learning: \\(k\\)-means clustering",
    "section": "Unsupervised learning",
    "text": "Unsupervised learning\n\nNo response variable (i.e. data are not labeled)\nOnly given a set of features measured on a set of observations\nObjective: understand the variation and grouping structure of a set of unlabeled data\n\ne.g., discover subgroups among the variables or among the observations\n\nDifficult to know how “good” you are doing"
  },
  {
    "objectID": "lectures/05-kmeans.html#unsupervised-learning-1",
    "href": "lectures/05-kmeans.html#unsupervised-learning-1",
    "title": "Unsupervised learning: \\(k\\)-means clustering",
    "section": "Unsupervised learning",
    "text": "Unsupervised learning\n\nIt is often easier to obtain unlabeled data than labeled data\nUnsupervised learning is more subjective than supervised learning\nThere is no simple goal for the analysis, such as prediction of a response\nUnsupervised learning can be useful as a pre-processing step for supervised learning\n\nThink of unsupervised learning as an extension of EDA - there’s no unique right answer!"
  },
  {
    "objectID": "lectures/05-kmeans.html#clustering-cluster-analysis",
    "href": "lectures/05-kmeans.html#clustering-cluster-analysis",
    "title": "Unsupervised learning: \\(k\\)-means clustering",
    "section": "Clustering (cluster analysis)",
    "text": "Clustering (cluster analysis)\n\n\nClustering refers to a very broad set of techniques for finding subgroups, or clusters, in a data set — ISLR\n\n\n\nGoals: partition of the observations into distinct clusters so that\n\nobservations within clusters are more similar to each other\nobservations in different clusters are more different from each other\n\n\n\n\nThis often involves domain-specific considerations based on knowledge of the data being studied"
  },
  {
    "objectID": "lectures/05-kmeans.html#distance-between-observations",
    "href": "lectures/05-kmeans.html#distance-between-observations",
    "title": "Unsupervised learning: \\(k\\)-means clustering",
    "section": "Distance between observations",
    "text": "Distance between observations\n\nWhat does it means for two or more observations to be similar or different?\n\n\n\nThis require characterizing the distance between observations\n\nClusters: groups of observations that are “close” together\n\n\n\n\n\nThis is easy to do for 2 quantitative variables: just make a scatterplot\n\n\n\nBut how do we define “distance” beyond 2D data?\n\n\nLet \\(\\boldsymbol{x}_i = (x_{i1}, \\dots, x_{ip})\\) be a vector of \\(p\\) features for observation \\(i\\)\nQuestion of interest: How “far away” is \\(\\boldsymbol{x}_i\\) from \\(\\boldsymbol{x}_j\\)?\n\n\nWhen looking at a scatterplot, we’re using Euclidean distance \\[d(\\boldsymbol{x}_i, \\boldsymbol{x}_j) = \\sqrt{(x_{i1} - x_{j1})^2 + \\dots + (x_{ip} - x_{jp})^2}\\]"
  },
  {
    "objectID": "lectures/05-kmeans.html#distances-in-general",
    "href": "lectures/05-kmeans.html#distances-in-general",
    "title": "Unsupervised learning: \\(k\\)-means clustering",
    "section": "Distances in general",
    "text": "Distances in general\n\nThere’s a variety of different types of distance metrics: Manhattan, Mahalanobis, Cosine, Kullback-Leibler, Hellinger, Wasserstein\nWe’re just going to focus on Euclidean distance\n\n\n\nLet \\(d(\\boldsymbol{x}_i, \\boldsymbol{x}_j)\\) denote the pairwise distance between two observations \\(i\\) and \\(j\\)\n\n\nIdentity: \\(\\boldsymbol{x}_i = \\boldsymbol{x}_j \\Leftrightarrow d(\\boldsymbol{x}_i, \\boldsymbol{x}_j) = 0\\)\nNon-negativity: \\(d(\\boldsymbol{x}_i, \\boldsymbol{x}_j) \\geq 0\\)\nSymmetry: \\(d(\\boldsymbol{x}_i, \\boldsymbol{x}_j) = d(\\boldsymbol{x}_j, \\boldsymbol{x}_i)\\)\nTriangle inequality: \\(d(\\boldsymbol{x}_i, \\boldsymbol{x}_j) \\leq d(\\boldsymbol{x}_i, \\boldsymbol{x}_k) + d(\\boldsymbol{x}_k, \\boldsymbol{x}_j)\\)\n\n\n\n\n\nDistance Matrix: matrix \\(D\\) of all pairwise distances\n\n\\(D_{ij} = d(\\boldsymbol{x}_i, \\boldsymbol{x}_j)\\)\nwhere \\(D_{ii} = 0\\) and \\(D_{ij} = D_{ji}\\)\n\n\n\\[D = \\begin{pmatrix}\n                0 & D_{12} & \\cdots & D_{1n} \\\\\n                D_{21} & 0 & \\cdots & D_{2n} \\\\\n                \\vdots & \\vdots & \\ddots & \\vdots \\\\\n                D_{n1} & \\cdots & \\cdots & 0\n            \\end{pmatrix}\\]"
  },
  {
    "objectID": "lectures/05-kmeans.html#units-matter-in-clustering",
    "href": "lectures/05-kmeans.html#units-matter-in-clustering",
    "title": "Unsupervised learning: \\(k\\)-means clustering",
    "section": "Units matter in clustering",
    "text": "Units matter in clustering\n\nVariables are typically measured in different units\nOne variable may dominate others when computing Euclidean distance because its range is much larger\nScaling of the variables matters!\nStandardize each variable in the dataset to have mean 0 and standard deviation 1 with scale()"
  },
  {
    "objectID": "lectures/05-kmeans.html#k-means-clustering-1",
    "href": "lectures/05-kmeans.html#k-means-clustering-1",
    "title": "Unsupervised learning: \\(k\\)-means clustering",
    "section": "\\(k\\)-means clustering",
    "text": "\\(k\\)-means clustering\n\nGoal: partition the observations into a pre-specified number of clusters\n\n\n\nLet \\(C_1, \\dots, C_K\\) denote sets containing indices of observations in each of the \\(k\\) clusters\n\nif observation \\(i\\) is in cluster \\(k\\), then \\(i \\in C_k\\)\n\n\n\n\n\nWe want to minimize the within-cluster variation \\(W(C_k)\\) for each cluster \\(C_k\\) (i.e. the amount by which the observations within a cluster differ from each other)\nThis is equivalent to solving \\[\\underset{C_1, \\dots, C_K}{\\text{minimize }} \\Big\\{ \\sum_{k=1}^K W(C_k) \\Big\\}\\]\nIn other words, we want to partition the observations into \\(K\\) clusters such that the total within-cluster variation, summed over all K clusters, is as small as possible"
  },
  {
    "objectID": "lectures/05-kmeans.html#k-means-clustering-2",
    "href": "lectures/05-kmeans.html#k-means-clustering-2",
    "title": "Unsupervised learning: \\(k\\)-means clustering",
    "section": "\\(k\\)-means clustering",
    "text": "\\(k\\)-means clustering\nHow do we define within-cluster variation?\n\nUse the (squared) Euclidean distance \\[W(C_k) = \\frac{1}{|C_k|}\\sum_{i,j \\in C_k} d(x_i, x_j)^2 \\,,\\] where \\(|C_k|\\) denote the number of observations in cluster \\(k\\)\nCommonly referred to as the within-cluster sum of squares (WSS)\n\n\nSo how do we solve this?"
  },
  {
    "objectID": "lectures/05-kmeans.html#lloyds-algorithm",
    "href": "lectures/05-kmeans.html#lloyds-algorithm",
    "title": "Unsupervised learning: \\(k\\)-means clustering",
    "section": "Lloyd’s algorithm",
    "text": "Lloyd’s algorithm\n\n\n\nChoose \\(k\\) random centers, aka centroids\nAssign each observation closest center (using Euclidean distance)\nRepeat until cluster assignment stop changing:\n\n\nCompute new centroids as the averages of the updated groups\nReassign each observations to closest center\n\nConverges to a local optimum, not the global\nResults will change from run to run (set the seed!)\nTakes \\(k\\) as an input!"
  },
  {
    "objectID": "lectures/05-kmeans.html#gapminder-data",
    "href": "lectures/05-kmeans.html#gapminder-data",
    "title": "Unsupervised learning: \\(k\\)-means clustering",
    "section": "Gapminder data",
    "text": "Gapminder data\nHealth and income outcomes for 184 countries from 1960 to 2016 from the famous Gapminder project\n\nlibrary(tidyverse)\ntheme_set(theme_light())\nlibrary(dslabs)\nglimpse(gapminder)\n\nRows: 10,545\nColumns: 9\n$ country          &lt;fct&gt; \"Albania\", \"Algeria\", \"Angola\", \"Antigua and Barbuda\"…\n$ year             &lt;int&gt; 1960, 1960, 1960, 1960, 1960, 1960, 1960, 1960, 1960,…\n$ infant_mortality &lt;dbl&gt; 115.40, 148.20, 208.00, NA, 59.87, NA, NA, 20.30, 37.…\n$ life_expectancy  &lt;dbl&gt; 62.87, 47.50, 35.98, 62.97, 65.39, 66.86, 65.66, 70.8…\n$ fertility        &lt;dbl&gt; 6.19, 7.65, 7.32, 4.43, 3.11, 4.55, 4.82, 3.45, 2.70,…\n$ population       &lt;dbl&gt; 1636054, 11124892, 5270844, 54681, 20619075, 1867396,…\n$ gdp              &lt;dbl&gt; NA, 13828152297, NA, NA, 108322326649, NA, NA, 966778…\n$ continent        &lt;fct&gt; Europe, Africa, Africa, Americas, Americas, Asia, Ame…\n$ region           &lt;fct&gt; Southern Europe, Northern Africa, Middle Africa, Cari…"
  },
  {
    "objectID": "lectures/05-kmeans.html#gdp-is-severely-skewed-right",
    "href": "lectures/05-kmeans.html#gdp-is-severely-skewed-right",
    "title": "Unsupervised learning: \\(k\\)-means clustering",
    "section": "GDP is severely skewed right…",
    "text": "GDP is severely skewed right…\n\ngapminder |&gt; \n  ggplot(aes(x = gdp)) + \n  geom_histogram()"
  },
  {
    "objectID": "lectures/05-kmeans.html#some-initial-cleaning",
    "href": "lectures/05-kmeans.html#some-initial-cleaning",
    "title": "Unsupervised learning: \\(k\\)-means clustering",
    "section": "Some initial cleaning…",
    "text": "Some initial cleaning…\n\nEach row is at the country-year level\nFocus on data for 2011 where gdp is not missing\nLog-transform gdp\n\n\nclean_gapminder &lt;- gapminder |&gt;\n  filter(year == 2011, !is.na(gdp)) |&gt;\n  mutate(log_gdp = log(gdp))"
  },
  {
    "objectID": "lectures/05-kmeans.html#k-means-clustering-example-gdp-and-life_expectancy",
    "href": "lectures/05-kmeans.html#k-means-clustering-example-gdp-and-life_expectancy",
    "title": "Unsupervised learning: \\(k\\)-means clustering",
    "section": "\\(k\\)-means clustering example (gdp and life_expectancy)",
    "text": "\\(k\\)-means clustering example (gdp and life_expectancy)\n\n\n\nUse the kmeans() function, but must provide number of clusters \\(k\\)\n\n\ninit_kmeans &lt;- clean_gapminder |&gt; \n  select(log_gdp, life_expectancy) |&gt; \n  kmeans(algorithm = \"Lloyd\", centers = 4, nstart = 1)\n\nclean_gapminder |&gt;\n  mutate(\n    country_clusters = as.factor(init_kmeans$cluster)\n  ) |&gt;\n  ggplot(aes(x = log_gdp, y = life_expectancy,\n             color = country_clusters)) +\n  geom_point(size = 4) + \n  ggthemes::scale_color_colorblind() +\n  theme(legend.position = \"bottom\")"
  },
  {
    "objectID": "lectures/05-kmeans.html#careful-with-units",
    "href": "lectures/05-kmeans.html#careful-with-units",
    "title": "Unsupervised learning: \\(k\\)-means clustering",
    "section": "Careful with units…",
    "text": "Careful with units…\n\n\n\nUse coord_fixed() so that the axes match with unit scales\n\n\nclean_gapminder |&gt;\n  mutate(\n    country_clusters = as.factor(init_kmeans$cluster)\n  ) |&gt;\n  ggplot(aes(x = log_gdp, y = life_expectancy,\n             color = country_clusters)) +\n  geom_point(size = 4) + \n  ggthemes::scale_color_colorblind() +\n  theme(legend.position = \"bottom\") +\n  coord_fixed()"
  },
  {
    "objectID": "lectures/05-kmeans.html#standardize-the-variables",
    "href": "lectures/05-kmeans.html#standardize-the-variables",
    "title": "Unsupervised learning: \\(k\\)-means clustering",
    "section": "Standardize the variables!",
    "text": "Standardize the variables!\n\n\n\nUse the scale() function to first standardize the variables, \\(\\frac{\\text{value} - \\text{mean}}{\\text{sd}}\\)\n\n\nclean_gapminder &lt;- clean_gapminder |&gt;\n  mutate(\n    std_log_gdp = as.numeric(scale(log_gdp, center = TRUE, scale = TRUE)),\n    std_life_exp = as.numeric(scale(life_expectancy, center = TRUE, scale = TRUE))\n  )\n\nstd_kmeans &lt;- clean_gapminder |&gt; \n  select(std_log_gdp, std_life_exp) |&gt; \n  kmeans(algorithm = \"Lloyd\", centers = 4, nstart = 1)\n\nclean_gapminder |&gt;\n  mutate(\n    country_clusters = as.factor(std_kmeans$cluster)\n  ) |&gt;\n  ggplot(aes(x = log_gdp, y = life_expectancy,\n             color = country_clusters)) +\n  geom_point(size = 4) + \n  ggthemes::scale_color_colorblind() +\n  theme(legend.position = \"bottom\") +\n  coord_fixed()"
  },
  {
    "objectID": "lectures/05-kmeans.html#standardize-the-variables-1",
    "href": "lectures/05-kmeans.html#standardize-the-variables-1",
    "title": "Unsupervised learning: \\(k\\)-means clustering",
    "section": "Standardize the variables!",
    "text": "Standardize the variables!\n\n\n\nclean_gapminder |&gt;\n  mutate(\n    country_clusters = as.factor(std_kmeans$cluster)\n  ) |&gt;\n  ggplot(aes(x = std_log_gdp, y = std_life_exp,\n             color = country_clusters)) +\n  geom_point(size = 4) + \n  ggthemes::scale_color_colorblind() +\n  theme(legend.position = \"bottom\") +\n  coord_fixed()"
  },
  {
    "objectID": "lectures/05-kmeans.html#and-if-we-run-it-again",
    "href": "lectures/05-kmeans.html#and-if-we-run-it-again",
    "title": "Unsupervised learning: \\(k\\)-means clustering",
    "section": "And if we run it again?",
    "text": "And if we run it again?\n\n\nWe get different clustering results!\n\nanother_kmeans &lt;- clean_gapminder |&gt; \n  select(std_log_gdp, std_life_exp) |&gt; \n  kmeans(algorithm = \"Lloyd\", centers = 4, nstart = 1)\n\nclean_gapminder |&gt;\n  mutate(\n    country_clusters = as.factor(another_kmeans$cluster)\n  ) |&gt;\n  ggplot(aes(x = log_gdp, y = life_expectancy,\n             color = country_clusters)) +\n  geom_point(size = 4) + \n  ggthemes::scale_color_colorblind() +\n  theme(legend.position = \"bottom\")\n\nResults depend on initialization\nKeep in mind: the labels / colors are arbitrary"
  },
  {
    "objectID": "lectures/05-kmeans.html#fix-randomness-issue-with-nstart",
    "href": "lectures/05-kmeans.html#fix-randomness-issue-with-nstart",
    "title": "Unsupervised learning: \\(k\\)-means clustering",
    "section": "Fix randomness issue with nstart",
    "text": "Fix randomness issue with nstart\n\n\nRun the algorithm nstart times, then pick the results with lowest total within-cluster variation \\[\\text{total WSS} = \\sum_{k=1}^K W(C_k)\\]\n\nnstart_kmeans &lt;- clean_gapminder |&gt; \n  select(std_log_gdp, std_life_exp) |&gt; \n  kmeans(algorithm = \"Lloyd\", centers = 4, nstart = 30)\n\nclean_gapminder |&gt;\n  mutate(\n    country_clusters = as.factor(nstart_kmeans$cluster)\n  ) |&gt; \n  ggplot(aes(x = log_gdp, y = life_expectancy,\n             color = country_clusters)) +\n  geom_point(size = 4) + \n  ggthemes::scale_color_colorblind() +\n  theme(legend.position = \"bottom\")"
  },
  {
    "objectID": "lectures/05-kmeans.html#by-default-r-uses-hartiganwong-method",
    "href": "lectures/05-kmeans.html#by-default-r-uses-hartiganwong-method",
    "title": "Unsupervised learning: \\(k\\)-means clustering",
    "section": "By default R uses Hartigan–Wong method",
    "text": "By default R uses Hartigan–Wong method\n\n\nUpdates based on changing a single observation\nComputational advantages over re-computing distances for every observation\n\ndefault_kmeans &lt;- clean_gapminder |&gt; \n  select(std_log_gdp, std_life_exp) |&gt; \n  kmeans(algorithm = \"Hartigan-Wong\",\n         centers = 4, nstart = 30) \n\nclean_gapminder |&gt;\n  mutate(\n    country_clusters = as.factor(default_kmeans$cluster)\n  ) |&gt; \n  ggplot(aes(x = log_gdp, y = life_expectancy,\n             color = country_clusters)) +\n  geom_point(size = 4) + \n  ggthemes::scale_color_colorblind() +\n  theme(legend.position = \"bottom\")\n\nVery little differences for our purposes…"
  },
  {
    "objectID": "lectures/05-kmeans.html#better-alternative-to-nstart-k-means",
    "href": "lectures/05-kmeans.html#better-alternative-to-nstart-k-means",
    "title": "Unsupervised learning: \\(k\\)-means clustering",
    "section": "Better alternative to nstart: \\(k\\)-means++",
    "text": "Better alternative to nstart: \\(k\\)-means++\nObjective: initialize the cluster centers before proceeding with the standard \\(k\\)-means clustering algorithm\n\nIntuition:\n\nrandomly choose a data point the first cluster center\neach subsequent cluster center is chosen from the remaining data points with probability proportional to its squared distance from the point’s closest existing cluster center"
  },
  {
    "objectID": "lectures/05-kmeans.html#k-means",
    "href": "lectures/05-kmeans.html#k-means",
    "title": "Unsupervised learning: \\(k\\)-means clustering",
    "section": "\\(k\\)-means++",
    "text": "\\(k\\)-means++\nPick a random observation to be the center \\(c_1\\) of the first cluster \\(C_1\\)\n\nThis initializes a set \\(\\text{Centers} = \\{c_1 \\}\\)\n\n\nThen for each remaining cluster \\(c^* \\in 2, \\dots, K\\):\n\nFor each observation (that is not a center), compute \\(D(x_i) = \\underset{c \\in \\text{Centers}}{\\text{min}} d(x_i, c)\\)\n\nDistance between observation and its closest center \\(c \\in \\text{Centers}\\)\n\n\n\n\n\nRandomly pick a point \\(x_i\\) with probability: \\(\\displaystyle p_i = \\frac{D^2(x_i)}{\\sum_{j=1}^n D^2(x_j)}\\)\n\n\n\n\nAs distance to closest center increases, the probability of selection increases\nCall this randomly selected observation \\(c^*\\), update \\(\\text{Centers} = \\text{Centers} \\cup c^*\\)\n\n\n\nThen run \\(k\\)-means using these \\(\\text{Centers}\\) as the starting points"
  },
  {
    "objectID": "lectures/05-kmeans.html#k-means-in-r-using-flexclust",
    "href": "lectures/05-kmeans.html#k-means-in-r-using-flexclust",
    "title": "Unsupervised learning: \\(k\\)-means clustering",
    "section": "\\(k\\)-means++ in R using flexclust",
    "text": "\\(k\\)-means++ in R using flexclust\n\n\n\nlibrary(flexclust)\ninit_kmeanspp &lt;- clean_gapminder |&gt; \n  select(std_log_gdp, std_life_exp) |&gt; \n  kcca(k = 4, control = list(initcent = \"kmeanspp\"))\n\nclean_gapminder |&gt;\n  mutate(\n    country_clusters = as.factor(init_kmeanspp@cluster)\n  ) |&gt;\n  ggplot(aes(x = log_gdp, y = life_expectancy,\n             color = country_clusters)) +\n  geom_point(size = 4) + \n  ggthemes::scale_color_colorblind() +\n  theme(legend.position = \"bottom\")\n\nNote the use of @ instead of $…"
  },
  {
    "objectID": "lectures/05-kmeans.html#so-how-do-we-choose-the-number-of-clusters",
    "href": "lectures/05-kmeans.html#so-how-do-we-choose-the-number-of-clusters",
    "title": "Unsupervised learning: \\(k\\)-means clustering",
    "section": "So, how do we choose the number of clusters?",
    "text": "So, how do we choose the number of clusters?"
  },
  {
    "objectID": "lectures/05-kmeans.html#so-how-do-we-choose-the-number-of-clusters-1",
    "href": "lectures/05-kmeans.html#so-how-do-we-choose-the-number-of-clusters-1",
    "title": "Unsupervised learning: \\(k\\)-means clustering",
    "section": "So, how do we choose the number of clusters?",
    "text": "So, how do we choose the number of clusters?\nThere is no universally accepted way to conclude that a particular choice of \\(k\\) is optimal!\nFrom Cosma Shalizi’s notes\n\nOne reason you should be intensely skeptical of clustering results — including your own! — is that there is currently very little theory about how to find the right number of clusters. It’s not even completely clear what “the right number of clusters” means!\n\nAdditional readings: here and here"
  },
  {
    "objectID": "lectures/05-kmeans.html#popular-heuristic-elbow-plot-use-with-caution",
    "href": "lectures/05-kmeans.html#popular-heuristic-elbow-plot-use-with-caution",
    "title": "Unsupervised learning: \\(k\\)-means clustering",
    "section": "Popular heuristic: elbow plot (use with caution)",
    "text": "Popular heuristic: elbow plot (use with caution)\nLook at the total within-cluster variation as a function of the number of clusters\n\n\n\n# function to perform clustering for each value of k\ngapminder_kmeans &lt;- function(k) {\n  \n  kmeans_results &lt;- clean_gapminder |&gt;\n    select(std_log_gdp, std_life_exp) |&gt;\n    kmeans(centers = k, nstart = 30)\n  \n  kmeans_out &lt;- tibble(\n    clusters = k,\n    total_wss = kmeans_results$tot.withinss\n  )\n  return(kmeans_out)\n}\n\n\n\n# number of clusters to search over\nn_clusters_search &lt;- 2:12\n\n# iterate over each k to compute total wss\nkmeans_search &lt;- n_clusters_search |&gt; \n  map(gapminder_kmeans) |&gt; \n  bind_rows()\n\nkmeans_search |&gt; \n  ggplot(aes(x = clusters, y = total_wss)) +\n  geom_line() + \n  geom_point(size = 4) +\n  scale_x_continuous(breaks = n_clusters_search)"
  },
  {
    "objectID": "lectures/05-kmeans.html#popular-heuristic-elbow-plot-use-with-caution-1",
    "href": "lectures/05-kmeans.html#popular-heuristic-elbow-plot-use-with-caution-1",
    "title": "Unsupervised learning: \\(k\\)-means clustering",
    "section": "Popular heuristic: elbow plot (use with caution)",
    "text": "Popular heuristic: elbow plot (use with caution)\n\n\nChoose \\(k\\) where marginal improvements is low at the bend (hence the elbow)\nThis is just a guideline and should not dictate your choice of \\(k\\)\nGap statistic is a popular choice (see clusGap function in cluster package)\nLater on: model-based approach to choosing the number of clusters"
  },
  {
    "objectID": "lectures/05-kmeans.html#appendix-elbow-plot-with-flexclust",
    "href": "lectures/05-kmeans.html#appendix-elbow-plot-with-flexclust",
    "title": "Unsupervised learning: \\(k\\)-means clustering",
    "section": "Appendix: elbow plot with flexclust",
    "text": "Appendix: elbow plot with flexclust\n\n\n\ngapminder_kmpp &lt;- function(k) {\n  \n  kmeans_results &lt;- clean_gapminder |&gt;\n    select(std_log_gdp, std_life_exp) |&gt;\n    kcca(k = k, control = list(initcent = \"kmeanspp\"))\n  \n  kmeans_out &lt;- tibble(\n    clusters = k,\n    total_wss = sum(kmeans_results@clusinfo$size * \n                      kmeans_results@clusinfo$av_dist)\n  )\n  return(kmeans_out)\n}\n\nn_clusters_search &lt;- 2:12\nkmpp_search &lt;- n_clusters_search |&gt; \n  map(gapminder_kmpp) |&gt; \n  bind_rows()\nkmpp_search |&gt; \n  ggplot(aes(x = clusters, y = total_wss)) +\n  geom_line() + \n  geom_point(size = 4) +\n  scale_x_continuous(breaks = n_clusters_search)"
  },
  {
    "objectID": "lectures/05-kmeans.html#appendix-k-means-for-image-segmentation-and-compression",
    "href": "lectures/05-kmeans.html#appendix-k-means-for-image-segmentation-and-compression",
    "title": "Unsupervised learning: \\(k\\)-means clustering",
    "section": "Appendix: \\(k\\)-means for image segmentation and compression",
    "text": "Appendix: \\(k\\)-means for image segmentation and compression\nGoal: partition an image into multiple segments, where each segment typically represents an object in the image\n\nTreat each pixel in the image as a point in 3-dimensional space comprising the intensities of the (red, blue, green) channels\nTreat each pixel in the image as a separate data point\nApply \\(k\\)-means clustering and identify the clusters\nAll the pixels belonging to a cluster are treated as a segment in the image\n\nThen for any \\(k\\), reconstruct the image by replacing each pixel vector with the (red, blue, green) triplet given by the center to which that pixel has been assigned"
  },
  {
    "objectID": "lectures/05-kmeans.html#appendix-k-means-for-image-segmentation-and-compression-1",
    "href": "lectures/05-kmeans.html#appendix-k-means-for-image-segmentation-and-compression-1",
    "title": "Unsupervised learning: \\(k\\)-means clustering",
    "section": "Appendix: \\(k\\)-means for image segmentation and compression",
    "text": "Appendix: \\(k\\)-means for image segmentation and compression\n\n# https://raw.githubusercontent.com/36-SURE/36-SURE.github.io/main/data/spongebob.jpeg\nset.seed(2)\nlibrary(jpeg)\nimg_raw &lt;- readJPEG(\"../data/spongebob.jpeg\")\nimg_width &lt;- dim(img_raw)[1]\nimg_height &lt;- dim(img_raw)[2]\nimg_tbl &lt;- tibble(x = rep(1:img_height, each = img_width),\n                  y = rep(img_width:1, img_height),\n                  r = as.vector(img_raw[, , 1]),\n                  g = as.vector(img_raw[, , 2]),\n                  b = as.vector(img_raw[, , 3]))\n\n\nimg_kmpp &lt;- function(k) {\n  km_fit &lt;- img_tbl |&gt;\n    select(r, g, b) |&gt;\n    kcca(k = k, control = list(initcent = \"kmeanspp\"))\n  km_col &lt;- rgb(km_fit@centers[km_fit@cluster,])\n  return(km_col)\n}\n\n\nimg_tbl &lt;- img_tbl |&gt;\n  mutate(original = rgb(r, g, b),\n         k2 = img_kmpp(2),\n         k4 = img_kmpp(4),\n         k8 = img_kmpp(8),\n         k11 = img_kmpp(11)) |&gt;\n  pivot_longer(original:k11, names_to = \"k\", values_to = \"hex\") |&gt;\n  mutate(k = factor(k, levels = c(\"original\", \"k2\", \"k4\", \"k8\", \"k11\")))"
  },
  {
    "objectID": "lectures/05-kmeans.html#appendix-k-means-for-image-segmentation-and-compression-2",
    "href": "lectures/05-kmeans.html#appendix-k-means-for-image-segmentation-and-compression-2",
    "title": "Unsupervised learning: \\(k\\)-means clustering",
    "section": "Appendix: \\(k\\)-means for image segmentation and compression",
    "text": "Appendix: \\(k\\)-means for image segmentation and compression\n\nimg_tbl |&gt; \n  ggplot(aes(x, y, color = hex)) +\n  geom_point() +\n  scale_color_identity() +\n  facet_wrap(~ k, nrow = 1) +\n  coord_fixed() +\n  theme_classic()\n\n\n\n\n\n\n\n\n\n\n\n\n36-SURE.github.io"
  },
  {
    "objectID": "lectures/10-tradeoffs.html#concepts-that-hopefully-youll-be-able-to-distinguish",
    "href": "lectures/10-tradeoffs.html#concepts-that-hopefully-youll-be-able-to-distinguish",
    "title": "Supervised learning: the tradeoffs",
    "section": "Concepts that hopefully you’ll be able to distinguish",
    "text": "Concepts that hopefully you’ll be able to distinguish\n\nSupervised vs. unsupervised learning\nClassification vs. regression\nClassification vs. clustering\nExplanatory vs. response variable\nInference vs. prediction\nFlexibility-interpretability tradeoff\nBias-variance tradeoff\nModel assessment vs. model selection\nParametric vs. nonparametric models"
  },
  {
    "objectID": "lectures/10-tradeoffs.html#statistical-learning",
    "href": "lectures/10-tradeoffs.html#statistical-learning",
    "title": "Supervised learning: the tradeoffs",
    "section": "Statistical learning",
    "text": "Statistical learning\n\nStatistical learning refers to a set of tools for making sense of complex datasets. — Preface of ISLR\n\n\nGeneral setup: Given a dataset of \\(p\\) variables (columns) and \\(n\\) observations (rows) \\(x_1,\\dots,x_n\\).\nFor observation \\(i\\), \\[x_{i1},x_{i2},\\ldots,x_{ip} \\sim P \\,,\\] where \\(P\\) is a \\(p\\)-dimensional distribution that we might not know much about a priori"
  },
  {
    "objectID": "lectures/10-tradeoffs.html#supervised-learning",
    "href": "lectures/10-tradeoffs.html#supervised-learning",
    "title": "Supervised learning: the tradeoffs",
    "section": "Supervised learning",
    "text": "Supervised learning\n\nResponse variable \\(Y\\) in one of the \\(p\\) variables (columns)\nThe remaining \\(p-1\\) variables are predictor measurements \\(X\\)\n\nRegression: \\(Y\\) is quantitative\nClassification: \\(Y\\) is categorical\n\n\n\nGoal: uncover associations between a set of predictor (independent / explanatory) variables / features and a single response (or dependent) variable\n\nAccurately predict unseen test cases\nUnderstand which features affect the response (and how)\nAssess the quality of our predictions and inferences"
  },
  {
    "objectID": "lectures/10-tradeoffs.html#theyre-all-the-same",
    "href": "lectures/10-tradeoffs.html#theyre-all-the-same",
    "title": "Supervised learning: the tradeoffs",
    "section": "They’re all the same",
    "text": "They’re all the same"
  },
  {
    "objectID": "lectures/10-tradeoffs.html#supervised-learning-1",
    "href": "lectures/10-tradeoffs.html#supervised-learning-1",
    "title": "Supervised learning: the tradeoffs",
    "section": "Supervised learning",
    "text": "Supervised learning\nExamples\n\nIdentify the risk factors for prostate cancer\n\n\n\nPredict whether someone will have a heart attack based on demographic, diet, and clinical measurements\n\n\n\n\nPredict a player’s batting average in year \\(t+1\\) using their batting average in year \\(t\\) and uncover meaningful relationships between other measurements and batting average in year \\(t+1\\)\n\n\n\n\nGiven NFL player tracking data which contain 2D coordinates of every player on the field at every tenth of the second, predict how far a ball-carrier will go at any given moment within a play"
  },
  {
    "objectID": "lectures/10-tradeoffs.html#examples-of-statistical-learning-methods-algorithms",
    "href": "lectures/10-tradeoffs.html#examples-of-statistical-learning-methods-algorithms",
    "title": "Supervised learning: the tradeoffs",
    "section": "Examples of statistical learning methods / algorithms",
    "text": "Examples of statistical learning methods / algorithms\nYou are probably already familiar with statistical learning - even if you did not know what the phrase meant before\n\nExamples of statistical learning algorithms include:\n\nGeneralized linear models (GLMs) and penalized versions (lasso, ridge, elastic net)\nSmoothing splines, generalized additive models (GAMs)\nDecision trees and its variants (e.g., random forests, boosting)\nNeural networks\n\n\n\nTwo main types of problems\n\nRegression models: estimate average value of response (i.e. the response is quantitative)\nClassification models: determine the most likely class of a set of discrete response variable classes (i.e. the response is categorical)"
  },
  {
    "objectID": "lectures/10-tradeoffs.html#which-method-should-i-use-in-my-analysis",
    "href": "lectures/10-tradeoffs.html#which-method-should-i-use-in-my-analysis",
    "title": "Supervised learning: the tradeoffs",
    "section": "Which method should I use in my analysis?",
    "text": "Which method should I use in my analysis?\n\nIT DEPENDS - the big picture: inference vs prediction\nLet \\(Y\\) be the response variable, and \\(X\\) be the predictors, then the learned model will take the form:\n\\[\n\\hat{Y}=\\hat{f}(X)\n\\]\n\n\n\nCare about the details of \\(\\hat{f}(X)\\)? \\(\\longrightarrow\\) inference\nFine with treating \\(\\hat{f}(X)\\) as a obscure/mystical machine? \\(\\longrightarrow\\) prediction\n\n\n\nAny algorithm can be used for prediction, however options are limited for inference\nActive area of research on using more mystical models for statistical inference"
  },
  {
    "objectID": "lectures/10-tradeoffs.html#some-tradeoffs",
    "href": "lectures/10-tradeoffs.html#some-tradeoffs",
    "title": "Supervised learning: the tradeoffs",
    "section": "Some tradeoffs",
    "text": "Some tradeoffs\n\nPrediction accuracy vs interpretability\n\nLinear models are easy to interpret; boosted trees are not\n\n\n\n\nGood fit vs overfit or underfit\n\nHow do we know when the fit is just right?\n\n\n\n\n\nParsimony versus black-box\n\nWe often prefer a simpler model involving fewer variables over a black-box involving more (or all) predictors"
  },
  {
    "objectID": "lectures/10-tradeoffs.html#model-flexibility-vs-interpretability",
    "href": "lectures/10-tradeoffs.html#model-flexibility-vs-interpretability",
    "title": "Supervised learning: the tradeoffs",
    "section": "Model flexibility vs interpretability",
    "text": "Model flexibility vs interpretability\nGenerally speaking: tradeoff between a model’s flexibility (i.e. how “wiggly” it is) and how interpretable it is\n\nThe simpler parametric form of the model, the easier it is to interpret\nHence why linear regression is popular in practice\n\n\n\n\n\n\n\n\n\n\n\n\n[ISLR Figure 2.7]"
  },
  {
    "objectID": "lectures/10-tradeoffs.html#model-flexibility-vs-interpretability-1",
    "href": "lectures/10-tradeoffs.html#model-flexibility-vs-interpretability-1",
    "title": "Supervised learning: the tradeoffs",
    "section": "Model flexibility vs interpretability",
    "text": "Model flexibility vs interpretability\n\n\nParametric models, for which we can write down a mathematical expression for \\(f(X)\\) before observing the data, a priori (e.g. linear regression), are inherently less flexible\n\n\n\nNonparametric models, in which \\(f(X)\\) is estimated from the data (e.g. kernel regression)"
  },
  {
    "objectID": "lectures/10-tradeoffs.html#model-flexibility-vs-interpretability-2",
    "href": "lectures/10-tradeoffs.html#model-flexibility-vs-interpretability-2",
    "title": "Supervised learning: the tradeoffs",
    "section": "Model flexibility vs interpretability",
    "text": "Model flexibility vs interpretability\n\n\nIf your goal is prediction, your model can be as arbitrarily flexible as it needs to be\nWe’ll discuss how to estimate the optimal amount of flexibility shortly…"
  },
  {
    "objectID": "lectures/10-tradeoffs.html#looks-about-right",
    "href": "lectures/10-tradeoffs.html#looks-about-right",
    "title": "Supervised learning: the tradeoffs",
    "section": "Looks about right…",
    "text": "Looks about right…"
  },
  {
    "objectID": "lectures/10-tradeoffs.html#model-assessment-vs-selection-whats-the-difference",
    "href": "lectures/10-tradeoffs.html#model-assessment-vs-selection-whats-the-difference",
    "title": "Supervised learning: the tradeoffs",
    "section": "Model assessment vs selection, what’s the difference?",
    "text": "Model assessment vs selection, what’s the difference?\n\nModel assessment:\n\nevaluating how well a learned model performs, via the use of a single-number metric\n\n\n\nModel selection:\n\nselecting the best model from a suite of learned models (e.g., linear regression, random forest, etc.)"
  },
  {
    "objectID": "lectures/10-tradeoffs.html#model-flexibility-islr-figure-2.9",
    "href": "lectures/10-tradeoffs.html#model-flexibility-islr-figure-2.9",
    "title": "Supervised learning: the tradeoffs",
    "section": "Model flexibility (ISLR Figure 2.9)",
    "text": "Model flexibility (ISLR Figure 2.9)\n\n\nLeft panel: intuitive notion of the meaning of model flexibility\nData are generated from a smoothly varying non-linear model (shown in black), with random noise added: \\[\nY = f(X) + \\epsilon\n\\]"
  },
  {
    "objectID": "lectures/10-tradeoffs.html#model-flexibility",
    "href": "lectures/10-tradeoffs.html#model-flexibility",
    "title": "Supervised learning: the tradeoffs",
    "section": "Model flexibility",
    "text": "Model flexibility\n\nOrange line: an inflexible, fully parametrized model (simple linear regression)\n\n\nCannot provide a good estimate of \\(f(X)\\)\n\n\n\n\nCannot overfit by modeling the noisy deviations of the data from \\(f(X)\\)"
  },
  {
    "objectID": "lectures/10-tradeoffs.html#model-flexibility-1",
    "href": "lectures/10-tradeoffs.html#model-flexibility-1",
    "title": "Supervised learning: the tradeoffs",
    "section": "Model flexibility",
    "text": "Model flexibility\n\nGreen line: an overly flexible, nonparametric model\n\n\nIt can provide a good estimate of \\(f(X)\\)\n\n\n\n\n… BUT it goes too far and overfits by modeling the noise\n\n\n\nThis is NOT generalizable: bad job of predicting response given new data NOT used in learning the model"
  },
  {
    "objectID": "lectures/10-tradeoffs.html#so-how-do-we-deal-with-flexibility",
    "href": "lectures/10-tradeoffs.html#so-how-do-we-deal-with-flexibility",
    "title": "Supervised learning: the tradeoffs",
    "section": "So… how do we deal with flexibility?",
    "text": "So… how do we deal with flexibility?\nGOAL: We want to learn a statistical model that provides a good estimate of \\(f(X)\\) without overfitting\n\nThere are two common approaches:\n\nWe can split the data into two groups:\n\ntraining data: data used to train models,\ntest data: data used to test them\nBy assessing models using “held-out” test set data, we act to ensure that we get a generalizable(!) estimate of \\(f(X)\\)\n\n\n\n\n\nWe can repeat data splitting \\(k\\) times:\n\nEach observation is placed in the “held-out” / test data exactly once\nThis is called k-fold cross validation (typically set \\(k\\) to 5 or 10)\n\n\n\n\n\\(k\\)-fold cross validation is the preferred approach, but the tradeoff is that CV analyses take \\({\\sim}k\\) times longer than analyses that utilize data splitting"
  },
  {
    "objectID": "lectures/10-tradeoffs.html#model-assessment",
    "href": "lectures/10-tradeoffs.html#model-assessment",
    "title": "Supervised learning: the tradeoffs",
    "section": "Model assessment",
    "text": "Model assessment\n\n\nRight panel shows a metric of model assessment, the mean squared error (MSE) as a function of flexibility for both a training and test datasets\n\n\n\nTraining error (gray line) decreases as flexibility increases\n\n\n\n\nTest error (red line) decreases while flexibility increases until the point a good estimate of \\(f(X)\\) is reached, and then it increases as it overfits to the training data"
  },
  {
    "objectID": "lectures/10-tradeoffs.html#model-assessment-metrics",
    "href": "lectures/10-tradeoffs.html#model-assessment-metrics",
    "title": "Supervised learning: the tradeoffs",
    "section": "Model assessment metrics",
    "text": "Model assessment metrics\nLoss function (aka objective or cost function) is a metric that represents the quality of fit of a model\n\nFor regression we typically use mean squared error (MSE) - quadratic loss: squared differences between model predictions \\(\\hat{f}(X)\\) and observed data \\(Y\\)\n\\[\\text{MSE} = \\frac{1}{n} \\sum_i^n (Y_i - \\hat{f}(X_i))^2\\]\n\n\nFor classification, the situation is not quite so clear-cut\n\nmisclassification rate: percentage of predictions that are wrong\narea under the ROC curve (AUC)\n\n\n\n\ninterpretation can be affected by class imbalance:\n\nif two classes are equally represented in a dataset, a misclassification rate of 2% is good\nbut if one class comprises 99% of the data, that 2% is no longer such a good result…"
  },
  {
    "objectID": "lectures/10-tradeoffs.html#back-to-model-selection",
    "href": "lectures/10-tradeoffs.html#back-to-model-selection",
    "title": "Supervised learning: the tradeoffs",
    "section": "Back to model selection",
    "text": "Back to model selection\nModel selection: picking the best model from a suite of possible models\n\nExample: Picking the best regression model based on MSE, or best classification model based on misclassification rate\n\n\nTwo things to keep in mind:\n\nEnsure an apples-to-apples comparison of metrics\n\n\nevery model should be learned using the same training and test data\nDo not resample the data between the time when you, e.g., perform linear regression and vs you perform random forest\n\n\n\n\nAn assessment metric is a random variable, i.e., if you choose different data to be in your training set, the metric will be different.\n\n\n\nFor regression, a third point should be kept in mind: a metric like the MSE is unit-dependent\n\nan MSE of 0.001 in one analysis context is not necessarily better or worse than an MSE of 100 in another context"
  },
  {
    "objectID": "lectures/10-tradeoffs.html#an-example-true-model",
    "href": "lectures/10-tradeoffs.html#an-example-true-model",
    "title": "Supervised learning: the tradeoffs",
    "section": "An example true model",
    "text": "An example true model"
  },
  {
    "objectID": "lectures/10-tradeoffs.html#the-repeated-experiments",
    "href": "lectures/10-tradeoffs.html#the-repeated-experiments",
    "title": "Supervised learning: the tradeoffs",
    "section": "The repeated experiments…",
    "text": "The repeated experiments…"
  },
  {
    "objectID": "lectures/10-tradeoffs.html#the-linear-regression-fits",
    "href": "lectures/10-tradeoffs.html#the-linear-regression-fits",
    "title": "Supervised learning: the tradeoffs",
    "section": "The linear regression fits",
    "text": "The linear regression fits\n\nLook at the plots. For any given value of \\(x\\):\n\nThe average estimated \\(y\\) value is offset from the truth (high bias)\nThe dispersion (variance) in the estimated \\(y\\) values is relatively small (low variance)"
  },
  {
    "objectID": "lectures/10-tradeoffs.html#the-spline-fits",
    "href": "lectures/10-tradeoffs.html#the-spline-fits",
    "title": "Supervised learning: the tradeoffs",
    "section": "The spline fits",
    "text": "The spline fits\n\nLook at the plots. For any given value of \\(x\\):\n\n\nThe average estimated \\(y\\) value approximately matches the truth (low bias)\nThe dispersion (variance) in the estimated \\(y\\) values is relatively large (high variance)"
  },
  {
    "objectID": "lectures/10-tradeoffs.html#bias-variance-tradeoff",
    "href": "lectures/10-tradeoffs.html#bias-variance-tradeoff",
    "title": "Supervised learning: the tradeoffs",
    "section": "Bias-variance tradeoff",
    "text": "Bias-variance tradeoff\n“Best” model minimizes the test-set MSE, where the true MSE can be decomposed into \\({\\rm MSE} = {\\rm (Bias)}^2 + {\\rm Variance}\\)\n\n\n\n\n\n\n\n\n\n\nTowards the left: high bias, low variance. Towards the right: low bias, high variance.\nOptimal amount of flexibility lies somewhere in the middle (“just the right amount” — Goldilocks principle)"
  },
  {
    "objectID": "lectures/10-tradeoffs.html#the-sweet-spot",
    "href": "lectures/10-tradeoffs.html#the-sweet-spot",
    "title": "Supervised learning: the tradeoffs",
    "section": "“The sweet spot”",
    "text": "“The sweet spot”\n\n\nRemember, when building predictive models, try to find the sweet spot between bias and variance. #babybear\n\n— Dr. G. J. Matthews (@StatsClass) September 6, 2018"
  },
  {
    "objectID": "lectures/10-tradeoffs.html#principle-of-parsimony-occams-razor",
    "href": "lectures/10-tradeoffs.html#principle-of-parsimony-occams-razor",
    "title": "Supervised learning: the tradeoffs",
    "section": "Principle of parsimony (Occam’s razor)",
    "text": "Principle of parsimony (Occam’s razor)\n“Numquam ponenda est pluralitas sine necessitate” (plurality must never be posited without necessity)\nFrom ISLR:\n\nWhen faced with new data modeling and prediction problems, it’s tempting to always go for the trendy new methods. Often they give extremely impressive results, especially when the datasets are very large and can support the fitting of high-dimensional nonlinear models. However, if we can produce models with the simpler tools that perform as well, they are likely to be easier to fit and understand, and potentially less fragile than the more complex approaches. Wherever possible, it makes sense to try the simpler models as well, and then make a choice based on the performance/complexity tradeoff.\n\nIn short: “When faced with several methods that give roughly equivalent performance, pick the simplest.”"
  },
  {
    "objectID": "lectures/10-tradeoffs.html#the-curse-of-dimensionality",
    "href": "lectures/10-tradeoffs.html#the-curse-of-dimensionality",
    "title": "Supervised learning: the tradeoffs",
    "section": "The curse of dimensionality",
    "text": "The curse of dimensionality\nThe more features, the merrier?… Not quite\n\nAdding additional signal features that are truly associated with the response will improve the ftted model\n\nReduction in test set error\n\nAdding noise features that are not truly associated with the response will lead to a deterioration in the model\n\nIncrease in test set error\n\n\nNoise features increase the dimensionality of the problem, increasing the risk of overfitting"
  },
  {
    "objectID": "lectures/10-tradeoffs.html#check-out-this-song",
    "href": "lectures/10-tradeoffs.html#check-out-this-song",
    "title": "Supervised learning: the tradeoffs",
    "section": "Check out this song",
    "text": "Check out this song\n\n\n\n36-SURE.github.io"
  },
  {
    "objectID": "lectures/14-logistic.html#classification",
    "href": "lectures/14-logistic.html#classification",
    "title": "Supervised learning: logistic regression",
    "section": "Classification",
    "text": "Classification\n\nCategorical variables take values in an unordered set with different categories\nClassification: predicting a categorical response \\(Y\\) for an observation (since it involves assigning the observation to a category/class)\nOften, we are more interested in estimating the probability for each category of the response \\(Y\\)\nClassification is different from clustering, since we know the true label \\(Y\\)\nWe will focus on binary categorical response (categorical response variables with only two categories)\n\nExamples: whether it will rain or not tomorrow, whether the kicker makes or misses a field goal attempt"
  },
  {
    "objectID": "lectures/14-logistic.html#logistic-regression",
    "href": "lectures/14-logistic.html#logistic-regression",
    "title": "Supervised learning: logistic regression",
    "section": "Logistic regression",
    "text": "Logistic regression\n\nReview: in linear regression, the response variable \\(Y\\) is continuous\nLogistic regression is used to model binary outcomes (i.e. \\(Y \\in \\{0, 1\\}\\))\nMany concepts are similar to the linear regression\n\ntests for coefficients\nindicator variables\nmodel selection techniques\n\nOther concepts are different\n\ninterpretation of coefficients\nresiduals are not normally distributed\nmodel evaluation"
  },
  {
    "objectID": "lectures/14-logistic.html#why-not-linear-regression-for-categorical-data",
    "href": "lectures/14-logistic.html#why-not-linear-regression-for-categorical-data",
    "title": "Supervised learning: logistic regression",
    "section": "Why not linear regression for categorical data?",
    "text": "Why not linear regression for categorical data?\nFrom ISLR:\n\n\n\n\nLinear regression\npredicted probabilities might be outside the \\([0, 1]\\) interval\n\nLogistic regression\nall probabilities lie between 0 and 1"
  },
  {
    "objectID": "lectures/14-logistic.html#logistic-regression-1",
    "href": "lectures/14-logistic.html#logistic-regression-1",
    "title": "Supervised learning: logistic regression",
    "section": "Logistic regression",
    "text": "Logistic regression\n\nLogistic regression models the probability of success \\(p(x)\\) of a binary response variable \\(Y \\in \\{0, 1\\}\\)\nTo limit the regression between \\([0, 1]\\), we use the logit function, or the log-odds ratio\n\n\\[\n\\log \\left( \\frac{p(x)}{1 - p(x)} \\right) = \\beta_0 + \\beta_1 x_1 + \\cdots + \\beta_p x_p\n\\]\n\nor equivalently,\n\\[\np(x) =  \\frac{e^{\\beta_0 + \\beta_1 x_1 + \\cdots + \\beta_p x_p}}{1 + e^{\\beta_0 + \\beta_1 x_1 + \\cdots + \\beta_p x_p}}\n\\] . . .\n\nNote: \\(p(x)= P(Y = 1 \\mid X = x) = \\mathbb E[Y \\mid X = x]\\)"
  },
  {
    "objectID": "lectures/14-logistic.html#major-difference-between-linear-and-logistic-regression",
    "href": "lectures/14-logistic.html#major-difference-between-linear-and-logistic-regression",
    "title": "Supervised learning: logistic regression",
    "section": "Major difference between linear and logistic regression",
    "text": "Major difference between linear and logistic regression\nLogistic regression involves numerical optimization\n\n\\(y_i\\) is observed response for \\(n\\) observations, either 0 or 1\n\n\n\nWe need to use an iterative algorithm to find \\(\\beta\\)’s that maximize the likelihood \\[\\prod_{i=1}^{n} p\\left(x_{i}\\right)^{y_{i}}\\left(1-p\\left(x_{i}\\right)\\right)^{1-y_{i}}\\]\n\n\n\n\nNewton’s method: start with initial guess, calculate gradient of log-likelihood, add amount proportional to the gradient to parameters, moving up log-likelihood surface\n\n\n\n\nThis means logistic regression runs more slowly than linear regression\n\n\n\n\nif you’re interested: iteratively re-weighted least squares, see here"
  },
  {
    "objectID": "lectures/14-logistic.html#inference-with-logistic-regression",
    "href": "lectures/14-logistic.html#inference-with-logistic-regression",
    "title": "Supervised learning: logistic regression",
    "section": "Inference with logistic regression",
    "text": "Inference with logistic regression\n\nMajor motivation for logistic regression (and all GLMs) is inference\n\nHow does the response change when we change a predictor by one unit?\n\n\n\n\nFor linear regression, the answer is straightforward \\[\\mathbb{E}[Y \\mid x] = \\beta_0 + \\beta_1 x_1\\]\n\n\n\n\nFor logistic regression, it is a little less straightforward \\[\\mathbb E[Y \\mid x] = \\frac{e^{\\beta_0 + \\beta_1 x_1 + \\cdots + \\beta_p x_p}}{1 + e^{\\beta_0 + \\beta_1 x_1 + \\cdots + \\beta_p x_p}}\\]\n\nThe predicted response varies non-linearly with the predictor variable values\nInterpretation on the odds scale"
  },
  {
    "objectID": "lectures/14-logistic.html#the-odds-interpretation",
    "href": "lectures/14-logistic.html#the-odds-interpretation",
    "title": "Supervised learning: logistic regression",
    "section": "The odds interpretation",
    "text": "The odds interpretation\n\nThe odds of an event are the probability of the event divided by the probability of the event not occurring\n\nPretend the predicted probability is 0.8 given a particular predictor variable value\n\njust pretend we only have one predictor variable\n\n\nThis means that if we were to repeatedly sample response values given that predictor variable value: we expect class 1 to appear 4 times as often as class 0\n\\[Odds = \\frac{\\mathbb{E}[Y \\mid x]}{1-\\mathbb{E}[Y \\mid x]} = \\frac{0.8}{1-0.8} = 4 = e^{\\beta_0+\\beta_1x}\\]\nThus we say that for the given predictor variable value, the \\(Odds\\) are 4 (or 4-1) in favor of class 1\n\n\nHow does the odds change if I change the value of a predictor variable by one unit?\n\n\n\\[Odds_{\\rm new} = e^{\\beta_0+\\beta_1(x+1)} = e^{\\beta_0+\\beta_1x}e^{\\beta_1} = e^{\\beta_1}Odds_{\\rm old}\\]\nFor every unit change in \\(x\\), the odds change by a factor \\(e^{\\beta_1}\\)"
  },
  {
    "objectID": "lectures/14-logistic.html#data-diabetes-in-the-pima",
    "href": "lectures/14-logistic.html#data-diabetes-in-the-pima",
    "title": "Supervised learning: logistic regression",
    "section": "Data: diabetes in the Pima",
    "text": "Data: diabetes in the Pima\n\nEach observation represents one Pima woman, at least 21 years old, who was tested for diabetes as part of the study.\ntype: if they had diabetes according to the diagnostic criteria\nnpreg: number of pregnancies\nbp: blood pressure\n\n\nlibrary(tidyverse)\ntheme_set(theme_light())\npima &lt;- as_tibble(MASS::Pima.tr)\npima &lt;- pima |&gt; \n  mutate(pregnancy = ifelse(npreg &gt; 0, \"Yes\", \"No\")) # whether the patient has had any pregnancies"
  },
  {
    "objectID": "lectures/14-logistic.html#fitting-a-logistic-regression-model",
    "href": "lectures/14-logistic.html#fitting-a-logistic-regression-model",
    "title": "Supervised learning: logistic regression",
    "section": "Fitting a logistic regression model",
    "text": "Fitting a logistic regression model\n\nUse the glm function, but specify the family is binomial\nGet coefficients table using the tidy() function in the broom package\nCould exponentiate the coefficient estimates (log-odds) to back-transform and get the odds\n\n\npima_logit &lt;- glm(type ~ pregnancy + bp, data = pima, family = binomial)\nlibrary(broom)\n# summary(pima_logit)\ntidy(pima_logit)\n\n# A tibble: 3 × 5\n  term         estimate std.error statistic p.value\n  &lt;chr&gt;           &lt;dbl&gt;     &lt;dbl&gt;     &lt;dbl&gt;   &lt;dbl&gt;\n1 (Intercept)   -3.16      1.08       -2.94 0.00329\n2 pregnancyYes  -0.468     0.427      -1.10 0.273  \n3 bp             0.0403    0.0140      2.89 0.00391\n\ntidy(pima_logit, conf.int = TRUE, exponentiate = TRUE)\n\n# A tibble: 3 × 7\n  term         estimate std.error statistic p.value conf.low conf.high\n  &lt;chr&gt;           &lt;dbl&gt;     &lt;dbl&gt;     &lt;dbl&gt;   &lt;dbl&gt;    &lt;dbl&gt;     &lt;dbl&gt;\n1 (Intercept)    0.0422    1.08       -2.94 0.00329  0.00472     0.329\n2 pregnancyYes   0.626     0.427      -1.10 0.273    0.272       1.47 \n3 bp             1.04      0.0140      2.89 0.00391  1.01        1.07"
  },
  {
    "objectID": "lectures/14-logistic.html#interpreting-a-logistic-regression-model",
    "href": "lectures/14-logistic.html#interpreting-a-logistic-regression-model",
    "title": "Supervised learning: logistic regression",
    "section": "Interpreting a logistic regression model",
    "text": "Interpreting a logistic regression model\n\ntidy(pima_logit)\n\n# A tibble: 3 × 5\n  term         estimate std.error statistic p.value\n  &lt;chr&gt;           &lt;dbl&gt;     &lt;dbl&gt;     &lt;dbl&gt;   &lt;dbl&gt;\n1 (Intercept)   -3.16      1.08       -2.94 0.00329\n2 pregnancyYes  -0.468     0.427      -1.10 0.273  \n3 bp             0.0403    0.0140      2.89 0.00391\n\n\n\nIntercept: the estimated log-odds of diabetes for a Pima woman who has never been pregnant and has a diastolic blood pressure of 0 (hmm…) are -3.16, so the odds are exp(-3.16) = 0.0422\nPregnancy: pregnancy changes the log-odds of diabetes, relative to the baseline value, by -0.468. The odds ratio is exp(-0.47) = 0.626\n\nThe odds of diabetes for those that has had at least one pregnancies is 0.626 times that the odds of diabetes for those with no previous pregnancies\n\nBlood pressure: each unit of increase in blood pressure (in mmHg) is associated with an increase in the log-odds of diabetes of 0.0403, or an increase in the odds of diabetes by a multiplicative factor of exp(0.0403) = 1.04\n\nEvery extra mmHg in blood pressure multiplies the odds of diabetes by 1.04"
  },
  {
    "objectID": "lectures/14-logistic.html#view-predicted-probability-relationship",
    "href": "lectures/14-logistic.html#view-predicted-probability-relationship",
    "title": "Supervised learning: logistic regression",
    "section": "View predicted probability relationship",
    "text": "View predicted probability relationship\n\npima |&gt; \n  mutate(pred_prob = fitted(pima_logit),\n         i_type = as.numeric(type == \"Yes\")) |&gt; \n  ggplot(aes(bp)) +\n  geom_line(aes(y = pred_prob, color = pregnancy), linewidth = 2) +\n  geom_point(aes(y = i_type), alpha = 0.3, color = \"darkorange\", size = 4)"
  },
  {
    "objectID": "lectures/14-logistic.html#effects-plot",
    "href": "lectures/14-logistic.html#effects-plot",
    "title": "Supervised learning: logistic regression",
    "section": "Effects plot",
    "text": "Effects plot\n\nNonlinear relationship between blood pressure and probability of diabetes\nThe gap between women who were or were not previously pregnant is different across the range\n\n\nlibrary(ggeffects)\npima_logit |&gt; \n  ggeffect(terms = c(\"bp\", \"pregnancy\")) |&gt;\n  plot()"
  },
  {
    "objectID": "lectures/14-logistic.html#deviancehttpsen.wikipedia.orgwikideviance_statistics",
    "href": "lectures/14-logistic.html#deviancehttpsen.wikipedia.orgwikideviance_statistics",
    "title": "Supervised learning: logistic regression",
    "section": "[Deviance](https://en.wikipedia.org/wiki/Deviance_(statistics)?",
    "text": "[Deviance](https://en.wikipedia.org/wiki/Deviance_(statistics)?\nFor model of interest \\(\\mathcal{M}\\) the total deviance is:\n\\[D_{\\mathcal{M}}= -2 \\log \\frac{\\mathcal{L}_{\\mathcal{M}}}{\\mathcal{L}_{\\mathcal{S}}} = 2\\left(\\log  \\mathcal{L}_{\\mathcal{S}}-\\log  \\mathcal{L}_{\\mathcal{M}}\\right)\\]\n\n\\(\\mathcal{L}_{\\mathcal{M}}\\) is the likelihood for model \\(\\mathcal{M}\\)\n\n\n\n\\(\\mathcal{L}_{\\mathcal{S}}\\) is the likelihood for the saturated model, with \\(n\\) parameters! (i.e., a perfect fit)\nCan think of \\(\\mathcal{L}_{\\mathcal{S}}\\) as some constant that does not change\n\n\n\nDeviance is a measure of goodness-of-fit: the smaller the deviance, the better the fit\n\nGeneralization of RSS in linear regression to any distribution family"
  },
  {
    "objectID": "lectures/14-logistic.html#deviance-residuals",
    "href": "lectures/14-logistic.html#deviance-residuals",
    "title": "Supervised learning: logistic regression",
    "section": "Deviance residuals",
    "text": "Deviance residuals\n\npima_logit |&gt; \n  residuals(type = \"deviance\") |&gt; \n  summary()\n\n   Min. 1st Qu.  Median    Mean 3rd Qu.    Max. \n-1.5293 -0.9156 -0.7463 -0.0990  1.3205  1.9326 \n\n\nThe deviance residuals are contributions to total deviance (signed square roots of unit deviances)\n\\[d_i = \\mbox{sign}(y_i-\\hat{p}_i) \\sqrt{-2[y_i \\log \\hat{p}_i + (1-y_i) \\log (1 - \\hat{p}_i)]}\\]\nwhere \\(y_i\\) is the \\(i\\)-th observed response and \\(\\hat p_i\\) is the estimated probability of success\nNote: The use of the sign of the difference ensures that the deviance residuals are positive/negative when the observation is larger/smaller than predicted."
  },
  {
    "objectID": "lectures/14-logistic.html#model-measures",
    "href": "lectures/14-logistic.html#model-measures",
    "title": "Supervised learning: logistic regression",
    "section": "Model measures",
    "text": "Model measures\n\nglance(pima_logit)\n\n# A tibble: 1 × 8\n  null.deviance df.null logLik   AIC   BIC deviance df.residual  nobs\n          &lt;dbl&gt;   &lt;int&gt;  &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt;    &lt;dbl&gt;       &lt;int&gt; &lt;int&gt;\n1          256.     199  -123.  252.  262.     246.         197   200\n\n\n\ndeviance = 2 * logLik\nnull.deviance corresponds to intercept-only model\nAIC = 2 * df.residual - 2 * logLik\nWe will consider these to be less important than out-of-sample/test set performances"
  },
  {
    "objectID": "lectures/14-logistic.html#predictions",
    "href": "lectures/14-logistic.html#predictions",
    "title": "Supervised learning: logistic regression",
    "section": "Predictions",
    "text": "Predictions\n\nThe default output of predict() is on the log-odds scale\nTo get predicted probabilities, specify type = \"response\n\n\npima_pred_prob &lt;- predict(pima_logit, type = \"response\")\n\n\nHow do we predict the class (diabetes or not)?\nTypically if predicted probability is &gt; 0.5 then we predict Yes, else No\n\nCould also be encoded as 1 and 0\n\n\n\npima_pred_class &lt;- ifelse(pima_pred_prob &gt; 0.5, \"Yes\", \"No\")\npima_pred_binary &lt;- ifelse(pima_pred_prob &gt; 0.5, 1, 0)"
  },
  {
    "objectID": "lectures/14-logistic.html#model-assessment",
    "href": "lectures/14-logistic.html#model-assessment",
    "title": "Supervised learning: logistic regression",
    "section": "Model assessment",
    "text": "Model assessment\n\nConfusion matrix\n\n\ntable(\"Predicted\" = pima_pred_class, \"Observed\" = pima$type)\n\n         Observed\nPredicted  No Yes\n      No  125  60\n      Yes   7   8\n\n\n\n\nIn-sample misclassification rate\n\n\nmean(pima_pred_class != pima$type)\n\n[1] 0.335\n\n\n\n\n\nBrier score\n\n\nmean((pima_pred_binary - pima_pred_prob)^2)\n\n[1] 0.1140525"
  },
  {
    "objectID": "lectures/14-logistic.html#calibration",
    "href": "lectures/14-logistic.html#calibration",
    "title": "Supervised learning: logistic regression",
    "section": "Calibration",
    "text": "Calibration\nTODO: revise\n\nOrigination: weather forecasting\nSuppose you look at the forecast for 6/6/2024 and see there’s a 20% chance of rain\nBut what does it mean that there’s a 20% chance of rain on 6/6/2024? It either does or does not rain.\nWeather forecasters ran into this problem of wanting to provide a probability of rain but were unsure of how to define this probability\nSolution for defining a 20% chance of rain\n\nif the model outputs 20% of rain on 6/6/2024, and\nfor previous days (years), the ratio of the number of days it actually rained to the total number of days the classifier predicted rain at 20% chance was approximately 20% percent\n\nA classifier is calibrated if in the last time period, (i.e. a year), the percentage of days it actually rained when the forecast predicted x percent chance of rain is x percent."
  },
  {
    "objectID": "lectures/14-logistic.html#calibration-1",
    "href": "lectures/14-logistic.html#calibration-1",
    "title": "Supervised learning: logistic regression",
    "section": "Calibration",
    "text": "Calibration\nIf we’ve got a model which tells us that the probability of rain on a certain group of days is 50%, it had better rain on half of those days, or the model is just wrong about the probability of rain\n\nIn short, for a model to be calibrated, the actual probabilities should match the predicted probabilities\nCalibration should be used together with other model evaluation and diagnostic tools\nA model is well-calibrated does not mean it is good at making predictions"
  },
  {
    "objectID": "lectures/14-logistic.html#calibration-plot-smoothing",
    "href": "lectures/14-logistic.html#calibration-plot-smoothing",
    "title": "Supervised learning: logistic regression",
    "section": "Calibration plot: smoothing",
    "text": "Calibration plot: smoothing\nPlot the observed outcome against the fitted probability, and apply a smoother\n\npima |&gt; \n  mutate(pred = predict(pima_logit, type = \"response\"),\n         obs = ifelse(type == \"Yes\", 1, 0)) |&gt; \n  ggplot(aes(pred, obs)) +\n  geom_point() +\n  geom_smooth(se = FALSE) +\n  geom_abline(slope = 1, intercept = 0, linetype = \"dashed\") \n\n\nCalibration looks good for the most part… odd behavior for predicted probabilities over 0.6, since there are only a few observations with high predicted probability"
  },
  {
    "objectID": "lectures/14-logistic.html#calibration-plot-binning",
    "href": "lectures/14-logistic.html#calibration-plot-binning",
    "title": "Supervised learning: logistic regression",
    "section": "Calibration plot: binning",
    "text": "Calibration plot: binning\n\npima |&gt; \n  mutate(pred_prob = predict(pima_logit, type = \"response\"),\n         bin_pred_prob = cut(pred_prob, breaks = seq(0, 1, 0.1))) |&gt; \n  group_by(bin_pred_prob) |&gt; \n  summarize(n = n(),\n            bin_actual_prob = mean(type == \"Yes\")) |&gt; \n  mutate(mid_point = seq(0.15, 0.65, 0.1)) |&gt; \n  ggplot(aes(x = mid_point, y = bin_actual_prob)) +\n  geom_point(aes(size = n)) +\n  geom_line() +\n  geom_abline(slope = 1, intercept = 0, \n              color = \"black\", linetype = \"dashed\") +\n  # expand_limits(x = c(0, 1), y = c(0, 1)) +\n  coord_fixed()\n\n\n\n\n36-SURE.github.io"
  },
  {
    "objectID": "lectures/16-pca.html#the-big-picture",
    "href": "lectures/16-pca.html#the-big-picture",
    "title": "Unsupervised learning: principal component analysis",
    "section": "The big picture",
    "text": "The big picture\nHow do we visualize structure of high-dimensional data?\n\nExample: What if you’re given a dataset with 50 variables, and ask you to make one visualization that best represents the data? What do you do?\n\n\nTedious task: Make a series of all \\(\\displaystyle \\binom{50}{2} = 1225\\) pairs of plots?\n\n\nIntuition: Take high-dimensional data and represent it in 2-3 dimensions, then visualize those dimensions"
  },
  {
    "objectID": "lectures/16-pca.html#what-is-the-goal-of-dimension-reduction",
    "href": "lectures/16-pca.html#what-is-the-goal-of-dimension-reduction",
    "title": "Unsupervised learning: principal component analysis",
    "section": "What is the goal of dimension reduction?",
    "text": "What is the goal of dimension reduction?\nWe have \\(p\\) variables (columns) for \\(n\\) observations (rows) BUT which variables are interesting?\n\nCan we find a smaller number of dimensions that captures the interesting structure in the data?\n\nCould examine all pairwise scatterplots of each variable - tedious, manual process\nClustering of variables based on correlation\nCan we find a combination of the original \\(p\\) variables?\n\n\n\nDimension reduction:\n\nFocus on reducing the dimensionality of the feature space (i.e., number of columns)\nRetain most of the information / variability in a lower dimensional space (i.e., reducing the number of columns)\nThe process: (big) \\(n \\times p\\) matrix \\(\\longrightarrow\\) dimension reduction method \\(\\longrightarrow\\) (smaller) \\(n \\times k\\) matrix"
  },
  {
    "objectID": "lectures/16-pca.html#principal-components-analysis-pca",
    "href": "lectures/16-pca.html#principal-components-analysis-pca",
    "title": "Unsupervised learning: principal component analysis",
    "section": "Principal components analysis (PCA)",
    "text": "Principal components analysis (PCA)"
  },
  {
    "objectID": "lectures/16-pca.html#principal-components-analysis-pca-1",
    "href": "lectures/16-pca.html#principal-components-analysis-pca-1",
    "title": "Unsupervised learning: principal component analysis",
    "section": "Principal components analysis (PCA)",
    "text": "Principal components analysis (PCA)\n\\[\n\\begin{pmatrix}\n& & \\text{really} & & \\\\\n& & \\text{wide} & & \\\\\n& & \\text{matrix} & &\n\\end{pmatrix}\n\\rightarrow \\text{matrix algebra razzmatazz} \\rightarrow\n\\begin{pmatrix}\n\\text{much}  \\\\\n\\text{thinner}  \\\\\n\\text{matrix}\n\\end{pmatrix}\n\\]\n\nPCA explores the covariance between variables, and combines variables into a smaller set of uncorrelated variables called principal components (PCs)\n\nTurn a \\(n \\times p\\) matrix of correlated variables into a \\(n \\times k\\) matrix of uncorrelated variables\n\nEach of the \\(k\\) columns in the right-hand matrix are principal components (PCs), all uncorrelated with each other\n\nPCs are weighted, linear combinations of the original variables\nWeights reveal how different variables are loaded into the PCs\n\nFirst column accounts for most variation in the data, second column for second-most variation, and so on\n\nIntuition: we want a small number of PCs (first few PCs) to account for most of the information/variance in the data"
  },
  {
    "objectID": "lectures/16-pca.html#what-are-principal-components",
    "href": "lectures/16-pca.html#what-are-principal-components",
    "title": "Unsupervised learning: principal component analysis",
    "section": "What are principal components?",
    "text": "What are principal components?\n\nAssume \\(\\boldsymbol{X}\\) is a \\(n \\times p\\) matrix that is centered and stardardized\nTotal variation \\(= p\\), since \\(\\text{Var}(\\boldsymbol{x}_j)\\) = 1 for all \\(j = 1, \\dots, p\\)\nPCA will give us \\(p\\) principal components that are \\(n\\)-length columns - call these \\(Z_1, \\dots, Z_p\\)\n\nFirst principal component\n\\[Z_1 = \\phi_{11} X_1 + \\phi_{21} X_2 + \\dots + \\phi_{p1} X_p\\]\n\n\n\\(\\phi_{j1}\\) are the weights indicating the contributions of each variable \\(j \\in 1, \\dots, p\\)\nWeights are normalized \\(\\sum_{j=1}^p \\phi_{j1}^2 = 1\\)\n\\(\\phi_{1} = (\\phi_{11}, \\phi_{21}, \\dots, \\phi_{p1})\\) is the loading vector for PC1\n\n\n\n\n\\(Z_1\\) is a linear combination of the \\(p\\) variables that has the largest variance"
  },
  {
    "objectID": "lectures/16-pca.html#principal-components-analysis-pca-2",
    "href": "lectures/16-pca.html#principal-components-analysis-pca-2",
    "title": "Unsupervised learning: principal component analysis",
    "section": "Principal components analysis (PCA)",
    "text": "Principal components analysis (PCA)\nSecond principal component\n\\[Z_2 = \\phi_{12} X_1 + \\phi_{22} X_2 + \\dots + \\phi_{p2} X_p\\]\n\n\\(\\phi_{j2}\\) are the weights indicating the contributions of each variable \\(j \\in 1, \\dots, p\\)\nWeights are normalized \\(\\sum_{j=1}^p \\phi_{j1}^2 = 1\\)\n\\(\\phi_{2} = (\\phi_{12}, \\phi_{22}, \\dots, \\phi_{p2})\\) is the loading vector for PC2\n\\(Z_2\\) is a linear combination of the \\(p\\) variables that has the largest variance\n\nSubject to constraint it is uncorrelated with \\(Z_1\\)\n\n\n\nRepeat this process to create \\(p\\) principal components\n\nUncorrelated: Each (\\(Z_j, Z_{j'}\\)) is uncorrelated with each other\nOrdered Variance: \\(\\text{Var}(Z_1) &gt; \\text{Var}(Z_2) &gt; \\dots &gt; \\text{Var}(Z_p)\\)\nTotal Variance: \\(\\sum_{j=1}^p \\text{Var}(Z_j) = p\\)"
  },
  {
    "objectID": "lectures/16-pca.html#visualizing-pca-in-two-dimensions",
    "href": "lectures/16-pca.html#visualizing-pca-in-two-dimensions",
    "title": "Unsupervised learning: principal component analysis",
    "section": "Visualizing PCA in two dimensions",
    "text": "Visualizing PCA in two dimensions"
  },
  {
    "objectID": "lectures/16-pca.html#visualizing-pca-in-two-dimensions-1",
    "href": "lectures/16-pca.html#visualizing-pca-in-two-dimensions-1",
    "title": "Unsupervised learning: principal component analysis",
    "section": "Visualizing PCA in two dimensions",
    "text": "Visualizing PCA in two dimensions"
  },
  {
    "objectID": "lectures/16-pca.html#visualizing-pca-in-two-dimensions-2",
    "href": "lectures/16-pca.html#visualizing-pca-in-two-dimensions-2",
    "title": "Unsupervised learning: principal component analysis",
    "section": "Visualizing PCA in two dimensions",
    "text": "Visualizing PCA in two dimensions"
  },
  {
    "objectID": "lectures/16-pca.html#visualizing-pca-in-two-dimensions-3",
    "href": "lectures/16-pca.html#visualizing-pca-in-two-dimensions-3",
    "title": "Unsupervised learning: principal component analysis",
    "section": "Visualizing PCA in two dimensions",
    "text": "Visualizing PCA in two dimensions"
  },
  {
    "objectID": "lectures/16-pca.html#visualizing-pca-in-two-dimensions-4",
    "href": "lectures/16-pca.html#visualizing-pca-in-two-dimensions-4",
    "title": "Unsupervised learning: principal component analysis",
    "section": "Visualizing PCA in two dimensions",
    "text": "Visualizing PCA in two dimensions"
  },
  {
    "objectID": "lectures/16-pca.html#searching-for-variance-in-orthogonal-directions",
    "href": "lectures/16-pca.html#searching-for-variance-in-orthogonal-directions",
    "title": "Unsupervised learning: principal component analysis",
    "section": "Searching for variance in orthogonal directions",
    "text": "Searching for variance in orthogonal directions"
  },
  {
    "objectID": "lectures/16-pca.html#pca-singular-value-decomposition-svd",
    "href": "lectures/16-pca.html#pca-singular-value-decomposition-svd",
    "title": "Unsupervised learning: principal component analysis",
    "section": "PCA: singular value decomposition (SVD)",
    "text": "PCA: singular value decomposition (SVD)\n\\[\nX = U D V^T\n\\]\n\n\\(U\\) and \\(V\\): matrices containing the left and right singular vectors of scaled matrix \\(X\\)\n\\(D\\): diagonal matrix of the singular values\n\n\n\nSVD simplifies matrix-vector multiplication as rotate, scale, and rotate again\n\n\n\n\\(V\\): loading matrix for \\(X\\) with \\(\\phi_{j}\\) as columns\n\n\\(Z = X V\\): PC matrix\n\n\n\nBonus: Eigenvalue decomposition (or spectral decomposition)\n\n\\(V\\): eigenvectors of \\(X^TX\\) (covariance matrix, \\(^T\\): transpose)\n\\(U\\): eigenvectors of \\(XX^T\\)\nThe singular values (diagonal of \\(D\\)) are square roots of the eigenvalues of \\(X^TX\\) or \\(XX^T\\)\nMeaning that \\(Z = UD\\)"
  },
  {
    "objectID": "lectures/16-pca.html#eigenvalues-guide-dimension-reduction",
    "href": "lectures/16-pca.html#eigenvalues-guide-dimension-reduction",
    "title": "Unsupervised learning: principal component analysis",
    "section": "Eigenvalues guide dimension reduction",
    "text": "Eigenvalues guide dimension reduction\nWe want to choose \\(p^* &lt; p\\) such that we are explaining variation in the data\n\nEigenvalues \\(\\lambda_j\\) for \\(j \\in 1, \\dots, p\\) indicate the variance explained by each component\n\n\\(\\displaystyle \\sum_j^p \\lambda_j = p\\), meaning \\(\\lambda_j \\geq 1\\) indicates \\(\\text{PC}j\\) contains at least one variable’s worth in variability\n\\(\\displaystyle \\frac{\\lambda_j}{p}\\): proportion of variance explained by \\(\\text{PC}j\\)\nArranged in descending order so that \\(\\lambda_1\\) is largest eigenvalue and corresponds to PC1\n\n\n\n\nCompute the cumulative proportion of variance explained (CVE) with \\(p^*\\) components \\[\\text{CVE}_{p^*} = \\sum_j^{p*} \\frac{\\lambda_j}{p}\\] Use scree plot to plot eigenvalues and guide choice for \\(p^* &lt;p\\) by looking for “elbow” (rapid to slow change)"
  },
  {
    "objectID": "lectures/16-pca.html#data-nutritional-information-of-starbucks-drinks",
    "href": "lectures/16-pca.html#data-nutritional-information-of-starbucks-drinks",
    "title": "Unsupervised learning: principal component analysis",
    "section": "Data: nutritional information of Starbucks drinks",
    "text": "Data: nutritional information of Starbucks drinks\n\nlibrary(tidyverse)\ntheme_set(theme_light())\nstarbucks &lt;- read_csv(\n  \"https://raw.githubusercontent.com/rfordatascience/tidytuesday/master/data/2021/2021-12-21/starbucks.csv\"\n) |&gt;\n  # convert columns to numeric that were saved as character\n  mutate(trans_fat_g = as.numeric(trans_fat_g), fiber_g = as.numeric(fiber_g))\nglimpse(starbucks)\n\nRows: 1,147\nColumns: 15\n$ product_name    &lt;chr&gt; \"brewed coffee - dark roast\", \"brewed coffee - dark ro…\n$ size            &lt;chr&gt; \"short\", \"tall\", \"grande\", \"venti\", \"short\", \"tall\", \"…\n$ milk            &lt;dbl&gt; 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 2, …\n$ whip            &lt;dbl&gt; 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, …\n$ serv_size_m_l   &lt;dbl&gt; 236, 354, 473, 591, 236, 354, 473, 591, 236, 354, 473,…\n$ calories        &lt;dbl&gt; 3, 4, 5, 5, 3, 4, 5, 5, 3, 4, 5, 5, 3, 4, 5, 5, 35, 50…\n$ total_fat_g     &lt;dbl&gt; 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1,…\n$ saturated_fat_g &lt;dbl&gt; 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0,…\n$ trans_fat_g     &lt;dbl&gt; 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0,…\n$ cholesterol_mg  &lt;dbl&gt; 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 10,…\n$ sodium_mg       &lt;dbl&gt; 5, 10, 10, 10, 5, 10, 10, 10, 5, 5, 5, 5, 5, 5, 5, 5, …\n$ total_carbs_g   &lt;dbl&gt; 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 5, 5, …\n$ fiber_g         &lt;dbl&gt; 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, …\n$ sugar_g         &lt;dbl&gt; 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 5, 5, …\n$ caffeine_mg     &lt;dbl&gt; 130, 193, 260, 340, 15, 20, 25, 30, 155, 235, 310, 410…"
  },
  {
    "objectID": "lectures/16-pca.html#implement-pca",
    "href": "lectures/16-pca.html#implement-pca",
    "title": "Unsupervised learning: principal component analysis",
    "section": "Implement PCA",
    "text": "Implement PCA\nUse the prcomp() function (based on SVD) for PCA on centered and scaled data\n\nfeat &lt;- starbucks |&gt; \n  select(serv_size_m_l:caffeine_mg)\nstarbucks_pca &lt;- prcomp(feat, center = TRUE, scale. = TRUE)\nsummary(starbucks_pca)\n\nImportance of components:\n                          PC1    PC2    PC3     PC4     PC5     PC6    PC7\nStandard deviation     2.4748 1.3074 1.0571 0.97919 0.67836 0.56399 0.4413\nProportion of Variance 0.5568 0.1554 0.1016 0.08716 0.04183 0.02892 0.0177\nCumulative Proportion  0.5568 0.7122 0.8138 0.90093 0.94276 0.97168 0.9894\n                           PC8     PC9    PC10    PC11\nStandard deviation     0.28123 0.16874 0.08702 0.04048\nProportion of Variance 0.00719 0.00259 0.00069 0.00015\nCumulative Proportion  0.99657 0.99916 0.99985 1.00000"
  },
  {
    "objectID": "lectures/16-pca.html#pca-output",
    "href": "lectures/16-pca.html#pca-output",
    "title": "Unsupervised learning: principal component analysis",
    "section": "PCA output",
    "text": "PCA output\n\n# str(starbucks_pca)\n\nExamine the output after running prcomp()\n\nstarbucks_pca$sdev: singular values (\\(\\sqrt{\\lambda_j}\\))\nstarbucks_pca$rotation: loading matrix (\\(V\\))\nstarbucks_pca$x: principal component scores matrix (\\(Z=XV\\))\n\nCan use the broom package for tidying prcomp()\n\ntidy(starbucks_pca, matrix = \"eigenvalues\") # equivalent to starbucks_pca$sdev\ntidy(starbucks_pca, matrix = \"rotation\") # equivalent to starbucks_pca$rotation\ntidy(starbucks_pca, matrix = \"scores\") # equivalent to starbucks_pca$x"
  },
  {
    "objectID": "lectures/16-pca.html#proportion-of-variance-explained",
    "href": "lectures/16-pca.html#proportion-of-variance-explained",
    "title": "Unsupervised learning: principal component analysis",
    "section": "Proportion of variance explained",
    "text": "Proportion of variance explained\n\nlibrary(broom)\nstarbucks_pca |&gt;\n  tidy(matrix = \"eigenvalues\") |&gt;\n  ggplot(aes(x = PC, y = percent)) +\n  geom_line() + \n  geom_point() +\n  geom_hline(yintercept = 1 / ncol(feat), color = \"darkred\", linetype = \"dashed\") +\n  scale_x_continuous(breaks = 1:ncol(starbucks_pca$x))"
  },
  {
    "objectID": "lectures/16-pca.html#cumulative-proportion-of-variance-explained",
    "href": "lectures/16-pca.html#cumulative-proportion-of-variance-explained",
    "title": "Unsupervised learning: principal component analysis",
    "section": "Cumulative proportion of variance explained",
    "text": "Cumulative proportion of variance explained\n\nlibrary(broom)\nstarbucks_pca |&gt;\n  tidy(matrix = \"eigenvalues\") |&gt;\n  ggplot(aes(x = PC, y = cumulative)) +\n  geom_line() + \n  geom_point() +\n  scale_x_continuous(breaks = 1:ncol(starbucks_pca$x))"
  },
  {
    "objectID": "lectures/16-pca.html#computing-principal-components",
    "href": "lectures/16-pca.html#computing-principal-components",
    "title": "Unsupervised learning: principal component analysis",
    "section": "Computing principal components",
    "text": "Computing principal components\nExtract the matrix of principal components \\(\\boldsymbol{Z} = XV\\) (dimension of \\(\\boldsymbol{Z}\\) will match original data)\n\nstarbucks_pc_matrix &lt;- starbucks_pca$x\nhead(starbucks_pc_matrix)\n\n           PC1        PC2        PC3           PC4         PC5         PC6\n[1,] -3.766852 -1.0023657  0.2482698 -0.1521871448  0.24739830 -0.11365847\n[2,] -3.633234 -0.6946439  1.2059943 -0.3720566566  0.06052789 -0.06406410\n[3,] -3.518063 -0.3981399  2.2165170 -0.5967175941 -0.13122572 -0.01937237\n[4,] -3.412061 -0.1067045  3.3741594 -0.8490378243 -0.26095965 -0.00899485\n[5,] -3.721426 -0.9868147 -1.0705094  0.0949330091 -0.27181508  0.17491809\n[6,] -3.564899 -0.6712499 -0.7779083 -0.0003019903 -0.72054963  0.37005543\n             PC7         PC8        PC9       PC10        PC11\n[1,] -0.02812472 0.006489978 0.05145094 0.06678083 0.019741873\n[2,]  0.05460952 0.021148978 0.07094211 0.08080545 0.023480029\n[3,]  0.09050806 0.031575955 0.08901403 0.09389227 0.028669251\n[4,]  0.11585507 0.037521689 0.11287190 0.11582260 0.034691142\n[5,]  0.07009414 0.037736197 0.02892317 0.03631676 0.005775410\n[6,]  0.20236484 0.068154160 0.03705252 0.03497690 0.002469611\n\n\nColumns are uncorrelated, such that \\(\\text{Var}(Z_1) &gt; \\text{Var}(Z_2) &gt; \\dots &gt; \\text{Var}(Z_p)\\)"
  },
  {
    "objectID": "lectures/16-pca.html#visualizing-first-two-principal-components",
    "href": "lectures/16-pca.html#visualizing-first-two-principal-components",
    "title": "Unsupervised learning: principal component analysis",
    "section": "Visualizing first two principal components",
    "text": "Visualizing first two principal components\n\n\n\nstarbucks &lt;- starbucks |&gt; \n  mutate(pc1 = starbucks_pc_matrix[,1], \n         pc2 = starbucks_pc_matrix[,2])\nstarbucks |&gt; \n  ggplot(aes(x = pc1, y = pc2)) +\n  geom_point(alpha = 0.5) +\n  labs(x = \"PC 1\", y = \"PC 2\")\n\n\nPrincipal components are not interpretable\nMake a biplot with arrows showing the linear relationship between one variable and other variables"
  },
  {
    "objectID": "lectures/16-pca.html#making-pcs-interpretable-with-biplots",
    "href": "lectures/16-pca.html#making-pcs-interpretable-with-biplots",
    "title": "Unsupervised learning: principal component analysis",
    "section": "Making PCs interpretable with biplots",
    "text": "Making PCs interpretable with biplots\nBiplot displays both the space of observations and the space of variables\nCheck out the factoextra package\n\n\n\nlibrary(factoextra)\n# fviz_pca_var(): projection of variables\n# fviz_pca_ind(): display observations with first two PCs\nstarbucks_pca |&gt; \n  fviz_pca_biplot(label = \"var\",\n                  alpha.ind = 0.25,\n                  alpha.var = 0.75,\n                  col.var = \"darkblue\",\n                  repel = TRUE)\n\n\nArrow direction: “as the variable increases…”\nArrow angles: correlation\n\n\\(90^{\\circ}\\): uncorrelated\n\\(&lt; 90^{\\circ}\\): positively correlated\n\\(&gt; 90^{\\circ}\\): negatively correlated\n\nArrow length: strength of relationship with PCs"
  },
  {
    "objectID": "lectures/16-pca.html#how-many-principal-components-to-use",
    "href": "lectures/16-pca.html#how-many-principal-components-to-use",
    "title": "Unsupervised learning: principal component analysis",
    "section": "How many principal components to use?",
    "text": "How many principal components to use?\nIntuition: Additional principal components will add smaller and smaller variance\n\nKeep adding components until the added variance drops off\n\n\nsummary(starbucks_pca)\n\nImportance of components:\n                          PC1    PC2    PC3     PC4     PC5     PC6    PC7\nStandard deviation     2.4748 1.3074 1.0571 0.97919 0.67836 0.56399 0.4413\nProportion of Variance 0.5568 0.1554 0.1016 0.08716 0.04183 0.02892 0.0177\nCumulative Proportion  0.5568 0.7122 0.8138 0.90093 0.94276 0.97168 0.9894\n                           PC8     PC9    PC10    PC11\nStandard deviation     0.28123 0.16874 0.08702 0.04048\nProportion of Variance 0.00719 0.00259 0.00069 0.00015\nCumulative Proportion  0.99657 0.99916 0.99985 1.00000"
  },
  {
    "objectID": "lectures/16-pca.html#create-a-scree-plot-or-elbow-plot",
    "href": "lectures/16-pca.html#create-a-scree-plot-or-elbow-plot",
    "title": "Unsupervised learning: principal component analysis",
    "section": "Create a scree plot (or elbow plot)",
    "text": "Create a scree plot (or elbow plot)\n\nstarbucks_pca |&gt; \n  fviz_eig(addlabels = TRUE) +\n  geom_hline(yintercept = 100 * (1 / ncol(starbucks_pca$x)), linetype = \"dashed\", color = \"darkred\")\n\n\n\nRule of thumb: horizontal line at \\(1/p\\) (Why?)"
  },
  {
    "objectID": "lectures/16-pca.html#remember-the-spelling",
    "href": "lectures/16-pca.html#remember-the-spelling",
    "title": "Unsupervised learning: principal component analysis",
    "section": "Remember the spelling…",
    "text": "Remember the spelling…\n\n\nfolks. it's principal (not principle) components analysis.\n\n— Stephanie Hicks, PhD (@stephaniehicks) February 17, 2023\n\n\n\n\n36-SURE.github.io"
  },
  {
    "objectID": "lectures/00-welcome.html#plan-for-today",
    "href": "lectures/00-welcome.html#plan-for-today",
    "title": "Welcome to SURE 2024",
    "section": "Plan for today",
    "text": "Plan for today\n\n9:15: Opening remarks + program overview\n10:00: Meet the department staff (Jess/Chrissie/LeeAnn)\n10:15: Meet the academic advisor (Glenn)\n11:30 (ish): Lunch\n\nHealth (Posner 151)\n\n11:30: Webinar\n2:00: Lab\n\nSports (Posner 153)\n\n12:30: Guest speaker\n2:00: Lab"
  },
  {
    "objectID": "lectures/00-welcome.html#who-are-we",
    "href": "lectures/00-welcome.html#who-are-we",
    "title": "Welcome to SURE 2024",
    "section": "Who are we?",
    "text": "Who are we?\n\nDepartment of Statistics & Data Science, Carnegie Mellon University\nLead instructor: Quang Nguyen (preferred form of address: Quang)\n\nOffice hours: 12:45-1:45pm Fridays\n\nTeaching assistants\n\nHealth: Akshay Prasadan (lead TA/project manager), Princess Allotey, Nick Kissel\nSports: Yuchen Chen, JungHo Lee, Daven Lagu"
  },
  {
    "objectID": "lectures/00-welcome.html#summer-undergraduate-research-experience",
    "href": "lectures/00-welcome.html#summer-undergraduate-research-experience",
    "title": "Welcome to SURE 2024",
    "section": "Summer Undergraduate Research Experience",
    "text": "Summer Undergraduate Research Experience\nExplore cutting-edge statistics and data science methodology with applications in\n\nHeathcare: UnitedHealth Group Bridges to Healthcare Technology\nSports: Carnegie Mellon Sports Analytics Camp (CMSACamp)\n\n\n\n“The best thing about being a statistician…is that you get to play in everyone’s backyard.” — John W. Tukey"
  },
  {
    "objectID": "lectures/00-welcome.html#goals",
    "href": "lectures/00-welcome.html#goals",
    "title": "Welcome to SURE 2024",
    "section": "Goals",
    "text": "Goals\n\nDevelop fundamentals research skills: data wrangling, visualization, modeling, communication\nBecome familiar with R, tidyverse, Quarto (Markdown syntax), Git/GitHub\nBecome familiar with cutting-edge statistical machine learning techniques\nCreate a portfolio of projects and practice reproducible research\nNetwork with academic researchers and industry professionals\n\nHelp navigate your next steps—industry vs. graduate school"
  },
  {
    "objectID": "lectures/00-welcome.html#resources",
    "href": "lectures/00-welcome.html#resources",
    "title": "Welcome to SURE 2024",
    "section": "Resources",
    "text": "Resources\n\nWebsite\nGoogle Calendar\nSlack\nEmail"
  },
  {
    "objectID": "lectures/00-welcome.html#a-typical-day-consists-of-3-main-events",
    "href": "lectures/00-welcome.html#a-typical-day-consists-of-3-main-events",
    "title": "Welcome to SURE 2024",
    "section": "A typical day consists of 3 main events",
    "text": "A typical day consists of 3 main events\n\nLectures\nPresentations\nLabs"
  },
  {
    "objectID": "lectures/00-welcome.html#lectures",
    "href": "lectures/00-welcome.html#lectures",
    "title": "Welcome to SURE 2024",
    "section": "Lectures",
    "text": "Lectures\nMon-Fri, 9:15-10:45am, Posner 151\n\n\nFirst ~2 weeks: EDA, basic data science tasks\n\n\n\n\nNext ~4 weeks: statistical modeling, machine learning\n\n\n\n\nAfter that: special topics, guest lectures\n\n\n\n\n8th week: MN trip (Health), sport-specific activities, presentations on last day\n\n\n\nHolidays: Juneteenth (Wed June 19) and Independence Day break (July 3-5)"
  },
  {
    "objectID": "lectures/00-welcome.html#presentations",
    "href": "lectures/00-welcome.html#presentations",
    "title": "Welcome to SURE 2024",
    "section": "Presentations",
    "text": "Presentations\nAround mid-day/lunchtime, Posner 151 (Health) or Posner 153 (Sports)\nNote: dates/times may vary; check calendar\n\nHealth: UHG webinar speakers\nSports: project pitches, guest speakers"
  },
  {
    "objectID": "lectures/00-welcome.html#labs",
    "href": "lectures/00-welcome.html#labs",
    "title": "Welcome to SURE 2024",
    "section": "Labs",
    "text": "Labs\nMon-Fri, 2-3:30pm, Posner 151 (Health) or Posner 153 (Sports)\n\nDemo labs\nProject labs\n\nwill begin with a mini EDA project\nthen shift to focus on main project"
  },
  {
    "objectID": "lectures/00-welcome.html#eda-project",
    "href": "lectures/00-welcome.html#eda-project",
    "title": "Welcome to SURE 2024",
    "section": "EDA project",
    "text": "EDA project\n\nPractice understanding the structure of a dataset and perform basic EDA and data visualization techniques in R, and using GitHub for collaboration\nWork in a group of 3\nTimeline\n\nRelease date: Thursday, June 6\n8-minute presentation on Tuesday, June 18 during lecture time"
  },
  {
    "objectID": "lectures/00-welcome.html#main-project",
    "href": "lectures/00-welcome.html#main-project",
    "title": "Welcome to SURE 2024",
    "section": "Main project",
    "text": "Main project\n\nAnalyze a dataset in health or sports analytics to answer a research question that is important to people in your respective field\nWork in a group of 2-4\nDeliverables (more details will be provided later on)\n\nReport\nPoster\n8-min presentation"
  },
  {
    "objectID": "lectures/00-welcome.html#reminders",
    "href": "lectures/00-welcome.html#reminders",
    "title": "Welcome to SURE 2024",
    "section": "Reminders",
    "text": "Reminders\n\nFill out the survey forms (Communication and Data Science Background)\nReset CMU wifi password (for non-CMU students)"
  },
  {
    "objectID": "lectures/00-welcome.html#expectations",
    "href": "lectures/00-welcome.html#expectations",
    "title": "Welcome to SURE 2024",
    "section": "Expectations",
    "text": "Expectations\n\nIn-person attendance\n\nBe on time. PLEASE\n\nParticipate and ask questions\nWork together. Help and support each other.\nEnjoy, learn and grow\n\n\n\n36-SURE.github.io"
  },
  {
    "objectID": "lectures/03-categorical.html#data",
    "href": "lectures/03-categorical.html#data",
    "title": "Data visualization: categorical data",
    "section": "Data",
    "text": "Data\n\nFlying ettiquette survey\nPublicly available on GitHub and also via the ggmosaic package (the dataset is called fly).\nWhat does each row represent here?\n\n\nlibrary(tidyverse)\ntheme_set(theme_light()) # setting the ggplot theme\nlibrary(ggmosaic) # make sure to install it first\nflying_etiquette &lt;- fly |&gt; \n  filter(!is.na(do_you_recline), !is.na(rude_to_recline))\nnames(flying_etiquette)\n\n [1] \"id\"                             \"flight_freq\"                   \n [3] \"do_you_recline\"                 \"height\"                        \n [5] \"has_child_under_18\"             \"three_seats_two_arms\"          \n [7] \"two_seats_one_arm\"              \"window_shade\"                  \n [9] \"rude_to_move_to_unsold_seat\"    \"rude_to_talk_to_neighbor\"      \n[11] \"six_hr_flight_leave_seat\"       \"reclining_obligation_to_behind\"\n[13] \"rude_to_recline\"                \"eliminate_reclining\"           \n[15] \"rude_to_switch_seats_friends\"   \"rude_to_switch_seats_family\"   \n[17] \"rude_to_wake_neighbor_bathroom\" \"rude_to_wake_neighbor_walk\"    \n[19] \"rude_to_bring_baby\"             \"rude_to_bring_unruly_child\"    \n[21] \"use_electronics_takeoff\"        \"smoked_inflight\"               \n[23] \"gender\"                         \"age\"                           \n[25] \"household_income\"               \"education\"                     \n[27] \"region\""
  },
  {
    "objectID": "lectures/03-categorical.html#categorical-data",
    "href": "lectures/03-categorical.html#categorical-data",
    "title": "Data visualization: categorical data",
    "section": "Categorical data",
    "text": "Categorical data\nTwo different versions of categorical data:\nNominal: categorical variables having unordered scales\n\nExamples: race, gender, species, etc,\n\nOrdinal: ordered categories; levels with a meaningful order\n\nExamples: education level, grades, ranks"
  },
  {
    "objectID": "lectures/03-categorical.html#factors-in-r",
    "href": "lectures/03-categorical.html#factors-in-r",
    "title": "Data visualization: categorical data",
    "section": "Factors in R",
    "text": "Factors in R\n\nIn R, factors are used to work with categorical variables\nR treats factors as ordinal - defaults to alphabetical\n\nMay need to manually define the factor levels (e.g., the reference level)\n\nSee the forcats package (automatically loaded with tidyverse)\n\n\nclass(flying_etiquette$do_you_recline)\n\n[1] \"factor\"\n\nlevels(flying_etiquette$do_you_recline)\n\n[1] \"never\"               \"once in a while\"     \"about half the time\"\n[4] \"usually\"             \"always\""
  },
  {
    "objectID": "lectures/03-categorical.html#summarizing-1d-categorical-data",
    "href": "lectures/03-categorical.html#summarizing-1d-categorical-data",
    "title": "Data visualization: categorical data",
    "section": "Summarizing 1D categorical data",
    "text": "Summarizing 1D categorical data\nHow often do these respondents recline?\nFrequency tables (counts)\n\ntable(flying_etiquette$do_you_recline)\n\n\n              never     once in a while about half the time             usually \n                170                 256                 117                 175 \n             always \n                136 \n\n# flying_etiquette |&gt; \n#   group_by(do_you_recline) |&gt;\n#   summarize(n = n(), .groups = \"drop\")\n\nflying_etiquette |&gt; \n  count(do_you_recline)\n\n# A tibble: 5 × 2\n  do_you_recline          n\n  &lt;fct&gt;               &lt;int&gt;\n1 never                 170\n2 once in a while       256\n3 about half the time   117\n4 usually               175\n5 always                136"
  },
  {
    "objectID": "lectures/03-categorical.html#summarizing-1d-categorical-data-1",
    "href": "lectures/03-categorical.html#summarizing-1d-categorical-data-1",
    "title": "Data visualization: categorical data",
    "section": "Summarizing 1D categorical data",
    "text": "Summarizing 1D categorical data\nProportion table\n\nprop.table(table(flying_etiquette$do_you_recline))\n\n\n              never     once in a while about half the time             usually \n          0.1990632           0.2997658           0.1370023           0.2049180 \n             always \n          0.1592506 \n\nflying_etiquette |&gt; \n  count(do_you_recline) |&gt; \n  mutate(prop = n / sum(n))\n\n# A tibble: 5 × 3\n  do_you_recline          n  prop\n  &lt;fct&gt;               &lt;int&gt; &lt;dbl&gt;\n1 never                 170 0.199\n2 once in a while       256 0.300\n3 about half the time   117 0.137\n4 usually               175 0.205\n5 always                136 0.159"
  },
  {
    "objectID": "lectures/03-categorical.html#visualizing-1d-categorical-data",
    "href": "lectures/03-categorical.html#visualizing-1d-categorical-data",
    "title": "Data visualization: categorical data",
    "section": "Visualizing 1D categorical data",
    "text": "Visualizing 1D categorical data\n\n\nCreate a bar chart with geom_bar()\n\nMap do_you_recline to the x-axis\nCounts of each category are displayed on the y-axis\n\n\nflying_etiquette |&gt; \n  ggplot(aes(x = do_you_recline)) +\n  geom_bar()"
  },
  {
    "objectID": "lectures/03-categorical.html#behind-the-scenes-of-geom_bar",
    "href": "lectures/03-categorical.html#behind-the-scenes-of-geom_bar",
    "title": "Data visualization: categorical data",
    "section": "Behind the scenes of geom_bar()",
    "text": "Behind the scenes of geom_bar()\n\nstart with the data\naggregate and count the number of observations in each bar\nmap to plot aesthetics"
  },
  {
    "objectID": "lectures/03-categorical.html#visualizing-1d-categorical-data-1",
    "href": "lectures/03-categorical.html#visualizing-1d-categorical-data-1",
    "title": "Data visualization: categorical data",
    "section": "Visualizing 1D categorical data",
    "text": "Visualizing 1D categorical data\n\n\nInstead of geom_bar(), do this “by hand” (Quang prefers this way)\n\naggregate and obtain the counts first with count() or (group_by and summarize())\nthen use geom_col()\n\n\nflying_etiquette |&gt;\n  count(do_you_recline, name = \"count\") |&gt; \n  ggplot(aes(x = do_you_recline, y = count)) +\n  geom_col()"
  },
  {
    "objectID": "lectures/03-categorical.html#visualizing-1d-categorical-data-2",
    "href": "lectures/03-categorical.html#visualizing-1d-categorical-data-2",
    "title": "Data visualization: categorical data",
    "section": "Visualizing 1D categorical data",
    "text": "Visualizing 1D categorical data\n\n\nFlip your bar chart axes!\nJust simply replace x with y (Quang prefers this way)\n\nflying_etiquette |&gt;\n  ggplot(aes(y = do_you_recline)) +\n  geom_bar()\n\nOr use coord_flip()\n\nflying_etiquette |&gt; \n  ggplot(aes(x = do_you_recline)) +\n  geom_bar() +\n  coord_flip()"
  },
  {
    "objectID": "lectures/03-categorical.html#what-does-a-bar-chart-show",
    "href": "lectures/03-categorical.html#what-does-a-bar-chart-show",
    "title": "Data visualization: categorical data",
    "section": "What does a bar chart show?",
    "text": "What does a bar chart show?\n\n\nMarginal distribution: probability that a categorical variable \\(X\\) (e.g., do_you_recline) takes each particular category value \\(x\\) (always, usually, …, never)\n\nFrequency bar charts (earlier version) give info about sample size, but this could be labeled in the chart (use geom_text() or geom_label())\nNow, we create a proportion/percent bar chart to display the individual probabilities\nThis shows the probability mass function (PMF) for discrete variables\n\n(e.g. \\(P(\\) do_you_recline \\(=\\) never\\()\\))\n\n\n\n\nflying_etiquette |&gt; \n  count(do_you_recline) |&gt; \n  mutate(prop = n / sum(n)) |&gt; \n  ggplot(aes(x = prop, y = do_you_recline)) +\n  geom_col()   # + geom_label(aes(label = n), hjust = 1)"
  },
  {
    "objectID": "lectures/03-categorical.html#population-vs-sample",
    "href": "lectures/03-categorical.html#population-vs-sample",
    "title": "Data visualization: categorical data",
    "section": "Population vs sample",
    "text": "Population vs sample\nPopulation: The collection of all subjects of interest\nSample: A representative subset of the population of interest\n\nThe survey respondents is just a subset of all airplane flyers\n\n\nEmpirical distribution: estimating the true marginal distribution with observed (sample) data\n\n\n\nEstimate \\(P(\\) do_you_recline = \\(C_j\\)) with \\(\\hat p_j\\) for each category \\(C_j\\) (e.g., \\(\\hat p_\\texttt{always}\\), …, \\(\\hat p_\\texttt{never}\\))\n\nStandard error for each \\(\\hat p_j\\): \\(\\quad \\displaystyle \\text{SE}(\\hat{p}_j) = \\sqrt{\\frac{\\hat{p}_j (1 - \\hat{p}_j)}{n}}\\)"
  },
  {
    "objectID": "lectures/03-categorical.html#adding-confidence-intervals-to-bar-chart",
    "href": "lectures/03-categorical.html#adding-confidence-intervals-to-bar-chart",
    "title": "Data visualization: categorical data",
    "section": "Adding confidence intervals to bar chart",
    "text": "Adding confidence intervals to bar chart\n\n\n\n\nflying_etiquette |&gt; \n  count(do_you_recline) |&gt; \n  mutate(prop = n / sum(n),\n         se = sqrt(prop * (1 - prop) / sum(n)),\n         lower = prop - 2 * se,\n         upper = prop + 2 * se) |&gt; \n  ggplot(aes(x = prop, y = do_you_recline)) +\n  geom_col() +\n  geom_errorbar(aes(xmin = lower, xmax = upper), \n                color = \"blue\", \n                width = 0.2, \n                linewidth = 1)"
  },
  {
    "objectID": "lectures/03-categorical.html#ordering-factors-in-a-bar-chart",
    "href": "lectures/03-categorical.html#ordering-factors-in-a-bar-chart",
    "title": "Data visualization: categorical data",
    "section": "Ordering factors in a bar chart",
    "text": "Ordering factors in a bar chart\n\n\nOrder the bars by proportion\n(Let’s also flip the axes)\n\nflying_etiquette |&gt; \n  count(do_you_recline) |&gt; \n  mutate(\n    prop = n / sum(n),\n    se = sqrt(prop * (1 - prop) / sum(n)),\n    lower = prop - 2 * se,\n    upper = prop + 2 * se,\n    do_you_recline = fct_reorder(do_you_recline, prop)\n  ) |&gt; \n  ggplot(aes(x = prop, y = do_you_recline)) +\n  geom_col() +\n  geom_errorbar(aes(xmin = lower, xmax = upper), \n                color = \"blue\", \n                width = 0.2, \n                linewidth = 1)"
  },
  {
    "objectID": "lectures/03-categorical.html#pie-charts-dont-make-them",
    "href": "lectures/03-categorical.html#pie-charts-dont-make-them",
    "title": "Data visualization: categorical data",
    "section": "Pie charts… don’t make them",
    "text": "Pie charts… don’t make them\nWhy?\n\nhttps://www.data-to-viz.com/caveat/pie.html\nhttps://github.com/cxli233/FriendsDontLetFriends\n\n3D pie charts?… even worse"
  },
  {
    "objectID": "lectures/03-categorical.html#inference-for-1d-categorical-data",
    "href": "lectures/03-categorical.html#inference-for-1d-categorical-data",
    "title": "Data visualization: categorical data",
    "section": "Inference for 1D categorical data",
    "text": "Inference for 1D categorical data\nChi-square test for 1D categorical data\n\nNull hypothesis: \\(H_0\\): \\(p_1 = p_2 = \\cdots = p_K\\)\nTest statistic: \\(\\displaystyle \\chi^2 = \\sum_{j=1}^K \\frac{(O_j - E_j)^2}{E_j}\\), where\n\n\\(O_j\\): observed counts in category \\(j\\)\n\\(E_j\\) : expected counts under the null (i.e., \\(n/K\\) or each category is equally likely to occur)\n\n\n\nchisq.test(table(flying_etiquette$do_you_recline))\n\n\n    Chi-squared test for given probabilities\n\ndata:  table(flying_etiquette$do_you_recline)\nX-squared = 66.644, df = 4, p-value = 1.159e-13"
  },
  {
    "objectID": "lectures/03-categorical.html#hypothesis-testing-in-general",
    "href": "lectures/03-categorical.html#hypothesis-testing-in-general",
    "title": "Data visualization: categorical data",
    "section": "Hypothesis testing in general",
    "text": "Hypothesis testing in general\nComputing \\(p\\)-values works like this:\n\nChoose a test statistic\nCompute the test statistic using the data\nIs test statistic “unusual” compared to what we would expect under the null?\nCompare \\(p\\)-value to the target error rate (“significance level”) \\(\\alpha\\)\n\nTypically choose \\(\\alpha = 0.05\\) (the origins of 0.05)"
  },
  {
    "objectID": "lectures/03-categorical.html#summarizing-2d-categorical-data",
    "href": "lectures/03-categorical.html#summarizing-2d-categorical-data",
    "title": "Data visualization: categorical data",
    "section": "Summarizing 2D categorical data",
    "text": "Summarizing 2D categorical data\nContinuing with the flying etiquette survey data, let’s look at the responses to 2 questions\n\ndo_you_recline (Do you ever recline your seat when you fly?)\nrude_to_recline (Is it rude to recline your seat on a plane?)\n\nHow many levels does each variable have?\n\ntable(flying_etiquette$do_you_recline)\n\n\n              never     once in a while about half the time             usually \n                170                 256                 117                 175 \n             always \n                136 \n\ntable(flying_etiquette$rude_to_recline)\n\n\n      no somewhat      yes \n     502      281       71"
  },
  {
    "objectID": "lectures/03-categorical.html#summarizing-2d-categorical-data-1",
    "href": "lectures/03-categorical.html#summarizing-2d-categorical-data-1",
    "title": "Data visualization: categorical data",
    "section": "Summarizing 2D categorical data",
    "text": "Summarizing 2D categorical data\nTwo-way table (or contingency table, cross tabulation, crosstab)\n\ntable(\"Recline?\" = flying_etiquette$do_you_recline, \n      \"Rude to reline?\" = flying_etiquette$rude_to_recline)\n\n                     Rude to reline?\nRecline?               no somewhat yes\n  never                35       81  54\n  once in a while     116      129  11\n  about half the time  82       35   0\n  usually             145       27   3\n  always              124        9   3\n\nxtabs(~ do_you_recline + rude_to_recline, data = flying_etiquette)\n\n                     rude_to_recline\ndo_you_recline         no somewhat yes\n  never                35       81  54\n  once in a while     116      129  11\n  about half the time  82       35   0\n  usually             145       27   3\n  always              124        9   3"
  },
  {
    "objectID": "lectures/03-categorical.html#visualizing-2d-categorical-data",
    "href": "lectures/03-categorical.html#visualizing-2d-categorical-data",
    "title": "Data visualization: categorical data",
    "section": "Visualizing 2D categorical data",
    "text": "Visualizing 2D categorical data\n\n\nStacked bar chart: a bar chart of spine charts\nEmphasizes the marginal distribution of each category of x variable\n\ne.g., \\(P(\\) rude_to_recline \\(=\\) somewhat \\()\\)\n\nSimilar to 1D bar charts, start with counting every combination of 2 variables (using count() or group_by() and summarize()), then plot with geom_col()\n\n# flying_etiquette |&gt;\n#   ggplot(aes(x = rude_to_recline,\n#              fill = do_you_recline)) +\n#   geom_bar()\nflying_etiquette |&gt;\n  count(rude_to_recline, do_you_recline) |&gt;\n  ggplot(aes(x = rude_to_recline, y = n, \n             # filled by the other categorical variable\n             fill = do_you_recline)) + \n  geom_col()"
  },
  {
    "objectID": "lectures/03-categorical.html#visualizing-2d-categorical-data-1",
    "href": "lectures/03-categorical.html#visualizing-2d-categorical-data-1",
    "title": "Data visualization: categorical data",
    "section": "Visualizing 2D categorical data",
    "text": "Visualizing 2D categorical data\n\n\nStacked bar chart (proportion version)\n\nflying_etiquette |&gt;\n  count(rude_to_recline, do_you_recline) |&gt;\n  ggplot(aes(x = rude_to_recline, y = n, \n             fill = do_you_recline)) +\n  geom_col(position = \"fill\")"
  },
  {
    "objectID": "lectures/03-categorical.html#visualizing-2d-categorical-data-2",
    "href": "lectures/03-categorical.html#visualizing-2d-categorical-data-2",
    "title": "Data visualization: categorical data",
    "section": "Visualizing 2D categorical data",
    "text": "Visualizing 2D categorical data\n\n\nSide-by-side (grouped, dodged) bar chart: a bar chart of bar charts\nShows the conditional distribution of fill variable given x variable\n\ne.g., \\(P(\\) do_you_recline \\(=\\) always \\(\\mid\\) rude_to_recline \\(=\\) somewhat \\()\\)\n\n\nflying_etiquette |&gt;\n  count(rude_to_recline, do_you_recline) |&gt;\n  ggplot(aes(x = rude_to_recline, y = n, \n             fill = do_you_recline)) + \n  geom_col(position = \"dodge\")"
  },
  {
    "objectID": "lectures/03-categorical.html#joint-marginal-and-conditional-probabilities",
    "href": "lectures/03-categorical.html#joint-marginal-and-conditional-probabilities",
    "title": "Data visualization: categorical data",
    "section": "Joint, marginal, and conditional probabilities",
    "text": "Joint, marginal, and conditional probabilities\n\n\nLet \\(X\\) = rude_to_recline and \\(Y\\) = do_you_recline\n\nJoint distribution: frequency of the intersection\n\ne.g., \\(P(X =\\) somewhat \\(, Y =\\) always \\()\\)\n\nMarginal distribution: row sums or column sums\n\ne.g., \\(P(X =\\) somewhat \\()\\), \\(P(Y =\\) always \\()\\)\n\nConditional distribution: probability event \\(X\\) given event \\(Y\\)\n\ne.g., \\(P(X =\\) somewhat \\(\\mid Y =\\) always \\()\\)\n\n\\(\\displaystyle \\qquad \\quad = \\frac{P(X = \\texttt{somewhat}, Y = \\texttt{always})}{P(Y = \\texttt{always})}\\)\n\n\n\nflying_etiquette |&gt; \n  select(do_you_recline, rude_to_recline) |&gt; \n  table()\n\n                     rude_to_recline\ndo_you_recline         no somewhat yes\n  never                35       81  54\n  once in a while     116      129  11\n  about half the time  82       35   0\n  usually             145       27   3\n  always              124        9   3\n\nflying_etiquette |&gt; \n  select(do_you_recline, rude_to_recline) |&gt; \n  table() |&gt; \n  prop.table()\n\n                     rude_to_recline\ndo_you_recline                 no    somewhat         yes\n  never               0.040983607 0.094847775 0.063231850\n  once in a while     0.135831382 0.151053864 0.012880562\n  about half the time 0.096018735 0.040983607 0.000000000\n  usually             0.169789227 0.031615925 0.003512881\n  always              0.145199063 0.010538642 0.003512881"
  },
  {
    "objectID": "lectures/03-categorical.html#joint-marginal-and-conditional-probabilities-1",
    "href": "lectures/03-categorical.html#joint-marginal-and-conditional-probabilities-1",
    "title": "Data visualization: categorical data",
    "section": "Joint, marginal, and conditional probabilities",
    "text": "Joint, marginal, and conditional probabilities\nTwo-way proportion table (the tidyverse way) with pivot_wider\n\nflying_etiquette |&gt;\n  group_by(rude_to_recline, do_you_recline) |&gt;\n  summarize(joint = n() / nrow(flying_etiquette)) |&gt;\n  pivot_wider(names_from = rude_to_recline, values_from = joint, values_fill = 0)\n\n# A tibble: 5 × 4\n  do_you_recline          no somewhat     yes\n  &lt;fct&gt;                &lt;dbl&gt;    &lt;dbl&gt;   &lt;dbl&gt;\n1 never               0.0410   0.0948 0.0632 \n2 once in a while     0.136    0.151  0.0129 \n3 about half the time 0.0960   0.0410 0      \n4 usually             0.170    0.0316 0.00351\n5 always              0.145    0.0105 0.00351"
  },
  {
    "objectID": "lectures/03-categorical.html#categorical-heatmaps",
    "href": "lectures/03-categorical.html#categorical-heatmaps",
    "title": "Data visualization: categorical data",
    "section": "Categorical heatmaps",
    "text": "Categorical heatmaps\n\n\n\nUse geom_tile to display joint distribution of two categorical variables\nAnnotate tiles with labels of percentages using geom_text() and the scales package (a very neat package)\n\n\nflying_etiquette |&gt;\n  group_by(rude_to_recline, do_you_recline) |&gt;\n  summarize(\n    freq = n(), \n    joint = n() / nrow(flying_etiquette)\n  ) |&gt; \n  ggplot(aes(x = rude_to_recline, y = do_you_recline)) +\n  geom_tile(aes(fill = freq), color = \"white\") +\n  geom_text(aes(label = scales::percent(joint))) +\n  scale_fill_gradient2()"
  },
  {
    "objectID": "lectures/03-categorical.html#visualizing-independence",
    "href": "lectures/03-categorical.html#visualizing-independence",
    "title": "Data visualization: categorical data",
    "section": "Visualizing independence",
    "text": "Visualizing independence\n\n\nMosaic plot\n\nspine chart of spine charts\nwidth: marginal distribution of rude_to_recline\nheight: conditional distribution of do_you_recline | rude_to_recline\narea: joint distribution\n\nUsing a mosaic plot to visually check for independence:\n\ncheck whether all proportions are the same (the boxes line up in a grid)\n\n\n\nflying_etiquette |&gt; \n  select(rude_to_recline, do_you_recline) |&gt; \n  table() |&gt; \n  mosaicplot(main = \"Relationship between reclining frequency and opinion on rudeness\")"
  },
  {
    "objectID": "lectures/03-categorical.html#visualizing-independence-1",
    "href": "lectures/03-categorical.html#visualizing-independence-1",
    "title": "Data visualization: categorical data",
    "section": "Visualizing independence",
    "text": "Visualizing independence\nMosaic plot with ggmosaic package\n\nflying_etiquette |&gt; \n  ggplot() +\n  geom_mosaic(aes(x = product(do_you_recline, rude_to_recline), fill = do_you_recline))"
  },
  {
    "objectID": "lectures/03-categorical.html#inference-for-2d-categorical-data",
    "href": "lectures/03-categorical.html#inference-for-2d-categorical-data",
    "title": "Data visualization: categorical data",
    "section": "Inference for 2D categorical data",
    "text": "Inference for 2D categorical data\nChi-square test for 2D categorical data\n\nNull hypothesis: \\(H_0\\): 2 categorical variables are independent of each other\n\ne.g., no association between do_you_recline and rude_to_recline\n\nTest statistic: \\(\\displaystyle \\chi^2 = \\sum_i^{k_1} \\sum_j^{k_2} \\frac{(O_{ij} - E_{ij})^2}{E_{ij}}\\)\n\n\n# chisq.test(table(flying_etiquette$rude_to_recline, flying_etiquette$do_you_recline))\nflying_etiquette |&gt; \n  select(rude_to_recline, do_you_recline) |&gt; \n  table() |&gt; \n  chisq.test()\n\n\n    Pearson's Chi-squared test\n\ndata:  table(select(flying_etiquette, rude_to_recline, do_you_recline))\nX-squared = 316.73, df = 8, p-value &lt; 2.2e-16"
  },
  {
    "objectID": "lectures/03-categorical.html#residuals",
    "href": "lectures/03-categorical.html#residuals",
    "title": "Data visualization: categorical data",
    "section": "Residuals",
    "text": "Residuals\nRecall the test statistic: \\(\\displaystyle \\chi^2 = \\sum_i^{k_1} \\sum_j^{k_2} \\frac{(O_{ij} - E_{ij})^2}{E_{ij}}\\)\nDefine the Pearson residuals: \\(\\displaystyle r_{ij} = \\frac{O_{ij} - E_{ij}}{\\sqrt{E_{ij}}}\\)\nSome rules of thumb:\n\n\\(r_{ij} \\approx 0\\): observed counts are close to expected counts\n\\(|r_{ij}| &gt; 2\\): significant at \\(\\alpha = 0.05\\)\nvery positive \\(r_{ij}\\): higher than expected\nvery negative \\(r_{ij}\\): lower than expected"
  },
  {
    "objectID": "lectures/03-categorical.html#residuals-1",
    "href": "lectures/03-categorical.html#residuals-1",
    "title": "Data visualization: categorical data",
    "section": "Residuals",
    "text": "Residuals\nMosaic plots with boxes color-coded by Pearson residuals\nTells us which combinations of 2 categorical variables (cells) are much higher/lower than expected\n\nflying_etiquette |&gt; \n  select(rude_to_recline, do_you_recline) |&gt; \n  table() |&gt; \n  mosaicplot(main = \"Relationship between reclining frequency and opinion on rudeness\", shade = TRUE)"
  },
  {
    "objectID": "lectures/03-categorical.html#beyond-2d-facets",
    "href": "lectures/03-categorical.html#beyond-2d-facets",
    "title": "Data visualization: categorical data",
    "section": "Beyond 2D: facets!",
    "text": "Beyond 2D: facets!\n\nflying_etiquette %&gt;%\n  ggplot(aes(x = rude_to_recline, fill = do_you_recline)) + \n  geom_bar() +\n  facet_wrap(~ flight_freq)"
  },
  {
    "objectID": "lectures/03-categorical.html#the-janitor-package",
    "href": "lectures/03-categorical.html#the-janitor-package",
    "title": "Data visualization: categorical data",
    "section": "The janitor package",
    "text": "The janitor package\nThe most popular janitor function is clean_names()… for cleaning column names\n\n# before\niris |&gt; \n  head()\n\n  Sepal.Length Sepal.Width Petal.Length Petal.Width Species\n1          5.1         3.5          1.4         0.2  setosa\n2          4.9         3.0          1.4         0.2  setosa\n3          4.7         3.2          1.3         0.2  setosa\n4          4.6         3.1          1.5         0.2  setosa\n5          5.0         3.6          1.4         0.2  setosa\n6          5.4         3.9          1.7         0.4  setosa\n\n# after\nlibrary(janitor)\niris |&gt; \n  clean_names() |&gt; \n  head()\n\n  sepal_length sepal_width petal_length petal_width species\n1          5.1         3.5          1.4         0.2  setosa\n2          4.9         3.0          1.4         0.2  setosa\n3          4.7         3.2          1.3         0.2  setosa\n4          4.6         3.1          1.5         0.2  setosa\n5          5.0         3.6          1.4         0.2  setosa\n6          5.4         3.9          1.7         0.4  setosa"
  },
  {
    "objectID": "lectures/03-categorical.html#tabulation-with-the-janitor-package",
    "href": "lectures/03-categorical.html#tabulation-with-the-janitor-package",
    "title": "Data visualization: categorical data",
    "section": "Tabulation with the janitor package",
    "text": "Tabulation with the janitor package\nThe lesser-known stars of janitor: functions for tabulation of categorical data\n\n\ntabyl\n\nflying_etiquette |&gt; \n  tabyl(do_you_recline)\n\n      do_you_recline   n   percent\n               never 170 0.1990632\n     once in a while 256 0.2997658\n about half the time 117 0.1370023\n             usually 175 0.2049180\n              always 136 0.1592506\n\n\n\nadorn_*() functions\n\nflying_etiquette |&gt; \n  tabyl(do_you_recline, rude_to_recline) |&gt; \n  adorn_percentages(\"row\") |&gt; \n  adorn_pct_formatting(digits = 2) |&gt; \n  adorn_ns()\n\n      do_you_recline           no     somewhat         yes\n               never 20.59%  (35) 47.65%  (81) 31.76% (54)\n     once in a while 45.31% (116) 50.39% (129)  4.30% (11)\n about half the time 70.09%  (82) 29.91%  (35)  0.00%  (0)\n             usually 82.86% (145) 15.43%  (27)  1.71%  (3)\n              always 91.18% (124)  6.62%   (9)  2.21%  (3)\n\n\n\n\nFor more, see this overview and this tutorial\n\n\n\n\n\n\n\n\n\n\n36-SURE.github.io"
  },
  {
    "objectID": "lectures/07-density.html#data-sabrina-ionescus-shots",
    "href": "lectures/07-density.html#data-sabrina-ionescus-shots",
    "title": "Data visualization: density estimation",
    "section": "Data: Sabrina Ionescu’s shots",
    "text": "Data: Sabrina Ionescu’s shots\nShot attempts by the Sabrina Ionescu in the 2023 WNBA season using wehoop\n\nlibrary(tidyverse)\ntheme_set(theme_light())\nionescu_shots &lt;- read_csv(\"https://raw.githubusercontent.com/36-SURE/36-SURE.github.io/main/data/ionescu_shots.csv\")\nglimpse(ionescu_shots)\n\nRows: 617\nColumns: 6\n$ shot_x        &lt;dbl&gt; -10, 11, 2, -13, -18, 19, -1, 0, -1, -12, -1, -14, -7, -…\n$ shot_y        &lt;dbl&gt; 24, 23, 2, 21, 19, 16, 3, 2, 4, 5, 2, 21, 26, 14, 7, 6, …\n$ shot_distance &lt;dbl&gt; 26.000000, 25.495098, 2.828427, 24.698178, 26.172505, 24…\n$ shot_type     &lt;chr&gt; \"Pullup Jump Shot\", \"Running Jump Shot\", \"Cutting Layup …\n$ scoring_play  &lt;lgl&gt; FALSE, TRUE, FALSE, FALSE, TRUE, FALSE, TRUE, FALSE, FAL…\n$ score_value   &lt;dbl&gt; 0, 3, 0, 0, 3, 0, 2, 0, 0, 0, 0, 3, 0, 2, 0, 3, 3, 0, 0,…\n\n\n\nEach row is a shot attempt by Ionescu in the 2023 WNBA season\nCategorical / qualitative variables: scoring_play, shot_type\nContinuous / quantitative variables: shot_x, shot_y, shot_distance, score_value"
  },
  {
    "objectID": "lectures/07-density.html#revisiting-histograms",
    "href": "lectures/07-density.html#revisiting-histograms",
    "title": "Data visualization: density estimation",
    "section": "Revisiting histograms",
    "text": "Revisiting histograms\n\n\nfd_bw &lt;- 2 * IQR(ionescu_shots$shot_distance) / length(ionescu_shots$shot_distance)^(1/3)\nionescu_shots |&gt;\n  ggplot(aes(x = shot_distance)) +\n  geom_histogram(binwidth = fd_bw)\n\n\n\n\nSplit observed data into bins\nCount number of observations in each bin\n\nNeed to choose the number of bins, adjust with:\n\nbins: number of bins (default is 30)\nbinwidth: width of bins (overrides bins), various rules of thumb\n\ne.g., see fd_bw for Freedman–Diaconis rule\n\nbreaks: vector of bin boundaries (overrides both bins and binwidth)"
  },
  {
    "objectID": "lectures/07-density.html#adjusting-the-binwidth",
    "href": "lectures/07-density.html#adjusting-the-binwidth",
    "title": "Data visualization: density estimation",
    "section": "Adjusting the binwidth",
    "text": "Adjusting the binwidth\n\n\nSmall binwidth \\(\\rightarrow\\) “undersmooth” / spiky\n\nionescu_shots |&gt;\n  ggplot(aes(x = shot_distance)) +\n  geom_histogram(binwidth = 1)\n\n\n\n\n\n\n\n\n\nLarge binwidth \\(\\rightarrow\\) “oversmooth” / flat\n\nionescu_shots |&gt;\n  ggplot(aes(x = shot_distance)) +\n  geom_histogram(binwidth = 25)"
  },
  {
    "objectID": "lectures/07-density.html#adjusting-the-binwidth-1",
    "href": "lectures/07-density.html#adjusting-the-binwidth-1",
    "title": "Data visualization: density estimation",
    "section": "Adjusting the binwidth",
    "text": "Adjusting the binwidth\n\nA binwidth that is too narrow shows too much detail\n\ntoo many bins: low bias, high variance\n\nA binwidth that is too wide hides detail\n\ntoo few bins: high bias, low variance\n\nAlways pick a value that is “just right” (The Goldilocks principle)\n\nTry several values, the R / ggplot2 default is NOT guaranteed to be an optimal choice"
  },
  {
    "objectID": "lectures/07-density.html#a-subtle-point-about-the-histogram-code",
    "href": "lectures/07-density.html#a-subtle-point-about-the-histogram-code",
    "title": "Data visualization: density estimation",
    "section": "A subtle point about the histogram code…",
    "text": "A subtle point about the histogram code…\n\n\nBy default the bins are centered on the integers\n\nleft-closed, right-open intervals\nstarting at -0.5 to 0.5, 0.5 to 1.5, …\n\n\nionescu_shots |&gt;\n  ggplot(aes(x = shot_distance)) +\n  geom_histogram(binwidth = 1)\n\n\n\n\n\n\n\n\n\nSpecify center of one bin (e.g. 0.5)\n\nUse closed = \"left\"\n\n\nionescu_shots |&gt;\n  ggplot(aes(x = shot_distance)) +\n  geom_histogram(binwidth = 1, center = 0.5, \n                 closed = \"left\")"
  },
  {
    "objectID": "lectures/07-density.html#how-do-histograms-relate-to-the-pdf-and-cdf",
    "href": "lectures/07-density.html#how-do-histograms-relate-to-the-pdf-and-cdf",
    "title": "Data visualization: density estimation",
    "section": "How do histograms relate to the PDF and CDF?",
    "text": "How do histograms relate to the PDF and CDF?\n\nHistograms approximate the PDF with bins, and points are equally likely within a bin\nPDF is the derivative of the cumulative distribution function (CDF)"
  },
  {
    "objectID": "lectures/07-density.html#kernel-density-estimation",
    "href": "lectures/07-density.html#kernel-density-estimation",
    "title": "Data visualization: density estimation",
    "section": "Kernel density estimation",
    "text": "Kernel density estimation\nGoal: estimate the PDF \\(f(x)\\) for all possible values (assuming it is smooth)\n\nThe kernel density estimator (KDE) is \\(\\displaystyle \\hat{f}(x) = \\frac{1}{n} \\sum_{i=1}^n \\frac{1}{h} K_h(x - x_i)\\)\n\n\n\n\\(n\\): sample size\n\\(x\\): new point to estimate \\(f(x)\\) (does NOT have to be in the dataset!)\n\\(h\\): bandwidth, analogous to histogram binwidth, ensures \\(\\hat{f}(x)\\) integrates to 1\n\\(x_i\\): \\(i\\)th observation in the dataset\n\n\n\n\n\\(K_h(x - x_i)\\): kernel function, creates weight given distance of \\(i\\)th observation from new point\n\nas \\(|x - x_i| \\rightarrow \\infty\\) then \\(K_h(x - x_i) \\rightarrow 0\\), i.e. the further apart the \\(i\\)th observation is from \\(x\\), the smaller the weight\nas bandwidth \\(h\\) increases, weights are more evenly spread out\nChoice of kernel functions: Gaussian/normal, etc.\n\\(K_h(x - x_i)\\) is large when \\(x_i\\) is close to \\(x\\)"
  },
  {
    "objectID": "lectures/07-density.html#kernel-density-estimation-1",
    "href": "lectures/07-density.html#kernel-density-estimation-1",
    "title": "Data visualization: density estimation",
    "section": "Kernel density estimation",
    "text": "Kernel density estimation\nIntuition:\n\nsmooth each data point into a small density bumps\nsum all these small bumps together to obtain the final density estimate"
  },
  {
    "objectID": "lectures/07-density.html#how-do-we-compute-and-display-the-density-estimate",
    "href": "lectures/07-density.html#how-do-we-compute-and-display-the-density-estimate",
    "title": "Data visualization: density estimation",
    "section": "How do we compute and display the density estimate?",
    "text": "How do we compute and display the density estimate?\n\n\n\nWe make kernel density estimates with geom_density()\n\n\nionescu_shots |&gt;\n  ggplot(aes(x = shot_distance)) + \n  geom_density() +\n  geom_rug(alpha = 0.3)\n\n\nPros:\n\nDisplays full shape of distribution\nCan easily layer\nAdd categorical variable with color\n\nCons:\n\nNeed to pick bandwidth and kernel…"
  },
  {
    "objectID": "lectures/07-density.html#what-about-the-bandwidth",
    "href": "lectures/07-density.html#what-about-the-bandwidth",
    "title": "Data visualization: density estimation",
    "section": "What about the bandwidth?",
    "text": "What about the bandwidth?\nUse Gaussian reference rule (rule-of-thumb) \\(\\approx 1.06 \\cdot \\sigma \\cdot n^{-1/5}\\) (\\(\\sigma\\): observed standard deviation)\nModify the bandwidth using the adjust argument - value to multiply default bandwidth by\n\n\n\nionescu_shots |&gt;\n  ggplot(aes(x = shot_distance)) + \n  geom_density(adjust = 0.5) +\n  geom_rug(alpha = 0.3)\n\n\n\n\n\n\n\n\n\n\nionescu_shots |&gt;\n  ggplot(aes(x = shot_distance)) + \n  geom_density(adjust = 2) +\n  geom_rug(alpha = 0.3)"
  },
  {
    "objectID": "lectures/07-density.html#notes-on-density-estimation",
    "href": "lectures/07-density.html#notes-on-density-estimation",
    "title": "Data visualization: density estimation",
    "section": "Notes on density estimation",
    "text": "Notes on density estimation\n\nIn KDE, the bandwidth parameter is analogous to the binwidth in histograms.\nIf the bandwidth is too small, the density estimate can become overly peaky and the main trends in the data may be obscured.\nIf the bandwidth is too large, then smaller features in the distribution of the data may disappear\nThe choice of the kernel can affect the shape of the density curve.\n\nA Gaussian kernel typically gives density estimates that look bell-shaped (ish)\nA rectangular kernel can generate the appearance of steps in the density curve\nKernel choice matters less with more data points\n\n\nDensity plots are often reliable and informative for large datasets but can be misleading for smaller ones."
  },
  {
    "objectID": "lectures/07-density.html#common-pitfall-bounded-data",
    "href": "lectures/07-density.html#common-pitfall-bounded-data",
    "title": "Data visualization: density estimation",
    "section": "Common pitfall: bounded data",
    "text": "Common pitfall: bounded data\n\n\n\nset.seed(36)\nbounded_data &lt;- tibble(x = runif(100))\nbounded_data |&gt; \n  ggplot(aes(x)) +\n  geom_density() +\n  geom_rug(alpha = 0.5) +\n  stat_function(data = tibble(x = c(0, 1)),\n                fun = dunif, color = \"red\") +\n  scale_x_continuous(limits = c(-0.5, 1.5))\n\n\nObserve density estimates for impossible values (in the tails) - ALWAYS be mindful\nReflection method: first perform standard KDE, then “reflect” tails outside of desired interval to be inside\nSee also: evmix package"
  },
  {
    "objectID": "lectures/07-density.html#use-density-curves-and-ecdfs-together",
    "href": "lectures/07-density.html#use-density-curves-and-ecdfs-together",
    "title": "Data visualization: density estimation",
    "section": "Use density curves and ECDFs together",
    "text": "Use density curves and ECDFs together"
  },
  {
    "objectID": "lectures/07-density.html#code-interlude-easy-way-to-arrange-multiple-figures",
    "href": "lectures/07-density.html#code-interlude-easy-way-to-arrange-multiple-figures",
    "title": "Data visualization: density estimation",
    "section": "Code interlude: easy way to arrange multiple figures",
    "text": "Code interlude: easy way to arrange multiple figures\nUse the cowplot package to easily arrange your plots (see also patchwork)\n\nlibrary(cowplot)\n\nionescu_shot_dens &lt;- ionescu_shots |&gt;\n  ggplot(aes(x = shot_distance)) + \n  geom_density() +\n  geom_rug(alpha = 0.3) +\n  theme_bw() +\n  labs(x = \"Shot distance (in feet)\",\n       y = \"Number of shot attempts\")\n\nionescu_shot_ecdf &lt;- ionescu_shots |&gt;\n  ggplot(aes(x = shot_distance)) + \n  stat_ecdf() +\n  geom_rug(alpha = 0.3) +\n  theme_bw() +\n  labs(x = \"Shot distance (in feet)\",\n       y = \"Proportion of Ionescu shot attempts\")\n\n# library(patchwork)\n# ionescu_shot_dens + ionescu_shot_ecdf\nplot_grid(ionescu_shot_dens, ionescu_shot_ecdf)"
  },
  {
    "objectID": "lectures/07-density.html#use-density-curves-and-ecdfs-together-1",
    "href": "lectures/07-density.html#use-density-curves-and-ecdfs-together-1",
    "title": "Data visualization: density estimation",
    "section": "Use density curves and ECDFs together",
    "text": "Use density curves and ECDFs together"
  },
  {
    "objectID": "lectures/07-density.html#another-code-interlude-collect-the-legends-with-patchwork",
    "href": "lectures/07-density.html#another-code-interlude-collect-the-legends-with-patchwork",
    "title": "Data visualization: density estimation",
    "section": "Another code interlude: collect the legends with patchwork",
    "text": "Another code interlude: collect the legends with patchwork\n\nionescu_shot_dens_made &lt;- ionescu_shots |&gt;\n  ggplot(aes(x = shot_distance, \n             color = scoring_play)) + \n  geom_density() +\n  geom_rug(alpha = 0.3) +\n  labs(x = \"Shot distance (in feet)\",\n       y = \"Number of shot attempts\")\n\nionescu_shot_ecdf_made &lt;- ionescu_shots |&gt;\n  ggplot(aes(x = shot_distance,\n             color = scoring_play)) + \n  stat_ecdf() +\n  geom_rug(alpha = 0.3) +\n  labs(x = \"Shot distance (in feet)\",\n       y = \"Proportion of Ionescu shot attempts\")\n\nlibrary(patchwork)\nionescu_shot_dens_made + ionescu_shot_ecdf_made + plot_layout(guides = \"collect\")"
  },
  {
    "objectID": "lectures/07-density.html#ridgeline-plots",
    "href": "lectures/07-density.html#ridgeline-plots",
    "title": "Data visualization: density estimation",
    "section": "Ridgeline plots",
    "text": "Ridgeline plots\n\n\n\nCheck out the ggridges package for a variety of customization options\n\n\nlibrary(ggridges)\nionescu_shots |&gt;\n  ggplot(aes(x = shot_distance, y = shot_type)) + \n  geom_density_ridges(rel_min_height = 0.01) \n\n\nUseful to display conditional distributions across many levels"
  },
  {
    "objectID": "lectures/07-density.html#going-from-1d-to-2d-density-estimation",
    "href": "lectures/07-density.html#going-from-1d-to-2d-density-estimation",
    "title": "Data visualization: density estimation",
    "section": "Going from 1D to 2D density estimation",
    "text": "Going from 1D to 2D density estimation\nIn 1D: estimate density \\(f(x)\\), assuming that \\(f(x)\\) is smooth:\n\\[\n\\hat{f}(x) = \\frac{1}{n} \\sum_{i=1}^n \\frac{1}{h} K_h(x - x_i)\n\\]\n\nIn 2D: estimate joint density \\(f(x_1, x_2)\\)\n\\[\\hat{f}(x_1, x_2) = \\frac{1}{n} \\sum_{i=1}^n \\frac{1}{h_1h_2} K\\left(\\frac{x_1 - x_{i1}}{h_1}\\right) K\\left(\\frac{x_2 - x_{i2}}{h_2}\\right)\\]\n\n\nIn 1D there’s one bandwidth, now we have two bandwidths\n\n\\(h_1\\): controls smoothness as \\(X_1\\) changes, holding \\(X_2\\) fixed\n\\(h_2\\): controls smoothness as \\(X_2\\) changes, holding \\(X_1\\) fixed\n\nGaussian kernels are still a popular choice"
  },
  {
    "objectID": "lectures/07-density.html#display-densities-for-2d-data",
    "href": "lectures/07-density.html#display-densities-for-2d-data",
    "title": "Data visualization: density estimation",
    "section": "Display densities for 2D data",
    "text": "Display densities for 2D data"
  },
  {
    "objectID": "lectures/07-density.html#how-to-read-contour-plots",
    "href": "lectures/07-density.html#how-to-read-contour-plots",
    "title": "Data visualization: density estimation",
    "section": "How to read contour plots",
    "text": "How to read contour plots\nBest known in topography: outlines (contours) denote levels of elevation"
  },
  {
    "objectID": "lectures/07-density.html#d-density-estimation",
    "href": "lectures/07-density.html#d-density-estimation",
    "title": "Data visualization: density estimation",
    "section": "2D density estimation",
    "text": "2D density estimation\nWe can visualize all of the shot locations: (shot_x, shot_y)\n\n\n\nionescu_shots |&gt;\n  ggplot(aes(x = shot_x, y = shot_y)) +\n  geom_point(size = 4, alpha = 0.3)\n\n\nAdjust transparency with alpha for overlapping points"
  },
  {
    "objectID": "lectures/07-density.html#create-contours-of-2d-kernel-density-estimate-kde",
    "href": "lectures/07-density.html#create-contours-of-2d-kernel-density-estimate-kde",
    "title": "Data visualization: density estimation",
    "section": "Create contours of 2D kernel density estimate (KDE)",
    "text": "Create contours of 2D kernel density estimate (KDE)\n\n\n\nUse geom_density2d() to display contour lines\n\n\nionescu_shots |&gt;\n  filter(shot_y &lt; 35) |&gt; # remove outliers\n  ggplot(aes(x = shot_x, y = shot_y)) + \n  geom_point(size = 4, alpha = 0.3) + \n  geom_density2d() +\n  coord_fixed() +\n  theme(legend.position = \"bottom\")\n\n\nExtend KDE for joint density estimates in 2D (see section 14.4.2 for details)\nInner lines denote “peaks”\ncoord_fixed() forced a fixed ratio"
  },
  {
    "objectID": "lectures/07-density.html#create-contours-of-2d-kernel-density-estimate-kde-1",
    "href": "lectures/07-density.html#create-contours-of-2d-kernel-density-estimate-kde-1",
    "title": "Data visualization: density estimation",
    "section": "Create contours of 2D kernel density estimate (KDE)",
    "text": "Create contours of 2D kernel density estimate (KDE)\n\n\n\nWe make 2D KDE contour plots using geom_density2d()\n\n\nionescu_shots |&gt;\n  filter(shot_y &lt; 35) |&gt; \n  ggplot(aes(x = shot_x, y = shot_y)) + \n  geom_point(size = 4, alpha = 0.3) + \n  geom_density2d(adjust = 0.1) +\n  coord_fixed() +\n  theme(legend.position = \"bottom\")\n\n\nCan use adjust to modify the multivariate bandwidth"
  },
  {
    "objectID": "lectures/07-density.html#contours-are-difficult-lets-make-a-heatmap-instead",
    "href": "lectures/07-density.html#contours-are-difficult-lets-make-a-heatmap-instead",
    "title": "Data visualization: density estimation",
    "section": "Contours are difficult… let’s make a heatmap instead",
    "text": "Contours are difficult… let’s make a heatmap instead\n\n\n\nUse stat_density_2d() and the after_stat() function to make 2D KDE heatmaps\nMay be easier to read than nested lines with color\nDefault color scale is awful. Always change it.\n\n\nionescu_shots |&gt;\n  filter(shot_y &lt; 35) |&gt; \n  ggplot(aes(x = shot_x, y = shot_y)) + \n  stat_density2d(aes(fill = after_stat(level)),\n                 h = 0.6, bins = 60, geom = \"polygon\") +\n  scale_fill_gradient(low = \"midnightblue\", \n                      high = \"gold\") +\n  coord_fixed() +\n  theme(legend.position = \"bottom\")\n\nMultivariate density estimation can be difficult"
  },
  {
    "objectID": "lectures/07-density.html#turn-off-contours-and-use-tiles-instead",
    "href": "lectures/07-density.html#turn-off-contours-and-use-tiles-instead",
    "title": "Data visualization: density estimation",
    "section": "Turn off contours and use tiles instead",
    "text": "Turn off contours and use tiles instead\n\n\n\nDivide the space into a grid and color the grid according to high/low values\n\n\nionescu_shots |&gt;\n  filter(shot_y &lt; 35) |&gt; \n  ggplot(aes(x = shot_x, y = shot_y)) + \n  stat_density2d(aes(fill = after_stat(density)),\n                 h = 0.6, bins = 60, contour = FALSE,\n                 geom = \"raster\") +\n  # scale_fill_gradient(low = \"white\", high = \"red\") +\n  scale_fill_gradient(low = \"midnightblue\", \n                      high = \"gold\") +\n  theme(legend.position = \"bottom\") +\n  coord_fixed()"
  },
  {
    "objectID": "lectures/07-density.html#best-alternative-hexagonal-binning",
    "href": "lectures/07-density.html#best-alternative-hexagonal-binning",
    "title": "Data visualization: density estimation",
    "section": "Best alternative? Hexagonal binning",
    "text": "Best alternative? Hexagonal binning\n\n\n\nUse geom_hex() to make hexagonal heatmaps\nNeed to have the hexbin package installed\n2D version of histogram\n\n\nionescu_shots |&gt;\n  filter(shot_y &lt; 35) |&gt;\n  ggplot(aes(x = shot_x, y = shot_y)) + \n  geom_hex(binwidth = c(1, 1)) +\n  scale_fill_gradient(low = \"midnightblue\", \n                      high = \"gold\") + \n  theme(legend.position = \"bottom\") +\n  coord_fixed()\n\n\nCan specify binwidth in both directions\nAvoids limitations from smoothing"
  },
  {
    "objectID": "lectures/07-density.html#what-about-shooting-efficiency",
    "href": "lectures/07-density.html#what-about-shooting-efficiency",
    "title": "Data visualization: density estimation",
    "section": "What about shooting efficiency?",
    "text": "What about shooting efficiency?\n\nCan compute a function of another variable inside hexagons with stat_summary_hex()\n\n\n\n\nionescu_shots |&gt;\n  filter(shot_y &lt; 35) |&gt;\n  ggplot(aes(x = shot_x, y = shot_y, \n             z = scoring_play, group = -1)) +\n  stat_summary_hex(binwidth = c(2, 2), fun = mean, \n                   color = \"black\") +\n  scale_fill_gradient(low = \"midnightblue\", \n                      high = \"gold\") + \n  theme(legend.position = \"bottom\") +\n  coord_fixed()"
  },
  {
    "objectID": "lectures/07-density.html#appendix-making-shot-charts-and-drawing-courts-with-sportyr",
    "href": "lectures/07-density.html#appendix-making-shot-charts-and-drawing-courts-with-sportyr",
    "title": "Data visualization: density estimation",
    "section": "Appendix: Making shot charts and drawing courts with sportyR",
    "text": "Appendix: Making shot charts and drawing courts with sportyR\n\nlibrary(sportyR)\nwnba_court &lt;- geom_basketball(\"wnba\", display_range = \"offense\", rotation = 270, x_trans = -41.5)\nwnba_court +\n  geom_hex(data = ionescu_shots, aes(x = shot_x, y = shot_y), binwidth = c(1, 1)) + \n  scale_fill_gradient(low = \"midnightblue\", high = \"gold\")"
  },
  {
    "objectID": "lectures/07-density.html#appendix-code-to-build-dataset",
    "href": "lectures/07-density.html#appendix-code-to-build-dataset",
    "title": "Data visualization: density estimation",
    "section": "Appendix: Code to build dataset",
    "text": "Appendix: Code to build dataset\n\n# install.packages(\"wehoop\")\nlibrary(wehoop)\nwnba_pbp &lt;- load_wnba_pbp(2023)\nionescu_shots &lt;- wnba_pbp |&gt; \n  filter(shooting_play) |&gt; \n  filter(str_detect(text, \"Ionescu\")) |&gt; \n  filter(!str_detect(text, \"Ionescu assists\")) |&gt; \n  filter(!str_detect(text, \"free throw\")) |&gt; \n  transmute(\n    shot_x = coordinate_x_raw - 25,\n    shot_y = coordinate_y_raw,\n    shot_distance = sqrt((abs(shot_x) ^ 2) + shot_y ^ 2), \n    shot_type = type_text,\n    scoring_play,\n    score_value\n  )\n\n\n\n36-SURE.github.io"
  },
  {
    "objectID": "lectures/08-simulation.html#why-simulation",
    "href": "lectures/08-simulation.html#why-simulation",
    "title": "Simulation",
    "section": "Why simulation?",
    "text": "Why simulation?\n\n\n\nWe’re in the 21st century!\nSimulations can often be\n\neasier than hand calculations\nmade more realistic than hand calculations\n\nR provides unique access to great (statistical) simulation tools (compared to other languages)"
  },
  {
    "objectID": "lectures/08-simulation.html#sampling-from-a-given-vector",
    "href": "lectures/08-simulation.html#sampling-from-a-given-vector",
    "title": "Simulation",
    "section": "Sampling from a given vector",
    "text": "Sampling from a given vector\n\nTo sample from a given vector, use sample()\n\n\n# base R built in English alphabet\n# letters\nsample(letters, size = 5) # sample without replacement, by default\n\n[1] \"k\" \"r\" \"o\" \"j\" \"d\"\n\nsample(c(0, 1), size = 7, replace = TRUE) # sample with replacement\n\n[1] 0 1 0 0 1 1 0\n\n# 5 (independent) coin tosses\ncoin &lt;- c(\"H\", \"T\")\nsample(coin, 5, replace = TRUE)\n\n[1] \"H\" \"H\" \"H\" \"T\" \"H\"\n\nsample(1:100, 1) # sample a random integer between 1 and 100\n\n[1] 77"
  },
  {
    "objectID": "lectures/08-simulation.html#probability-distributions",
    "href": "lectures/08-simulation.html#probability-distributions",
    "title": "Simulation",
    "section": "Probability distributions",
    "text": "Probability distributions\nA distribution is a mathematical function \\(f(x \\mid \\theta)\\) where\n\n\\(x\\) may take on continuous or discrete values over the domain (i.e. all possible inputs) of \\(f(x \\mid \\theta)\\)\n\\(\\theta\\) is a set of parameters governing the shape of the distribution\n\ne.g. \\(\\theta = \\{\\mu, \\sigma ^2 \\}\\) for a normal (Gaussian) distribution\n\nthe \\(\\mid\\) symbol means that the shape of the distribution is conditional on the values of \\(\\theta\\)\n\n\nLet \\(f\\) denote the distribution for its\n\nprobability density function (PDF) if \\(x\\) is continuous\nprobability mass function (PMF) if \\(x\\) is discrete\n\n\n\nNote:\n\n\\(f(x \\mid \\theta) \\geq 0\\) for all \\(x\\)\n\\(\\displaystyle \\sum_x f(x \\mid \\theta) = 1\\) (discrete case) or \\(\\displaystyle \\int_x f(x \\mid \\theta) = 1\\) (continuous case)"
  },
  {
    "objectID": "lectures/08-simulation.html#normal-distribution",
    "href": "lectures/08-simulation.html#normal-distribution",
    "title": "Simulation",
    "section": "Normal distribution",
    "text": "Normal distribution\n\n\n\nPDF: \\(\\displaystyle f(x \\mid \\mu, \\sigma^2)=\\frac{1}{\\sigma \\sqrt{2 \\pi}} e^{-\\frac{1}{2}\\left(\\frac{x-\\mu}{\\sigma}\\right)^{2}}\\); \\(x \\in (- \\infty, \\infty)\\)\nWe write \\(X \\sim N(\\mu, \\sigma^2)\\)\nStandard normal distribution: \\(N(0, 1)\\)\n\n\nlibrary(tidyverse)\ntheme_set(theme_light())\ntibble(x = c(-5, 5)) |&gt; \n  ggplot(aes(x)) +\n  stat_function(fun = dnorm, color = \"blue\",\n                args = list(mean = 0, sd = 1)) +\n  stat_function(fun = dnorm, color = \"red\",\n                args = list(mean = -2, sd = sqrt(0.5)))"
  },
  {
    "objectID": "lectures/08-simulation.html#binomial-distribution",
    "href": "lectures/08-simulation.html#binomial-distribution",
    "title": "Simulation",
    "section": "Binomial distribution",
    "text": "Binomial distribution\n\n\n\nPMF: \\(\\displaystyle f(x \\mid n, p)= \\binom{n}{x} p^{x}(1-p)^{n-x}\\); \\(x = 0, 1, \\dots, n\\)\nModel for the probability of \\(x\\) successes in \\(n\\) independent trials, each with success probability \\(p\\)\nWe write \\(X \\sim \\text{Binomial}(n,p)\\)\n\n\ntibble(x = 0:20) |&gt;\n  mutate(binom1 = dbinom(x, size = 20, prob = 0.5),\n         binom2 = dbinom(x, size = 20, prob = 0.1)) |&gt;\n  ggplot(aes(x)) + \n  geom_point(aes(y = binom1), color = \"blue\", size = 4) +\n  geom_point(aes(y = binom2), color = \"red\", size = 4)"
  },
  {
    "objectID": "lectures/08-simulation.html#poisson-distribution",
    "href": "lectures/08-simulation.html#poisson-distribution",
    "title": "Simulation",
    "section": "Poisson distribution",
    "text": "Poisson distribution\n\n\n\nPMF: \\(\\displaystyle f(x \\mid \\lambda)= \\frac{ e^{-\\lambda} \\lambda^x}{x!}\\); \\(x = 0, 1, 2, \\dots\\) and \\(\\lambda &gt; 0\\)\nModel for the counts of an event in a fixed period of time, with a rate of occurrence parameter \\(\\lambda\\)\nWe write \\(X \\sim \\text{Poisson}(\\lambda)\\)\n\n\ntibble(x = 0:10) |&gt; \n  mutate(y = dpois(x, 1)) |&gt; \n  ggplot(aes(x, y)) +\n  geom_point(size = 4)"
  },
  {
    "objectID": "lectures/08-simulation.html#random-number-generation",
    "href": "lectures/08-simulation.html#random-number-generation",
    "title": "Simulation",
    "section": "Random number generation",
    "text": "Random number generation\nExample: Sampling from a normal distribution\n\nrnorm(): generate normal random variables\npnorm(): normal cumulative distribution function\ndnorm(): normal density function\nqnorm(): normal quantile function\n\n\nNote: Replace “norm” with the name of another distribution, all the same functions apply.\n\nE.g., “t”, “exp”, “gamma”, “chisq”, “binom”, “pois”, “unif”, etc.\n\nSee this manual for more details"
  },
  {
    "objectID": "lectures/08-simulation.html#random-number-generation-1",
    "href": "lectures/08-simulation.html#random-number-generation-1",
    "title": "Simulation",
    "section": "Random number generation",
    "text": "Random number generation\n\n# these are the defaults for mean and sd\nz &lt;- rnorm(1000, mean = 0, sd = 1)\n# check: the sample mean is approximately 0\nmean(z) \n\n[1] 0.04957282\n\n# check: the sample sd is approximately 1\nsd(z)\n\n[1] 1.00069"
  },
  {
    "objectID": "lectures/08-simulation.html#revisiting-ecdf",
    "href": "lectures/08-simulation.html#revisiting-ecdf",
    "title": "Simulation",
    "section": "Revisiting ECDF",
    "text": "Revisiting ECDF\nRecall that we can use the ECDF to estimate the true cumulative distribution function\n\n\n\nz_ecdf &lt;- ecdf(z)\nz_ecdf(0) # should get close to 1/2\n\n[1] 0.477\n\nclass(z_ecdf)\n\n[1] \"ecdf\"     \"stepfun\"  \"function\"\n\nnormal_tbl &lt;- tibble(z = sort(z)) |&gt; \n  mutate(empirical = z_ecdf(z),\n         true = pnorm(z))\nnormal_tbl\n\n# A tibble: 1,000 × 3\n       z empirical        true\n   &lt;dbl&gt;     &lt;dbl&gt;       &lt;dbl&gt;\n 1 -4.88     0.001 0.000000528\n 2 -3.64     0.002 0.000135   \n 3 -3.32     0.003 0.000448   \n 4 -2.90     0.004 0.00188    \n 5 -2.80     0.005 0.00255    \n 6 -2.72     0.006 0.00325    \n 7 -2.68     0.007 0.00365    \n 8 -2.67     0.008 0.00377    \n 9 -2.50     0.009 0.00625    \n10 -2.47     0.01  0.00673    \n# ℹ 990 more rows\n\n\n\n\nnormal_tbl |&gt; \n  pivot_longer(!z, \n               names_to = \"method\", \n               values_to = \"val\") |&gt; \n  ggplot(aes(x = z, y = val, color = method)) +\n  geom_line()"
  },
  {
    "objectID": "lectures/08-simulation.html#stick-breaking-problem",
    "href": "lectures/08-simulation.html#stick-breaking-problem",
    "title": "Simulation",
    "section": "Stick breaking problem",
    "text": "Stick breaking problem\nIf a stick of unit length is broken in two at random, what is the average ratio of the smaller length to the larger?\n\nFirst… any guesses? (Also, any guess on what the average length of the smaller (or larger) piece is?)\n\n\n\nx &lt;- runif(10000)\nsmaller &lt;- ifelse(x &lt; 0.5, x, 1 - x)\nratio &lt;- smaller / (1 - smaller)\n# get a distribution\n# hist(ratio)\nmean(ratio) # exact answer: 2 * log(2) - 1\n\n[1] 0.3841102\n\n\n\n\nHow would you do this by hand…?"
  },
  {
    "objectID": "lectures/08-simulation.html#estimating-pi-using-monte-carlo-simulation",
    "href": "lectures/08-simulation.html#estimating-pi-using-monte-carlo-simulation",
    "title": "Simulation",
    "section": "Estimating \\(\\pi\\) using Monte Carlo simulation",
    "text": "Estimating \\(\\pi\\) using Monte Carlo simulation\n\nMonte Carlo methods rely on repeated random sampling to obtain numerical results\nUse randomness to solve problems that might be deterministic in principle\n\n\n\nExample: Estimating \\(\\pi\\)\n\nSimulate random \\((x, y)\\) points with domain as a square of side \\(2r\\) units centered at the origin\nConsider a circle inside the same domain with same radius \\(r\\) and inscribed into the square\nCalculate the ratio of number points inside the circle and total number of generated points (Why?)"
  },
  {
    "objectID": "lectures/08-simulation.html#estimating-pi-using-monte-carlo-simulation-1",
    "href": "lectures/08-simulation.html#estimating-pi-using-monte-carlo-simulation-1",
    "title": "Simulation",
    "section": "Estimating \\(\\pi\\) using Monte Carlo simulation",
    "text": "Estimating \\(\\pi\\) using Monte Carlo simulation\n\n\n\nn_points &lt;- 10000\nx &lt;- runif(n_points, -1, 1)\ny &lt;- runif(n_points, -1, 1)\ninside &lt;- ifelse(x^2 + y^2 &lt;= 1, 1, 0)\n4 * mean(inside)\n\n[1] 3.164\n\n\n\n\ntibble(x, y, inside) |&gt; \n  ggplot(aes(x, y, color = factor(inside))) +\n  geom_point(show.legend = FALSE) +\n  coord_equal()"
  },
  {
    "objectID": "lectures/08-simulation.html#same-command-different-results",
    "href": "lectures/08-simulation.html#same-command-different-results",
    "title": "Simulation",
    "section": "Same command, different results?",
    "text": "Same command, different results?\nNot surprisingly, we get different sample draws each time we call rnorm()\n\nmean(rnorm(100))\n\n[1] -0.1365536\n\nmean(rnorm(100))\n\n[1] -0.01041544\n\nmean(rnorm(100))\n\n[1] 0.1023712"
  },
  {
    "objectID": "lectures/08-simulation.html#is-it-really-random",
    "href": "lectures/08-simulation.html#is-it-really-random",
    "title": "Simulation",
    "section": "Is it really random?",
    "text": "Is it really random?\n\n\n\nRandom numbers generated in R (or any language) are not truly random; they are what we call pseudorandom\n\nThese are numbers generated by computer algorithms that very closely mimick truly random numbers\nThe default algorithm in R is called the Mersenne Twister\n\n\n\n\n\n\n\nTo learn more, type ?Random into R (check out how to change the algorithm used for pseudorandom number generation, which you should never really have to do…)"
  },
  {
    "objectID": "lectures/08-simulation.html#setting-the-random-seed",
    "href": "lectures/08-simulation.html#setting-the-random-seed",
    "title": "Simulation",
    "section": "Setting the random seed",
    "text": "Setting the random seed\n\nAll pseudorandom number generators depend on what is called a seed value\nThis puts the random number generator in a well-defined state, so that the numbers it generates, from then on, will be reproducible\nThe seed is just an integer, and can be set with set.seed()\nThe reason we set it: so that when someone else runs our simulation code, they can see the same—albeit, still random—results that we do\nNote: set.seed() will be helpful later on for things like cross-validation, \\(k\\)-means clustering, etc. — basically anything that involves randomly sampling of the data"
  },
  {
    "objectID": "lectures/08-simulation.html#setting-the-random-seed-1",
    "href": "lectures/08-simulation.html#setting-the-random-seed-1",
    "title": "Simulation",
    "section": "Setting the random seed",
    "text": "Setting the random seed\nSame seed, same results\n\nset.seed(1999)\nrnorm(3)\n\n[1]  0.73267249 -0.03782971  1.20300914\n\nset.seed(1999)\nrnorm(3)\n\n[1]  0.73267249 -0.03782971  1.20300914\n\nset.seed(1999)\nrnorm(3)\n\n[1]  0.73267249 -0.03782971  1.20300914"
  },
  {
    "objectID": "lectures/08-simulation.html#example-drug-effect-model",
    "href": "lectures/08-simulation.html#example-drug-effect-model",
    "title": "Simulation",
    "section": "Example: drug effect model",
    "text": "Example: drug effect model\nSuppose we have a model for the way a drug affected certain patients\n\nAll patients will undergo chemotherapy. We believe those who aren’t given the drug experience a reduction in tumor size of percentage \\(X_{\\mathrm{no\\,drug}} \\sim 100 \\cdot \\mathrm{Exponential}(R)\\), where \\(R \\sim \\mathrm{Uniform}(0,1)\\)\nAnd those who were given the drug experience a reduction in tumor size of percentage \\(X_{\\mathrm{drug}} \\sim 100 \\cdot \\mathrm{Exp}(2)\\)\n\n\nSuppose some scientist collaborators are wondering how many patients would we need to have in each group (drug, no drug), in order to reliably see that the average reduction in tumor size is large…\nWhat would you do?\n\nBefore: get out your pen and paper, make some approximations\nNow: just simulate from the model, no approximations"
  },
  {
    "objectID": "lectures/08-simulation.html#example-drug-effect-model-1",
    "href": "lectures/08-simulation.html#example-drug-effect-model-1",
    "title": "Simulation",
    "section": "Example: drug effect model",
    "text": "Example: drug effect model\n\n\n\n# suppose each group has 50 subjects\nset.seed(100)\nn_subjects &lt;- 50 \nmean_drug &lt;- 2\nmean_nodrug &lt;- runif(n_subjects, 0, 1)\nx_drug &lt;- 100 * rexp(n_subjects, 1 / mean_drug) \nx_nodrug &lt;- 100 * rexp(n_subjects, 1 / mean_nodrug)\n\ntibble(x_drug, x_nodrug) |&gt; \n  pivot_longer(everything(),\n               names_to = \"group\",\n               names_prefix = \"x_\",\n               values_to = \"reduction\") |&gt; \n  ggplot(aes(x = reduction, y = after_stat(density), \n             color = group)) +\n  geom_histogram(aes(fill = group), \n                 alpha = 0.5, color = \"black\",\n                 position = \"identity\") +\n  geom_density(aes(color = group))"
  },
  {
    "objectID": "lectures/08-simulation.html#repetition-and-reproducibility",
    "href": "lectures/08-simulation.html#repetition-and-reproducibility",
    "title": "Simulation",
    "section": "Repetition and reproducibility",
    "text": "Repetition and reproducibility\n\nOne single simulation is not always trustworthy (depends on the situation, of course)\nIn general, simulations should be repeated and aggregate results reported — requires iteration!\nTo make random number draws reproducible, we must set the seed with set.seed()\nMore than this, to make simulation results reproducible, we need to follow good programming practices (see this for example)\nGold standard: any time you show a simulation result (a figure, a table, etc.), you have code that can be run (by anyone) to produce exactly the same result"
  },
  {
    "objectID": "lectures/08-simulation.html#iteration-and-simulation-1",
    "href": "lectures/08-simulation.html#iteration-and-simulation-1",
    "title": "Simulation",
    "section": "Iteration and simulation",
    "text": "Iteration and simulation\n\nWriting a function to complete a single run of your simulation/analysis is often very helpful\nThis allows the simulation itself to be intricate (e.g., intricate steps, several simulation parameters), but makes running the simulation simple\nThen you can use iteration to run your simulation/analysis over and over again"
  },
  {
    "objectID": "lectures/08-simulation.html#iteration-and-simulation-2",
    "href": "lectures/08-simulation.html#iteration-and-simulation-2",
    "title": "Simulation",
    "section": "Iteration and simulation",
    "text": "Iteration and simulation\nExample: Revisiting \\(k\\)-means clustering with gapminder data - compute the total within-cluster variation for different values of \\(k\\)\n\nlibrary(dslabs)\nclean_gapminder &lt;- gapminder |&gt;\n  filter(year == 2011, !is.na(gdp)) |&gt;\n  mutate(std_log_gdp = as.numeric(scale(log(gdp), center = TRUE, scale = TRUE)),\n         std_life_exp = as.numeric(scale(life_expectancy, center = TRUE, scale = TRUE)))\n\n\n# function to perform clustering for each value of k\ngapminder_kmeans &lt;- function(k) {\n  kmeans_results &lt;- clean_gapminder |&gt;\n    select(std_log_gdp, std_life_exp) |&gt;\n    kmeans(centers = k, nstart = 30)\n  \n  kmeans_out &lt;- tibble(clusters = k,\n                       total_wss = kmeans_results$tot.withinss)\n  return(kmeans_out)\n}\n\n# number of clusters to search over\nn_clusters_search &lt;- 2:12\n\n# iterate over each cluster value to compute total wss\nkmeans_search &lt;- n_clusters_search |&gt; \n  map(gapminder_kmeans) |&gt; \n  bind_rows()"
  },
  {
    "objectID": "lectures/08-simulation.html#pre-allocation",
    "href": "lectures/08-simulation.html#pre-allocation",
    "title": "Simulation",
    "section": "Pre-allocation",
    "text": "Pre-allocation\nExample: When 100 coins are tossed, what is the probability that exactly 50 are heads?\n\nlibrary(tictoc)\nn_runs &lt;- 500000\na &lt;- c()\ntic()\nfor (i in 1:n_runs) {\n  tosses &lt;- sample(0:1, size = 100, replace = TRUE)\n  a[i] &lt;- sum(tosses)\n}\ntoc()\n\nb &lt;- rep(NA, n_runs)\ntic()\nfor (i in 1:n_runs) {\n  tosses &lt;- sample(0:1, size = 100, replace = TRUE)\n  b[i] &lt;- sum(tosses)\n}\ntoc()\n\n# exact: (factorial(100) / (factorial(50) * factorial(50))) * (1 / 2) ^ 100\nmean(b == 50)"
  },
  {
    "objectID": "lectures/08-simulation.html#pre-allocation-1",
    "href": "lectures/08-simulation.html#pre-allocation-1",
    "title": "Simulation",
    "section": "Pre-allocation",
    "text": "Pre-allocation\n\n\n\nNot only computations take time, memory allocations do too\nChanging the size of a vector takes just about as long as creating a new vector does\nEach time the size changes, R needs to reconsider its allocation of memory to the object\nNever reallocate a vector after each iteration\n\n\nAnalogy"
  },
  {
    "objectID": "lectures/08-simulation.html#readingwriting-fromto-a-file",
    "href": "lectures/08-simulation.html#readingwriting-fromto-a-file",
    "title": "Simulation",
    "section": "Reading/writing from/to a file",
    "text": "Reading/writing from/to a file\nSometimes simulations/analyses take a long time to run, and we want to save intermediate or final output, for quick reference later\nIntroducing the readr package (part of tidyverse; automatically loaded)\n\nwrite_*() functions: exporting data\nread_*() functions: importing data"
  },
  {
    "objectID": "lectures/08-simulation.html#readingwriting-fromto-a-file-1",
    "href": "lectures/08-simulation.html#readingwriting-fromto-a-file-1",
    "title": "Simulation",
    "section": "Reading/writing from/to a file",
    "text": "Reading/writing from/to a file\n\nwrite_csv() / read_csv(): export / import single R data frames or tibbles in .csv format\nwrite_rds() / read_rds(): export / import single R objects (like a vector, matrix, list, data frame, etc.) in .rds format\n\nNote that by default, the file will be written to the working directory (i.e. if you just specify the file name)\n\nexample_df &lt;- tibble(x = rnorm(100), y = rnorm(100))\nwrite_rds(example_df, \"INSERT PATH/example_df.csv\")\ndf &lt;- read_csv(\"INSERT PATH/example_df.csv\")\n\nexample_obj &lt;- matrix(rnorm(25), 5, 5)\nwrite_rds(example_obj, \"INSERT PATH/example_obj.rds\")\nobj &lt;- read_rds(\"INSERT PATH/example_obj.rds\")\n\nExample: saving \\(k\\)-means clustering results from earlier\n\n# saving to a folder named \"data\" in the working directory\nwrite_csv(kmeans_search, \"data/kmeans_search_results.csv\")"
  },
  {
    "objectID": "lectures/08-simulation.html#file-path-and-working-directory",
    "href": "lectures/08-simulation.html#file-path-and-working-directory",
    "title": "Simulation",
    "section": "File path and working directory",
    "text": "File path and working directory\n\nTo read in a file, you need to use the correct path, which should be relative and NOT absolute (or a path pointing to a location outside of the project directory) (read more here)\nThe key to getting paths to work is to understand working directory. In R, use the function getwd()\n\nNote: NEVER use setwd() to change working directory. It’s a bad practice. (Here’s why)"
  },
  {
    "objectID": "lectures/08-simulation.html#file-path-and-working-directory-1",
    "href": "lectures/08-simulation.html#file-path-and-working-directory-1",
    "title": "Simulation",
    "section": "File path and working directory",
    "text": "File path and working directory\n\nSpecial paths\n\n. is the working directory\n~ is the home directory (e.g., on Quang’s laptop: /Users/qntkhvn)\n.. is the parent directory. (e.g., ../steve.csv refers to a file called steve.csv in the directory that is one level above the working directory)\n\nCommon issue: By default, the working directory for an R Markdown or Quarto document is the directory in which that document is stored. This is NOT necessarily the working directory of your current R session.\nUse list.files() to see what files are available in the working directory (or any other directory)\n\n\nlist.files()\nlist.files(\"~\")"
  },
  {
    "objectID": "lectures/08-simulation.html#resources",
    "href": "lectures/08-simulation.html#resources",
    "title": "Simulation",
    "section": "Resources",
    "text": "Resources\n\nhere package\nfertile package\nWhat They Forgot to Teach You About R"
  },
  {
    "objectID": "lectures/08-simulation.html#in-reality",
    "href": "lectures/08-simulation.html#in-reality",
    "title": "Simulation",
    "section": "In reality…",
    "text": "In reality…\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n36-SURE.github.io"
  },
  {
    "objectID": "lectures/02-graphics.html#goals-of-data-visualization",
    "href": "lectures/02-graphics.html#goals-of-data-visualization",
    "title": "Data visualization: the grammar of graphics and ggplot2",
    "section": "Goals of data visualization",
    "text": "Goals of data visualization\n\nto represent the data in a visual way (enough with tables… though tables are useful in a lot of situations)\n\n\n\nmost importantly, to deliver the information to your audience and help them understand the story behind the data\n\n\n\n\nData Visualization (good DataViz anyway) answers a question. - Greggy J. M."
  },
  {
    "objectID": "lectures/02-graphics.html#always-visualize-your-data-before-modeling-and-analysis",
    "href": "lectures/02-graphics.html#always-visualize-your-data-before-modeling-and-analysis",
    "title": "Data visualization: the grammar of graphics and ggplot2",
    "section": "ALWAYS visualize your data before modeling and analysis",
    "text": "ALWAYS visualize your data before modeling and analysis\nAnscombe’s quartet\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n# A tibble: 4 × 6\n  set   x_mean x_var y_mean y_var x_y_cor\n  &lt;chr&gt;  &lt;dbl&gt; &lt;dbl&gt;  &lt;dbl&gt; &lt;dbl&gt;   &lt;dbl&gt;\n1 1          9    11   7.50  4.13   0.816\n2 2          9    11   7.50  4.13   0.816\n3 3          9    11   7.5   4.12   0.816\n4 4          9    11   7.50  4.12   0.817"
  },
  {
    "objectID": "lectures/02-graphics.html#always-visualize-your-data-before-modeling-and-analysis-1",
    "href": "lectures/02-graphics.html#always-visualize-your-data-before-modeling-and-analysis-1",
    "title": "Data visualization: the grammar of graphics and ggplot2",
    "section": "ALWAYS visualize your data before modeling and analysis",
    "text": "ALWAYS visualize your data before modeling and analysis\nThe Datasaurus dozen\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n# A tibble: 13 × 6\n   dataset    x_mean x_var y_mean y_var x_y_cor\n   &lt;chr&gt;       &lt;dbl&gt; &lt;dbl&gt;  &lt;dbl&gt; &lt;dbl&gt;   &lt;dbl&gt;\n 1 away         54.3  281.   47.8  726. -0.0641\n 2 bullseye     54.3  281.   47.8  726. -0.0686\n 3 circle       54.3  281.   47.8  725. -0.0683\n 4 dino         54.3  281.   47.8  726. -0.0645\n 5 dots         54.3  281.   47.8  725. -0.0603\n 6 h_lines      54.3  281.   47.8  726. -0.0617\n 7 high_lines   54.3  281.   47.8  726. -0.0685\n 8 slant_down   54.3  281.   47.8  726. -0.0690\n 9 slant_up     54.3  281.   47.8  726. -0.0686\n10 star         54.3  281.   47.8  725. -0.0630\n11 v_lines      54.3  281.   47.8  726. -0.0694\n12 wide_lines   54.3  281.   47.8  726. -0.0666\n13 x_shape      54.3  281.   47.8  725. -0.0656"
  },
  {
    "objectID": "lectures/02-graphics.html#viz-crime",
    "href": "lectures/02-graphics.html#viz-crime",
    "title": "Data visualization: the grammar of graphics and ggplot2",
    "section": "Viz crime?",
    "text": "Viz crime?\nFlorence Nightingale’s rose diagram"
  },
  {
    "objectID": "lectures/02-graphics.html#viz-crime-1",
    "href": "lectures/02-graphics.html#viz-crime-1",
    "title": "Data visualization: the grammar of graphics and ggplot2",
    "section": "Viz crime?",
    "text": "Viz crime?"
  },
  {
    "objectID": "lectures/02-graphics.html#previously",
    "href": "lectures/02-graphics.html#previously",
    "title": "Data visualization: the grammar of graphics and ggplot2",
    "section": "Previously…",
    "text": "Previously…\n\n\nYearly MLB batting statistics from Lahman with tidyverse:\n\ntotal hits, home runs, strikeouts, walks, at bats\ntotal batting average for each year= total H / total AB\nonly keeps AL and NL leagues\n\n\nlibrary(tidyverse)\nlibrary(Lahman) \nyearly_batting &lt;- Batting |&gt;\n  filter(lgID %in% c(\"AL\", \"NL\")) |&gt;\n  group_by(yearID) |&gt;\n  summarize(total_h = sum(H, na.rm = TRUE),\n            total_hr = sum(HR, na.rm = TRUE),\n            total_so = sum(SO, na.rm = TRUE),\n            total_bb = sum(BB, na.rm = TRUE),\n            total_ab = sum(AB, na.rm = TRUE)) |&gt;\n  mutate(batting_avg = total_h / total_ab)\n\n\nHow do we make data visualization?\nWhat are the steps to make this figure below?"
  },
  {
    "objectID": "lectures/02-graphics.html#the-grammar-of-graphics",
    "href": "lectures/02-graphics.html#the-grammar-of-graphics",
    "title": "Data visualization: the grammar of graphics and ggplot2",
    "section": "The grammar of graphics",
    "text": "The grammar of graphics\n\nKey idea: specify plotting “layers” and combine them to produce a graphic\nggplot2 provides an implementation of the grammar of graphics\nThe following layers are building blocks of data graphics\n\n\n\ndata - one or more datasets (in tidy tabular format)\ngeom - geometric objects to visually represent the data (e.g. points, lines, bars, etc.)\naes - mappings of variables to visual properties (i.e. aesthetics) of the geometric objects\nscale - one scale for each variable displayed (e.g. axis limits, log scale, colors, etc.)\nfacet - similar subplots (i.e. facets) for subsets of the same data using a conditioning variable\nstat - statistical transformations and summaries (e.g. identity, count, smooth, quantile, etc.)\ncoord - one or more coordinate systems (e.g. cartesian, polar, map projection)\nlabs - labels/guides for each variable and other parts of the plot (e.g. title, subtitle, caption, etc.)\ntheme - customization of plot layout (e.g. text size, alignment, legend position, etc.)\n\n\n\n\nLeland Wilkinson wrote the book “The Grammar of Graphics”, originally published in 1999."
  },
  {
    "objectID": "lectures/02-graphics.html#first-start-with-the-data",
    "href": "lectures/02-graphics.html#first-start-with-the-data",
    "title": "Data visualization: the grammar of graphics and ggplot2",
    "section": "First, start with the data",
    "text": "First, start with the data\n\n\n\n\nggplot(data = yearly_batting)\n\n\nor equivalently, using |&gt;\n\nyearly_batting |&gt; \n  ggplot()\n\n\nSo far, nothing is displayed"
  },
  {
    "objectID": "lectures/02-graphics.html#specify-variables-and-geometric-object",
    "href": "lectures/02-graphics.html#specify-variables-and-geometric-object",
    "title": "Data visualization: the grammar of graphics and ggplot2",
    "section": "Specify variables and geometric object",
    "text": "Specify variables and geometric object\n\n\n\n\nyearly_batting |&gt; \n  ggplot() +\n  geom_point(aes(x = yearID, y = total_hr))\n\n\nAdding (+) a geometric layer of points to the plot\nMap yearID to the x-axis and total_hr to the y-axis via aes()\nImplicitly using coord_cartesian()\n\n\nyearly_batting |&gt; \n  ggplot() + \n  geom_point(aes(x = yearID, y = total_hr)) +\n  coord_cartesian()"
  },
  {
    "objectID": "lectures/02-graphics.html#now-can-we-add-another-geometric-layer",
    "href": "lectures/02-graphics.html#now-can-we-add-another-geometric-layer",
    "title": "Data visualization: the grammar of graphics and ggplot2",
    "section": "Now, can we add another geometric layer?",
    "text": "Now, can we add another geometric layer?\n\n\n\n\nyearly_batting |&gt; \n  ggplot() +\n  geom_point(aes(x = yearID, y = total_hr)) +\n  geom_line(aes(x = yearID, y = total_hr))\n\n\nAdding (+) a line geometric layer\nInclude mappings shared across geometric layers inside ggplot()\n\n\nyearly_batting |&gt;\n  ggplot(aes(x = yearID, y = total_hr)) +\n  geom_point() +\n  geom_line()"
  },
  {
    "objectID": "lectures/02-graphics.html#scaling-axes-changing-axis-label-breaks",
    "href": "lectures/02-graphics.html#scaling-axes-changing-axis-label-breaks",
    "title": "Data visualization: the grammar of graphics and ggplot2",
    "section": "Scaling axes: changing axis label breaks",
    "text": "Scaling axes: changing axis label breaks\n\n\n\n\nyearly_batting |&gt;\n  ggplot(aes(x = yearID, y = total_hr)) +\n  geom_point() +\n  geom_line() +\n  scale_y_continuous(breaks = seq(0, 6000, 1000))"
  },
  {
    "objectID": "lectures/02-graphics.html#scaling-axes-customizing-axis-limits",
    "href": "lectures/02-graphics.html#scaling-axes-customizing-axis-limits",
    "title": "Data visualization: the grammar of graphics and ggplot2",
    "section": "Scaling axes: customizing axis limits",
    "text": "Scaling axes: customizing axis limits\n\n\n\n\nyearly_batting |&gt;\n  ggplot(aes(x = yearID, y = total_hr)) +\n  geom_point() +\n  geom_line() +\n  scale_x_continuous(limits = c(2000, 2015))"
  },
  {
    "objectID": "lectures/02-graphics.html#scaling-axes-having-different-axis-scales",
    "href": "lectures/02-graphics.html#scaling-axes-having-different-axis-scales",
    "title": "Data visualization: the grammar of graphics and ggplot2",
    "section": "Scaling axes: having different axis scales",
    "text": "Scaling axes: having different axis scales\n\n\n\n\nyearly_batting |&gt; \n  ggplot(aes(x = yearID, y = total_hr)) + \n  geom_point() +\n  geom_line() + \n  scale_x_reverse() +\n  scale_y_log10()\n\nWe can easily adjust variable scales without directly modifying the columns in the data"
  },
  {
    "objectID": "lectures/02-graphics.html#adding-a-statistical-summary",
    "href": "lectures/02-graphics.html#adding-a-statistical-summary",
    "title": "Data visualization: the grammar of graphics and ggplot2",
    "section": "Adding a statistical summary",
    "text": "Adding a statistical summary\n\n\n\n\nyearly_batting |&gt; \n  ggplot(aes(x = yearID, y = total_hr)) + \n  geom_point() +\n  geom_line() + \n  stat_smooth()\n\n\nSmoothing regression summary (will cover later) using yearID and total_hr\nGeometric layers implicitly use a default statistical summary\nTechnically we’re already using geom_point(stat = \"identity\")\n\n\nyearly_batting |&gt; \n  ggplot(aes(x = yearID, y = total_hr)) + \n  geom_point() +\n  geom_line() +\n  geom_smooth()"
  },
  {
    "objectID": "lectures/02-graphics.html#mapping-additional-variables",
    "href": "lectures/02-graphics.html#mapping-additional-variables",
    "title": "Data visualization: the grammar of graphics and ggplot2",
    "section": "Mapping additional variables",
    "text": "Mapping additional variables\n\n\n\n\nyearly_batting |&gt; \n  ggplot(aes(x = yearID, y = total_hr,\n             color = total_so,\n             size = total_bb)) +\n  geom_point() +\n  geom_line()\n\n\ntotal_hr, total_so, and total_bb are all displayed\ncolor and size are being shared globally across layers\nThis is a bit odd to look at…"
  },
  {
    "objectID": "lectures/02-graphics.html#customizing-mappings-by-layer",
    "href": "lectures/02-graphics.html#customizing-mappings-by-layer",
    "title": "Data visualization: the grammar of graphics and ggplot2",
    "section": "Customizing mappings by layer",
    "text": "Customizing mappings by layer\n\n\n\n\nyearly_batting |&gt; \n  ggplot(aes(x = yearID, y = total_hr)) + \n  geom_point(aes(color = total_so, size = total_bb)) +\n  geom_line()\n\n\nNow mapping total_so and total_bb to color and size of the point layer only"
  },
  {
    "objectID": "lectures/02-graphics.html#changing-aesthetics-without-mapping-variables",
    "href": "lectures/02-graphics.html#changing-aesthetics-without-mapping-variables",
    "title": "Data visualization: the grammar of graphics and ggplot2",
    "section": "Changing aesthetics without mapping variables",
    "text": "Changing aesthetics without mapping variables\n\n\n\n\nyearly_batting |&gt; \n  ggplot(aes(x = yearID, y = total_hr)) + \n  geom_point(aes(color = total_so, size = total_bb)) +\n  geom_line(color = \"darkred\", linetype = \"dashed\")\n\n\nManually set the color and linetype of the line layer"
  },
  {
    "objectID": "lectures/02-graphics.html#remember-one-scale-for-each-mapped-variable",
    "href": "lectures/02-graphics.html#remember-one-scale-for-each-mapped-variable",
    "title": "Data visualization: the grammar of graphics and ggplot2",
    "section": "Remember: one scale for each mapped variable",
    "text": "Remember: one scale for each mapped variable\n\n\n\n\nyearly_batting |&gt; \n  ggplot(aes(x = yearID, y = total_hr)) + \n  geom_point(aes(color = total_so, size = total_bb)) +\n  geom_line(color = \"darkred\", linetype = \"dashed\") +\n  scale_color_gradient(low = \"darkblue\", high = \"gold\") +\n  scale_size_continuous(breaks = seq(0, 20000, 2500))"
  },
  {
    "objectID": "lectures/02-graphics.html#always-label-your-plots-seriously",
    "href": "lectures/02-graphics.html#always-label-your-plots-seriously",
    "title": "Data visualization: the grammar of graphics and ggplot2",
    "section": "Always label your plots! (seriously…)",
    "text": "Always label your plots! (seriously…)\n\n\n\n\nyearly_batting |&gt; \n  ggplot(aes(x = yearID, y = total_hr)) + \n  geom_point(aes(color = total_so, size = total_bb)) +\n  geom_line(color = \"darkred\", linetype = \"dashed\") +\n  scale_color_gradient(low = \"darkblue\", high = \"gold\") +\n  labs(\n    x = \"Year\",\n    y = \"Homeruns\",\n    color = \"Strikeouts\",\n    size = \"Walks\",\n    title = \"The rise of three true outcomes in baseball\",\n    caption = \"Data courtesy of Lahman\"\n  )\n\n\nEach mapped aesthetic can be labeled"
  },
  {
    "objectID": "lectures/02-graphics.html#custom-theme",
    "href": "lectures/02-graphics.html#custom-theme",
    "title": "Data visualization: the grammar of graphics and ggplot2",
    "section": "Custom theme",
    "text": "Custom theme\n\n\n\n\nyearly_batting |&gt; \n  ggplot(aes(x = yearID, y = total_hr)) + \n  geom_point(aes(color = total_so, size = total_bb)) +\n  geom_line(color = \"darkred\", linetype = \"dashed\") +\n  scale_color_gradient(low = \"darkblue\", high = \"gold\") +\n  labs(\n    x = \"Year\",\n    y = \"Homeruns\",\n    color = \"Strikeouts\",\n    size = \"Walks\",\n    title = \"The rise of three true outcomes in baseball\",\n    caption = \"Data courtesy of Lahman\"\n  ) +\n  theme_bw(base_size = 20) +\n  theme(legend.position = \"bottom\",\n        plot.title = element_text(hjust = 0.5, \n                                  face = \"bold\"))\n\n\nFor more theme options, check out the ggthemes and hrbrthemes packages"
  },
  {
    "objectID": "lectures/02-graphics.html#a-lesson-about-data-visualization",
    "href": "lectures/02-graphics.html#a-lesson-about-data-visualization",
    "title": "Data visualization: the grammar of graphics and ggplot2",
    "section": "A lesson about data visualization…",
    "text": "A lesson about data visualization…\n\nSo far we’ve plotted total home runs across the years, with point size representing total walks and point color representing total strikeouts\n\n\n\nSimpler is better. What can we do to improve and make the plot simpler?\n\n\n\n\nHow about creating three separate plots for home runs, strikeouts, and walks, with each mapped to the y-axis?\nBut how do we do this without repeating the same code?"
  },
  {
    "objectID": "lectures/02-graphics.html#pivoting",
    "href": "lectures/02-graphics.html#pivoting",
    "title": "Data visualization: the grammar of graphics and ggplot2",
    "section": "Pivoting",
    "text": "Pivoting\nRemember: data should be in tidy format\nWithin the tidyverse, the tidyr package offers functions for reshaping the data\n\npivot_longer: casts/gathers information spread out across variables\n\ntransforms data from wide format into long format\nincrease number of rows and decrease number of columns\n\npivot_wider: melts/spreads information out from observations\n\ntransforms data from long format into wide format\ndecrease number of rows and increase number of columns\n\n\n\n\nSome terminology\nPredecessors: reshape and reshape2"
  },
  {
    "objectID": "lectures/02-graphics.html#pivoting-1",
    "href": "lectures/02-graphics.html#pivoting-1",
    "title": "Data visualization: the grammar of graphics and ggplot2",
    "section": "Pivoting",
    "text": "Pivoting\n\nyearly_batting |&gt; \n  select(yearID, HRs = total_hr, Strikeouts = total_so, Walks = total_bb) |&gt; # renaming while also selecting\n  pivot_longer(HRs:Walks, # can also do !yearID (to select everything but yearID)\n               names_to = \"stat\",\n               values_to = \"val\")\n\n# A tibble: 441 × 3\n   yearID stat         val\n    &lt;int&gt; &lt;chr&gt;      &lt;int&gt;\n 1   1876 HRs           40\n 2   1876 Strikeouts   589\n 3   1876 Walks        336\n 4   1877 HRs           24\n 5   1877 Strikeouts   726\n 6   1877 Walks        345\n 7   1878 HRs           23\n 8   1878 Strikeouts  1081\n 9   1878 Walks        364\n10   1879 HRs           58\n# ℹ 431 more rows\n\n\nWe’ve pivoted the data and created the following variables\n\nstat, to represent the name of the batting statistics\nval, to represent the total value of each statistic in each year."
  },
  {
    "objectID": "lectures/02-graphics.html#faceting",
    "href": "lectures/02-graphics.html#faceting",
    "title": "Data visualization: the grammar of graphics and ggplot2",
    "section": "Faceting",
    "text": "Faceting\n\n\n\nyearly_batting |&gt;\n  select(yearID, HRs = total_hr, \n         Strikeouts = total_so, Walks = total_bb) |&gt;\n  pivot_longer(HRs:Walks, \n               names_to = \"stat\", \n               values_to = \"val\") |&gt;\n  ggplot(aes(yearID, val)) +\n  geom_line(color = \"darkblue\") +\n  geom_point(alpha = 0.8, color = \"darkblue\") +\n  facet_wrap(~ stat, scales = \"free_y\", ncol = 1) +\n  labs(\n    x = \"Year\", \n    y = \"Total of statistic\",\n    title = \"The rise of three true outcomes in baseball\",\n    caption = \"Data courtesy of Lahman\"\n  ) +\n  theme_bw(base_size = 20) +\n  theme(strip.background = element_blank(),\n        plot.title = element_text(hjust = 0.5, \n                                  face = \"bold\"))\n\n\nCreate a multi-panel plot faceted by a conditioning variable (in our case, stat)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nThese facet/panel plots are sometimes called trellis plots (or lattice plots)"
  },
  {
    "objectID": "lectures/02-graphics.html#exercise",
    "href": "lectures/02-graphics.html#exercise",
    "title": "Data visualization: the grammar of graphics and ggplot2",
    "section": "Exercise",
    "text": "Exercise\nThe babynames package contains a dataset (also) named babynames, which contains information on the number of children of each sex given each name from 1880 to 2017, provided by the United States Social Security Administration.\nHow does the popularity (in terms of frequency) of your own name (combination of name and sex) change over time? Also, stick a thick, red, vertical dashed line on the plot at your birth year (try geom_vline()).\nNext, pick two other names and compare their popularity over time with your own name.\n\n# install.packages(\"babynames\")\n# library(babynames)\n# babynames |&gt; \n#   INSERT CODE HERE"
  },
  {
    "objectID": "lectures/02-graphics.html#resources",
    "href": "lectures/02-graphics.html#resources",
    "title": "Data visualization: the grammar of graphics and ggplot2",
    "section": "Resources",
    "text": "Resources\n\nggplot2 website: cheatsheets, FAQs, extensions, and more\nTidyTuesday\nDavid Robinson’s TidyTuesday screencasts\nVizBuzz: LIVE data viz replication game show\n\n\n\n\n\n\n\n\n\n\n\n36-SURE.github.io"
  },
  {
    "objectID": "lectures/11-linear.html#simple-linear-regression",
    "href": "lectures/11-linear.html#simple-linear-regression",
    "title": "Supervised learning: linear regression",
    "section": "Simple linear regression",
    "text": "Simple linear regression\nLinear regression is used when the response variable is quantitative\n\nAssume a linear relationship for \\(Y = f(X)\\) \\[Y_{i}=\\beta_{0}+\\beta_{1} X_{i}+\\epsilon_{i}, \\quad \\text { for } i=1,2, \\dots, n\\]\n\n\\(Y_i\\) is the \\(i\\)th value for the response variable\n\\(X_i\\) is the \\(i\\)th value for the predictor variable\n\n\n\n\n\\(\\beta_0\\) is an unknown, constant intercept\n\naverage value for \\(Y\\) if \\(X = 0\\) (be careful sometimes…)\n\n\\(\\beta_1\\) is an unknown, constant slope\n\nchange in average value for \\(Y\\) for each one-unit increase in \\(X\\)\n\n\n\n\n\n\\(\\epsilon_i\\) is the random noise\n\nassume independent, identically distributed (iid) from a normal distribution\n\\(\\epsilon_i \\overset{iid}{\\sim}N(0, \\sigma^2)\\) with constant variance \\(\\sigma^2\\)"
  },
  {
    "objectID": "lectures/11-linear.html#simple-linear-regression-estimation",
    "href": "lectures/11-linear.html#simple-linear-regression-estimation",
    "title": "Supervised learning: linear regression",
    "section": "Simple linear regression estimation",
    "text": "Simple linear regression estimation\nWe are estimating the conditional expection (mean) for \\(Y\\):\n\\[\\mathbb{E}[Y_i \\mid X_i] = \\beta_0 + \\beta_1X_i\\]\n\naverage value for \\(Y\\) given the value for \\(X\\)\n\n\nHow do we estimate the best fitting line?\n\n\nOrdinary least squares (OLS) - by minimizing the residual sum of squares (RSS)\n\\[\\text{RSS} \\left(\\beta_{0}, \\beta_{1}\\right)=\\sum_{i=1}^{n}\\left[Y_{i}-\\left(\\beta_{0}+\\beta_{1} X_{i}\\right)\\right]^{2}=\\sum_{i=1}^{n}\\left(Y_{i}-\\beta_{0}-\\beta_{1} X_{i}\\right)^{2}\\]\n\n\n\\[\\widehat{\\beta}_{1}=\\frac{\\sum_{i=1}^{n}\\left(X_{i}-\\bar{X}\\right)\\left(Y_{i}-\\bar{Y}\\right)}{\\sum_{i=1}^{n}\\left(X_{i}-\\bar{X}\\right)^{2}} \\quad \\text{ and } \\quad \\widehat{\\beta}_{0}=\\bar{Y}-\\widehat{\\beta}_{1} \\bar{X}\\]\nwhere \\(\\displaystyle \\bar{X} = \\frac{1}{n}\\sum_{i=1}^n X_i\\) and \\(\\displaystyle \\bar{Y} = \\frac{1}{n}\\sum_{i=1}^n Y_i\\)"
  },
  {
    "objectID": "lectures/11-linear.html#connection-to-covariance-and-correlation",
    "href": "lectures/11-linear.html#connection-to-covariance-and-correlation",
    "title": "Supervised learning: linear regression",
    "section": "Connection to covariance and correlation",
    "text": "Connection to covariance and correlation\nCovariance describes the joint variability of two variables \\[\\text{Cov}(X, Y) = \\sigma_{X,Y} = \\mathbb{E}[(X-\\mathbb{E}[X])(Y-\\mathbb{E}[Y])]\\]\n\nSample covariance (use \\(n - 1\\) since the means are used and we want unbiased estimates) \\[\\hat{\\sigma}_{X,Y} = \\frac{1}{n-1} \\sum_{i=1}^{n}\\left(X_{i}-\\bar{X}\\right)\\left(Y_{i}-\\bar{Y}\\right)\\]"
  },
  {
    "objectID": "lectures/11-linear.html#connection-to-covariance-and-correlation-1",
    "href": "lectures/11-linear.html#connection-to-covariance-and-correlation-1",
    "title": "Supervised learning: linear regression",
    "section": "Connection to covariance and correlation",
    "text": "Connection to covariance and correlation\nCorrelation is a normalized form of covariance, ranges from -1 to 1 \\[\\rho_{X,Y} = \\frac{\\text{Cov}(X,Y)}{\\sigma_X \\cdot \\sigma_Y}\\]\nSample correlation uses the sample covariance and standard deviations, e.g. \\(\\displaystyle s_X^2 = \\frac{1}{n-1} \\sum_i (X_i - \\bar{X})^2\\) \\[r_{X,Y} = \\frac{\\sum_{i=1}^{n}\\left(X_{i}-\\bar{X}\\right)\\left(Y_{i}-\\bar{Y}\\right)}{\\sqrt{\\sum_{i=1}^{n}\\left(X_{i}-\\bar{X}\\right)^{2} \\sum_{i=1}^{n}\\left(Y_{i}-\\bar{Y}\\right)^{2}}}\\]"
  },
  {
    "objectID": "lectures/11-linear.html#connection-to-covariance-and-correlation-2",
    "href": "lectures/11-linear.html#connection-to-covariance-and-correlation-2",
    "title": "Supervised learning: linear regression",
    "section": "Connection to covariance and correlation",
    "text": "Connection to covariance and correlation\nWe have \\[\\widehat{\\beta}_{1}=\\frac{\\sum_{i=1}^{n}(X_{i}-\\bar{X})(Y_{i}-\\bar{Y})}{\\sum_{i=1}^{n}(X_{i}-\\bar{X})^{2}} \\quad \\text{ and } \\quad r_{X,Y} = \\frac{\\sum_{i=1}^{n}(X_{i}-\\bar{X})(Y_{i}-\\bar{Y})}{\\sqrt{\\sum_{i=1}^{n}(X_{i}-\\bar{X})^{2} \\sum_{i=1}^{n}(Y_{i}-\\bar{Y})^{2}}}\\]\n\nWe can rewrite \\(\\hat{\\beta}_1\\) as \\[\\widehat{\\beta}_{1} = r_{X,Y} \\cdot \\frac{s_Y}{s_X}\\]\nWe can rewrite \\(\\displaystyle r_{X,Y}\\) as \\[r_{X,Y} = \\widehat{\\beta}_{1} \\cdot \\frac{s_X}{s_Y}\\]\n\n\nCan think of \\(\\widehat{\\beta}_{1}\\) weighting the ratio of variance between \\(X\\) and \\(Y\\)…"
  },
  {
    "objectID": "lectures/11-linear.html#gapminder-data",
    "href": "lectures/11-linear.html#gapminder-data",
    "title": "Supervised learning: linear regression",
    "section": "Gapminder data",
    "text": "Gapminder data\nHealth and income outcomes for 184 countries from 1960 to 2016 from the famous Gapminder project\n\nlibrary(tidyverse)\ntheme_set(theme_light())\nlibrary(dslabs)\nclean_gapminder &lt;- gapminder |&gt;\n  filter(year == 2011, !is.na(gdp)) |&gt;\n  mutate(log_gdp = log(gdp))\nglimpse(clean_gapminder)\n\nRows: 168\nColumns: 10\n$ country          &lt;fct&gt; \"Albania\", \"Algeria\", \"Angola\", \"Antigua and Barbuda\"…\n$ year             &lt;int&gt; 2011, 2011, 2011, 2011, 2011, 2011, 2011, 2011, 2011,…\n$ infant_mortality &lt;dbl&gt; 14.3, 22.8, 106.8, 7.2, 12.7, 15.3, 3.8, 3.4, 32.5, 1…\n$ life_expectancy  &lt;dbl&gt; 77.4, 76.1, 58.1, 75.9, 76.0, 73.5, 82.2, 80.7, 70.8,…\n$ fertility        &lt;dbl&gt; 1.75, 2.83, 6.10, 2.12, 2.20, 1.50, 1.88, 1.44, 1.96,…\n$ population       &lt;dbl&gt; 2886010, 36717132, 21942296, 88152, 41655616, 2967984…\n$ gdp              &lt;dbl&gt; 6321690864, 81143448101, 27013935821, 801787943, 4729…\n$ continent        &lt;fct&gt; Europe, Africa, Africa, Americas, Americas, Asia, Oce…\n$ region           &lt;fct&gt; Southern Europe, Northern Africa, Middle Africa, Cari…\n$ log_gdp          &lt;dbl&gt; 22.56725, 25.11948, 24.01962, 20.50235, 26.88222, 22.…"
  },
  {
    "objectID": "lectures/11-linear.html#modeling-life-expectancy",
    "href": "lectures/11-linear.html#modeling-life-expectancy",
    "title": "Supervised learning: linear regression",
    "section": "Modeling life expectancy",
    "text": "Modeling life expectancy\n\n\nInterested in modeling a country’s life expectancy\n\nclean_gapminder |&gt;\n  ggplot(aes(x = life_expectancy)) +\n  geom_histogram(color = \"black\", fill = \"gray\")"
  },
  {
    "objectID": "lectures/11-linear.html#relationship-between-life-expectancy-and-log-gdp",
    "href": "lectures/11-linear.html#relationship-between-life-expectancy-and-log-gdp",
    "title": "Supervised learning: linear regression",
    "section": "Relationship between life expectancy and log GDP",
    "text": "Relationship between life expectancy and log GDP\n\n\n\ngdp_plot &lt;- clean_gapminder |&gt;\n  ggplot(aes(x = log_gdp, y = life_expectancy)) +\n  geom_point(size = 3, alpha = 0.5)\ngdp_plot\n\nWe fit linear regression models using lm(), formula is input as: response ~ predictor\n\nsimple_lm &lt;- lm(life_expectancy ~ log_gdp, \n              data = clean_gapminder)"
  },
  {
    "objectID": "lectures/11-linear.html#summarize-the-model-using-summary",
    "href": "lectures/11-linear.html#summarize-the-model-using-summary",
    "title": "Supervised learning: linear regression",
    "section": "Summarize the model using summary()",
    "text": "Summarize the model using summary()\n\nsummary(simple_lm)\n\n\nCall:\nlm(formula = life_expectancy ~ log_gdp, data = clean_gapminder)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-18.901  -4.781   1.879   5.335  13.962 \n\nCoefficients:\n            Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)   24.174      5.758   4.198 4.38e-05 ***\nlog_gdp        1.975      0.242   8.161 7.87e-14 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 7.216 on 166 degrees of freedom\nMultiple R-squared:  0.2864,    Adjusted R-squared:  0.2821 \nF-statistic: 66.61 on 1 and 166 DF,  p-value: 7.865e-14"
  },
  {
    "objectID": "lectures/11-linear.html#summarize-the-model-using-the-broom-package",
    "href": "lectures/11-linear.html#summarize-the-model-using-the-broom-package",
    "title": "Supervised learning: linear regression",
    "section": "Summarize the model using the broom package",
    "text": "Summarize the model using the broom package\nThe 3 broom functions (Note: the output is always a tibble)\n\ntidy(): coefficients table in a tidy format\nglance(): produces summary metrics of a model fit\naugment(): adds/“augments” columns to the original data (e.g., adding predictions)\n\nNote: the output of tidy(), augment() and glance() are always a tibble.\n\nlibrary(broom)\ntidy(simple_lm)\n\n# A tibble: 2 × 5\n  term        estimate std.error statistic  p.value\n  &lt;chr&gt;          &lt;dbl&gt;     &lt;dbl&gt;     &lt;dbl&gt;    &lt;dbl&gt;\n1 (Intercept)    24.2      5.76       4.20 4.38e- 5\n2 log_gdp         1.97     0.242      8.16 7.87e-14\n\nglance(simple_lm)\n\n# A tibble: 1 × 12\n  r.squared adj.r.squared sigma statistic  p.value    df logLik   AIC   BIC\n      &lt;dbl&gt;         &lt;dbl&gt; &lt;dbl&gt;     &lt;dbl&gt;    &lt;dbl&gt; &lt;dbl&gt;  &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt;\n1     0.286         0.282  7.22      66.6 7.87e-14     1  -569. 1145. 1154.\n# ℹ 3 more variables: deviance &lt;dbl&gt;, df.residual &lt;int&gt;, nobs &lt;int&gt;"
  },
  {
    "objectID": "lectures/11-linear.html#inference-with-ols",
    "href": "lectures/11-linear.html#inference-with-ols",
    "title": "Supervised learning: linear regression",
    "section": "Inference with OLS",
    "text": "Inference with OLS\nIntercept and coefficient estimates \\[\\quad \\hat{\\beta}_0 = 24.174 \\quad \\text{and} \\quad \\hat{\\beta}_1 = 1.975\\]\n\nEstimates of uncertainty for \\(\\beta\\)s via standard errors \\[\\quad \\widehat{SE}(\\hat{\\beta}_0) = 5.758 \\quad \\text{and} \\quad \\widehat{SE}(\\hat{\\beta}_1) = 0.242\\]\n\n\n\\(t\\)-statistics: Estimates / Std. Error, i.e., number of standard deviations from 0\n\n\\(p\\)-values (i.e., Pr(&gt;|t|)): estimated probability observing value as extreme as |t value| given the null hypothesis \\(\\beta = 0\\)\n\\(p\\)-value \\(&lt; \\alpha = 0.05\\) (conventional threshold): sufficient evidence to reject the null hypothesis that the coefficient is zero\ni.e., there is a significant association between life_expectancy and log_gdp"
  },
  {
    "objectID": "lectures/11-linear.html#be-careful",
    "href": "lectures/11-linear.html#be-careful",
    "title": "Supervised learning: linear regression",
    "section": "Be careful!",
    "text": "Be careful!\nCaveats to keep in mind regarding p-values:\n\nIf the true value of a coefficient \\(\\beta = 0\\), then the \\(p\\)-value is sampled from a Uniform(0,1) distribution\n\ni.e., it is just as likely to have value 0.45 as 0.16 or 0.84 or 0.9999 or 0.00001…\n\n\n\nHence why we typically only reject for low \\(\\alpha\\) values like 0.05\n\nControlling the Type 1 error rate at \\(\\alpha = 0.05\\), i.e., the probability of a false positive mistake\n5% chance that you’ll conclude there’s a significant association between \\(x\\) and \\(y\\) even when there is none\n\n\n\nRemember what a standard error is? \\(\\displaystyle \\text{SE} = \\frac{\\sigma}{\\sqrt{n}}\\)\n\nAs \\(n\\) gets large standard error goes to zero, and all predictors are eventually deemed significant\nWhile the \\(p\\)-values might be informative, we will explore other approaches to determine which subset of predictors to include (e.g., holdout performance)"
  },
  {
    "objectID": "lectures/11-linear.html#back-to-the-model-summary-multiple-r-squared",
    "href": "lectures/11-linear.html#back-to-the-model-summary-multiple-r-squared",
    "title": "Supervised learning: linear regression",
    "section": "Back to the model summary: Multiple R-squared",
    "text": "Back to the model summary: Multiple R-squared\nBack to the connection between the coefficient and correlation \\[r_{X,Y} = \\widehat{\\beta}_{1} \\cdot \\frac{s_X}{s_Y} \\quad \\longrightarrow \\quad r^2_{X,Y} = \\widehat{\\beta}_{1}^2\\cdot \\frac{s_X^2}{s_Y^2}\\]\nCompute the correlation with cor():\n\ncor(clean_gapminder$log_gdp, clean_gapminder$life_expectancy)\n\n[1] 0.5351189\n\n\n\nThe squared cor matches the reported Multiple R-squared\n\ncor(clean_gapminder$log_gdp, clean_gapminder$life_expectancy) ^ 2\n\n[1] 0.2863522"
  },
  {
    "objectID": "lectures/11-linear.html#back-to-the-model-summary-multiple-r-squared-1",
    "href": "lectures/11-linear.html#back-to-the-model-summary-multiple-r-squared-1",
    "title": "Supervised learning: linear regression",
    "section": "Back to the model summary: Multiple R-squared",
    "text": "Back to the model summary: Multiple R-squared\nBack to the connection between the coefficient and correlation \\[r_{X,Y} = \\widehat{\\beta}_{1} \\cdot \\frac{s_X}{s_Y} \\quad \\longrightarrow \\quad r^2_{X,Y} = \\widehat{\\beta}_{1}^2\\cdot \\frac{s_X^2}{s_Y^2}\\]\n\\(r^2\\) (or also \\(R^2\\)) estimates the proportion of the variance of \\(Y\\) explained by \\(X\\)\n\nMore generally: variance of model predictions / variance of \\(Y\\)\n\n\nvar(predict(simple_lm)) / var(clean_gapminder$life_expectancy) \n\n[1] 0.2863522"
  },
  {
    "objectID": "lectures/11-linear.html#generating-predictions",
    "href": "lectures/11-linear.html#generating-predictions",
    "title": "Supervised learning: linear regression",
    "section": "Generating predictions",
    "text": "Generating predictions\nWe can use the predict() or fitted() function to either get the fitted values of the regression:\n\ntrain_preds &lt;- predict(simple_lm)\nhead(train_preds)\n\n       1        2        3        4        5        6 \n68.74401 73.78465 71.61243 64.66585 77.26605 67.97876 \n\n\n\nhead(fitted(simple_lm))\n\n       1        2        3        4        5        6 \n68.74401 73.78465 71.61243 64.66585 77.26605 67.97876 \n\n\n\nhead(simple_lm$fitted.values)\n\n       1        2        3        4        5        6 \n68.74401 73.78465 71.61243 64.66585 77.26605 67.97876 \n\n\n\nsimple_lm |&gt; \n  pluck(\"fitted.values\") |&gt; \n  head()\n\n       1        2        3        4        5        6 \n68.74401 73.78465 71.61243 64.66585 77.26605 67.97876"
  },
  {
    "objectID": "lectures/11-linear.html#predictions-for-new-data",
    "href": "lectures/11-linear.html#predictions-for-new-data",
    "title": "Supervised learning: linear regression",
    "section": "Predictions for new data",
    "text": "Predictions for new data\nOr we can provide it newdata which must contain the explanatory variables:\n\n\n\nus_data &lt;- clean_gapminder |&gt; \n  filter(country == \"United States\")\n\nnew_us_data &lt;- us_data |&gt;\n  select(country, gdp) |&gt;\n  slice(rep(1, 3)) |&gt; \n  mutate(adj_factor = c(0.25, 0.5, 0.75),\n         log_gdp = log(gdp * adj_factor))\nnew_us_data$pred_life_exp &lt;- \n  predict(simple_lm, newdata = new_us_data) \ngdp_plot +\n  geom_point(data = new_us_data,\n             aes(x = log_gdp, y = pred_life_exp),\n             color = \"darkred\", size = 3)"
  },
  {
    "objectID": "lectures/11-linear.html#plot-observed-values-against-predictions",
    "href": "lectures/11-linear.html#plot-observed-values-against-predictions",
    "title": "Supervised learning: linear regression",
    "section": "Plot observed values against predictions",
    "text": "Plot observed values against predictions\nUseful diagnostic (for any type of model, not just linear regression!)\n\n\n\nclean_gapminder |&gt;\n  mutate(pred_vals = predict(simple_lm)) |&gt; \n  ggplot(aes(x = pred_vals, y = life_expectancy)) +\n  geom_point(alpha = 0.5, size = 3) +\n  geom_abline(slope = 1, intercept = 0, \n              linetype = \"dashed\",\n              color = \"red\",\n              linewidth = 2)\n\n\n“Perfect” model will follow diagonal"
  },
  {
    "objectID": "lectures/11-linear.html#plot-observed-values-against-predictions-1",
    "href": "lectures/11-linear.html#plot-observed-values-against-predictions-1",
    "title": "Supervised learning: linear regression",
    "section": "Plot observed values against predictions",
    "text": "Plot observed values against predictions\nAugment the data with model output using the broom package\n\n\n\nclean_gapminder &lt;- simple_lm |&gt; \n  augment(clean_gapminder) \nclean_gapminder |&gt;\n  ggplot(aes(x = .fitted, y = life_expectancy)) + \n  geom_point(alpha = 0.5, size = 3) +\n  geom_abline(slope = 1, intercept = 0, color = \"red\", \n              linetype = \"dashed\", linewidth = 2)\n\n\nAdds various columns from model fit we can use in plotting for model diagnostics"
  },
  {
    "objectID": "lectures/11-linear.html#assessing-assumptions-of-linear-regression",
    "href": "lectures/11-linear.html#assessing-assumptions-of-linear-regression",
    "title": "Supervised learning: linear regression",
    "section": "Assessing assumptions of linear regression",
    "text": "Assessing assumptions of linear regression\nSimple linear regression assumes \\(Y_i \\overset{iid}{\\sim} N(\\beta_0 + \\beta_1 X_i, \\sigma^2)\\)\n\nIf this is true, then \\(Y_i - \\hat{Y}_i \\overset{iid}{\\sim} N(0, \\sigma^2)\\)\n\n\nPlot residuals against \\(\\hat{Y}_i\\), residuals vs fit plot\n\nUsed to assess linearity, any divergence from mean 0\nUsed to assess equal variance, i.e., if \\(\\sigma^2\\) is homogenous across predictions/fits \\(\\hat{Y}_i\\)\n\n\n\nMore difficult to assess the independence and fixed \\(X\\) assumptions\n\nMake these assumptions based on subject-matter knowledge"
  },
  {
    "objectID": "lectures/11-linear.html#plot-residuals-against-predicted-values",
    "href": "lectures/11-linear.html#plot-residuals-against-predicted-values",
    "title": "Supervised learning: linear regression",
    "section": "Plot residuals against predicted values",
    "text": "Plot residuals against predicted values\n\n\n\nResiduals = observed - predicted\nInterpretation of residuals in context?\nConditional on the predicted values, the residuals should have a mean of zero\n\n\nclean_gapminder |&gt;\n  ggplot(aes(x = .fitted, y = .resid)) + \n  geom_point(alpha = 0.5, size = 3) +\n  geom_hline(yintercept = 0, linetype = \"dashed\", \n             color = \"red\", linewidth = 2) +\n  # plot the residual mean\n  geom_smooth(se = FALSE)\n\n\nResiduals should NOT display any pattern\nTwo things to look for:\n\nAny trend around horizontal reference line?\nEqual vertical spread?"
  },
  {
    "objectID": "lectures/11-linear.html#multiple-linear-regression",
    "href": "lectures/11-linear.html#multiple-linear-regression",
    "title": "Supervised learning: linear regression",
    "section": "Multiple linear regression",
    "text": "Multiple linear regression\nWe can include as many variables as we want (assuming \\(n &gt; p\\)!)\n\\[Y=\\beta_{0}+\\beta_{1} X_{1}+\\beta_{2} X_{2}+\\cdots+\\beta_{p} X_{p}+\\epsilon\\]\nOLS estimates in matrix notation ( \\(\\boldsymbol{X}\\) is a \\(n \\times p\\) matrix):\n\\[\\hat{\\boldsymbol{\\beta}} = (\\boldsymbol{X} ^T \\boldsymbol{X})^{-1}\\boldsymbol{X}^T\\boldsymbol{Y}\\]\n\nCan just add more variables to the formula in R\n\nmultiple_lm &lt;- lm(life_expectancy ~ log_gdp + fertility, data = clean_gapminder)\n\n\nUse the Adjusted R-squared when including multiple variables \\(\\displaystyle 1 - \\frac{(1 - R^2)(n - 1)}{(n - p - 1)}\\)\n\nAdjusts for the number of variables in the model \\(p\\)\nAdding more variables will always increase Multiple R-squared"
  },
  {
    "objectID": "lectures/11-linear.html#what-about-the-normal-distribution-assumption",
    "href": "lectures/11-linear.html#what-about-the-normal-distribution-assumption",
    "title": "Supervised learning: linear regression",
    "section": "What about the Normal distribution assumption?",
    "text": "What about the Normal distribution assumption?\n\nModel: \\(Y=\\beta_{0}+\\beta_{1} X_{1}+\\beta_{2} X_{2}+\\cdots+\\beta_{p} X_{p}+\\epsilon\\)\n\\(\\epsilon_i\\) is the random noise: assume independent and identically distributed (iid) from a Normal distribution \\(\\epsilon_i \\overset{iid}{\\sim}N(0, \\sigma^2)\\) with constant variance \\(\\sigma^2\\)\n\n\n\nOLS doesn’t care about this assumption, it’s just estimating coefficients!\n\n\n\n\nIn order to perform inference, we need to impose additional assumptions\nBy assuming \\(\\epsilon_i \\overset{iid}{\\sim}N(0, \\sigma^2)\\), what we really mean is \\(Y \\overset{iid}{\\sim}N(\\beta_{0}+\\beta_{1} X_{1}+\\beta_{2} X_{2}+\\cdots+\\beta_{p} X_{p}, \\sigma^2)\\)\n\n\n\n\nSo we’re estimating the mean \\(\\mu\\) of this conditional distribution, but what about \\(\\sigma^2\\)?\n\n\n\n\nUnbiased estimate \\(\\displaystyle \\hat{\\sigma}^2 = \\frac{\\text{RSS}}{n - (p + 1)}\\), its square root is the Residual standard error\nDegrees of freedom: \\(n - (p + 1)\\), data supplies us with \\(n\\) “degrees of freedom” and we used up \\(p + 1\\)"
  },
  {
    "objectID": "lectures/11-linear.html#check-the-assumptions-about-normality-with-ggfortify",
    "href": "lectures/11-linear.html#check-the-assumptions-about-normality-with-ggfortify",
    "title": "Supervised learning: linear regression",
    "section": "Check the assumptions about normality with ggfortify",
    "text": "Check the assumptions about normality with ggfortify\n\nlibrary(ggfortify)\nautoplot(multiple_lm, ncol = 4)\n\n\n\nStandardized residuals = residuals / sd(residuals) (see also .std.resid from augment)\n\n\n\n36-SURE.github.io"
  },
  {
    "objectID": "r-setup.html#r-and-rstudio",
    "href": "r-setup.html#r-and-rstudio",
    "title": "R and RStudio Setup",
    "section": "R and RStudio",
    "text": "R and RStudio\nR is an open-source programming language for statistical computing. R is widely-used in both academia and industry, due to its capacity for statistical analysis and data science.\nIn order to use R effectively, you need a suitable editing environment, i.e. RStudio. For more context, RStudio is an integrated development environment (IDE) developed specifically for R programming. Although R can be run without RStudio, RStudio provides a more user-friendly experience with additional functionality.\nTo download R and RStudio, visit https://posit.co/download/rstudio-desktop.\nYou will see the following two tasks to be completed:\n\n1: Install R\n2: Install RStudio"
  },
  {
    "objectID": "r-setup.html#step-1-install-r",
    "href": "r-setup.html#step-1-install-r",
    "title": "R and RStudio Setup",
    "section": "Step 1: Install R",
    "text": "Step 1: Install R\n(Note that the following instructions apply to the latest R version (4.4.0) as of May 24, 2024)\nClick on DOWNLOAD AND INSTALL R. This will direct you to the CRAN (Comprehensive R Archive Network) website.\nmacOS\n\nClick on Download R for macOS.\nChoose the .pkg file suitable for your Mac (Apple silicon (M1-3) Macs or older Intel Macs.)\nOpen the .pkg file after the download is complete.\nFollow the installation instructions.\n\nWindows\n\nClick on Download R for Windows.\nClick on install R for the first time (on the same line as the base subdirectory.)\nChoose Download R 4.4.0 for Windows.\nOpen the .exe file after the download is complete.\nFollow the installation instructions."
  },
  {
    "objectID": "r-setup.html#step-2-install-rstudio",
    "href": "r-setup.html#step-2-install-rstudio",
    "title": "R and RStudio Setup",
    "section": "Step 2: Install RStudio",
    "text": "Step 2: Install RStudio\n\n\n\n\n\n\nImportant\n\n\n\nR must be installed before RStudio.\n\n\nClick on DOWNLOAD RSTUDIO DESKTOP… Your operating system is automatically detected. (If your OS is not correctly detected, scroll down and choose the right version for your system.)\nmacOS\n\nOpen the .dmg file after the download is complete.\nDrag and drop it to your Applications folder.\n\nWindows\n\nOpen the .exe file after the download is complete.\nFollow the installation instructions."
  },
  {
    "objectID": "r-setup.html#step3",
    "href": "r-setup.html#step3",
    "title": "R and RStudio Setup",
    "section": "Step 3: Check R and RStudio installations",
    "text": "Step 3: Check R and RStudio installations\nOpen RStudio and type in the following command in the Console pane.\n\nversion\n\nThis will print out the current version of R on your machine. The output should look similar to what shown below (the first 4 lines might be different, depending on your operating system.)\n\n\n               _                           \nplatform       aarch64-apple-darwin20      \narch           aarch64                     \nos             darwin20                    \nsystem         aarch64, darwin20           \nstatus                                     \nmajor          4                           \nminor          4.0                         \nyear           2024                        \nmonth          04                          \nday            24                          \nsvn rev        86474                       \nlanguage       R                           \nversion.string R version 4.4.0 (2024-04-24)\nnickname       Puppy Cup"
  },
  {
    "objectID": "r-setup.html#step-4-install-an-r-package",
    "href": "r-setup.html#step-4-install-an-r-package",
    "title": "R and RStudio Setup",
    "section": "Step 4: Install an R package",
    "text": "Step 4: Install an R package\nIn R, a package is a collection of functions, data, and compiled code. In addition to a set of built-in base packages, there are numerous external R packages written by the community to add specific functionality.\nIn general, to install an R package, you can use the install.packages() function and pass in the package name.\nThe following example shows how to install the tidyverse package in R. The tidyverse is a suite of R packages we will be using throughout this program. It features popular packages such as ggplot2 for data visualization and dplyr for data manipulation.\nAfter installing R and RStudio, open RStudio and enter the following command in the Console pane.\n\ninstall.packages(\"tidyverse\")\n\nTo verify that tidyverse is successfully installed, run the following command:\n\nlibrary(tidyverse)\n\nYou should get a message similar to the output below.\n\n\n── Attaching core tidyverse packages ──────────────────────── tidyverse 2.0.0 ──\n✔ dplyr     1.1.4     ✔ readr     2.1.5\n✔ forcats   1.0.0     ✔ stringr   1.5.1\n✔ ggplot2   3.5.1     ✔ tibble    3.2.1\n✔ lubridate 1.9.3     ✔ tidyr     1.3.1\n✔ purrr     1.0.2     \n── Conflicts ────────────────────────────────────────── tidyverse_conflicts() ──\n✖ dplyr::filter() masks stats::filter()\n✖ dplyr::lag()    masks stats::lag()\nℹ Use the conflicted package (&lt;http://conflicted.r-lib.org/&gt;) to force all conflicts to become errors\n\n\nAsk us for help if you encounter any issues or errors in any of the installation steps above."
  },
  {
    "objectID": "r-setup.html#r-primers-on-posit-cloud",
    "href": "r-setup.html#r-primers-on-posit-cloud",
    "title": "R and RStudio Setup",
    "section": "R Primers on Posit Cloud",
    "text": "R Primers on Posit Cloud\nIn addition to following the steps above for installing R and RStudio on your computer, we recommend you make a free Posit Cloud (formerly RStudio Cloud) account at https://posit.cloud/. This is a free, browser-based version of R and RStudio that also provides access to a growing number of relevant R tutorials / primers.\nAfter you create a Posit Cloud account, navigate to the menu on the left and click on “Recipes”. This brings up a menu of tutorials, with code primers you can choose to work through. Please complete the tutorials listed under “R Basics” (you can skip the first three). Also, feel free to explore the other tutorials."
  }
]